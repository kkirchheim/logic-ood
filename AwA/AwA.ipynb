{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "1b742a3a-55d6-49ad-9c60-68b883556503",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torchvision.models import resnet50, resnet18\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from torch import nn \n",
    "import torch \n",
    "\n",
    "class Model(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.model = resnet50(weights=ResNet50_Weights)\n",
    "        self.model.fc = nn.Linear(self.model.fc.in_features, num_classes)\n",
    "        \n",
    "    def features(self, x) -> torch.Tensor:\n",
    "        x = self.model.conv1(x)\n",
    "        x = self.model.bn1(x)\n",
    "        x = self.model.relu(x)\n",
    "        x = self.model.maxpool(x)\n",
    "\n",
    "        x = self.model.layer1(x)\n",
    "        x = self.model.layer2(x)\n",
    "        x = self.model.layer3(x)\n",
    "        x = self.model.layer4(x)\n",
    "\n",
    "        x = self.model.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.model.forward(x)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "60cedfa2-96f2-4530-9e97-97973270587e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ki/.local/share/anaconda3/envs/pytorch/lib/python3.9/site-packages/torchvision/models/_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and will be removed in 0.15. The current behavior is equivalent to passing `weights=ResNet50_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet50_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from os.path import join, exists\n",
    "from os import listdir\n",
    "import pandas as pd \n",
    "from PIL import Image\n",
    "from torch.optim import SGD\n",
    "import matplotlib.pyplot as plt \n",
    "from awa import AwA\n",
    "from torchvision.models import resnet50, resnet18\n",
    "from torchvision.models import ResNet50_Weights\n",
    "from pytorch_ood.utils import ToUnknown, ToRGB\n",
    "from torchvision.transforms import ToTensor, Compose, Resize \n",
    "from torchvision import transforms\n",
    "from pytorch_ood.utils import ToRGB\n",
    "import torch \n",
    "from pytorch_ood.dataset.img import Textures, FractalDataset, FoolingImages, ImageNetA, ImageNetO, ImageNetR\n",
    "from tqdm.notebook import tqdm\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "transform = transforms.Compose([            \n",
    "     ToRGB(),\n",
    "     transforms.Resize(256),\n",
    "     transforms.CenterCrop(224),\n",
    "     transforms.ToTensor(),\n",
    "     transforms.Normalize(\n",
    "     mean=[0.485, 0.456, 0.406],\n",
    "     std=[0.229, 0.224, 0.225]\n",
    "     )\n",
    "])\n",
    "\n",
    "device = \"cuda:0\"\n",
    "\n",
    "model = Model(num_classes=50) # weights=ResNet50_Weights\n",
    "model.eval().to(device)\n",
    "\n",
    "\n",
    "def get_loader_for_name(name, root, subset):\n",
    "    if name == \"awa\":\n",
    "        dataset = AwA(root=root, transform=transform)\n",
    "        train_dataset, test_dataset = torch.utils.data.random_split(dataset, [30000, 7322], generator=torch.Generator().manual_seed(123))\n",
    "        if subset == \"test\":\n",
    "            dataset = test_dataset\n",
    "        elif subset == \"train\":\n",
    "            dataset = train_dataset\n",
    "        else:\n",
    "            raise ValueError\n",
    "    else:\n",
    "        names = {\n",
    "            \"textures\": Textures,\n",
    "            \"fractals\": FractalDataset,\n",
    "            \"fooling\": FoolingImages,\n",
    "            \"imagenet-a\": ImageNetA,\n",
    "            \"imagenet-o\": ImageNetO,\n",
    "            \"imagenet-r\": ImageNetR\n",
    "        }\n",
    "        clazz = names[name]\n",
    "        dataset = clazz(root=root, transform=transform, target_transform=ToUnknown(), download=True)\n",
    "        \n",
    "    loader = torch.utils.data.DataLoader(dataset, batch_size=64, shuffle=False, num_workers=10)\n",
    "    return loader     \n",
    "        \n",
    "        \n",
    "def load_dataset_features(name, root, subset=\"test\", full=True):\n",
    "    print(f\"Loading {name}-{subset}\")\n",
    "    \n",
    "    path = join(\"data\", f\"features-{name}-{subset}.pt\")\n",
    "    \n",
    "    if exists(path):\n",
    "        if name == \"awa\":\n",
    "            x, y, z = torch.load(path, map_location=\"cpu\")\n",
    "            if full:\n",
    "                return TensorDataset(x, y, z)\n",
    "            else:\n",
    "                return TensorDataset(x, y)\n",
    "        else:\n",
    "            x, y = torch.load(path, map_location=\"cpu\")\n",
    "            return TensorDataset(x, y)\n",
    "    else:\n",
    "        loader = get_loader_for_name(name, root, subset)\n",
    "\n",
    "        bar =  tqdm(enumerate(loader), total=len(loader))\n",
    "        features = []\n",
    "        ys = []\n",
    "        zs = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for i, batch in bar:\n",
    "                if name == \"awa\":\n",
    "                    x, y, z  = batch\n",
    "                    x = x.to(device)\n",
    "                    outputs = model.features(x)\n",
    "                    features.append(outputs)\n",
    "                    ys.append(y)\n",
    "                    zs.append(z)\n",
    "                else:\n",
    "                    x, y  = batch\n",
    "                    x = x.to(device)\n",
    "                    outputs = model.features(x)\n",
    "                    features.append(outputs)\n",
    "                    ys.append(y)\n",
    "\n",
    "        xs = torch.cat(features)\n",
    "        ys = torch.cat(ys)\n",
    "        if name == \"awa\": \n",
    "            zs = torch.cat(zs)\n",
    "        \n",
    "        if name == \"awa\": \n",
    "            torch.save((xs, ys, zs), path)\n",
    "            if full:\n",
    "                return TensorDataset(xs.cpu(), ys.cpu(), zs.cpu())\n",
    "            else:\n",
    "                return TensorDataset(xs.cpu(), ys.cpu())\n",
    "        else:\n",
    "            torch.save((xs, ys), path)\n",
    "            return TensorDataset(xs.cpu(), ys.cpu())\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df8a1fe4-ed76-4ece-b4cc-ec6d978b1664",
   "metadata": {},
   "outputs": [],
   "source": [
    "root = \"/home/ki/datasets/\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8dbdf92b-7389-4a81-b07a-2692fc7419b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading awa-train\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f89cc99f902940f1ac837bf2f6ecb269",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading awa-test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4588584b949c405a8f98047021a2ea61",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/115 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "train_ds = load_dataset_features(\"awa\", root, subset=\"train\")\n",
    "test_ds = load_dataset_features(\"awa\", root, subset=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f79daf5f-dfb3-49e6-985b-21993380444d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4c19d606cce54bb2b26c5a4dfbbcb2ea",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test images: 87.56%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "90e96e9feeed4c8fbc8a7ac1dca77911",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test images: 90.00%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a773e2b5f597441eb1a46389d2ad0083",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on test images: 90.85%\n"
     ]
    }
   ],
   "source": [
    "mini_model = nn.Linear(2048, 50)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "momentum = 0.9\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(mini_model.parameters(), lr=learning_rate, momentum=momentum, nesterov=True)\n",
    "\n",
    "_ = mini_model.to(device)\n",
    "\n",
    "train_ld = torch.utils.data.DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=10)\n",
    "test_ld = torch.utils.data.DataLoader(test_ds, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "\n",
    "for epoch in range(3):\n",
    "    running_loss = 0.0\n",
    "    bar =  tqdm(enumerate(train_ld), total=len(train_ld))\n",
    "    for i, batch in bar:\n",
    "        x, y, z  = batch\n",
    "        x, y = x.to(device), y.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        outputs = mini_model(x)\n",
    "        loss = criterion(outputs, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss = 0.8* running_loss + 0.2 * loss.item()\n",
    "        bar.set_postfix({\"loss\": running_loss})\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in test_ld:\n",
    "            x, y, z  = batch\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            outputs = mini_model(x)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "    \n",
    "    bar.set_postfix({\"loss\": running_loss, \"acc\": correct / total})\n",
    "    print(f'Accuracy on test images: {correct / total:.2%}')\n",
    "    \n",
    "mini_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "0349084c-79d6-4b41-9c63-c68a00a63c0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# train with different attributes \n",
    "def train_for_attribute(att_index):\n",
    "    train_ds = load_dataset_features(\"awa\", root, subset=\"train\")\n",
    "    test_ds = load_dataset_features(\"awa\", root, subset=\"test\")\n",
    "    mini_model = nn.Linear(2048, 2)\n",
    "\n",
    "    learning_rate = 1e-3\n",
    "    momentum = 0.9\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(mini_model.parameters(), lr=learning_rate, momentum=momentum, nesterov=True)\n",
    "\n",
    "    _ = mini_model.to(device)\n",
    "\n",
    "    train_ld = torch.utils.data.DataLoader(train_ds, batch_size=64, shuffle=True, num_workers=10)\n",
    "    test_ld = torch.utils.data.DataLoader(test_ds, batch_size=64, shuffle=True, num_workers=4)\n",
    "\n",
    "    for epoch in range(3):\n",
    "        running_loss = 0.0\n",
    "        for batch in train_ld:\n",
    "            x, y, z  = batch\n",
    "            \n",
    "            x, z = x.to(device), z.to(device)\n",
    "            z = z[:,att_index].long()\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            outputs = mini_model(x)\n",
    "            loss = criterion(outputs, z)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss = 0.8* running_loss + 0.2 * loss.item()\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for batch in test_ld:\n",
    "                x, y, z  = batch\n",
    "                x, z = x.to(device), z.to(device)\n",
    "                z = z[:,att_index].long()\n",
    "            \n",
    "                outputs = mini_model(x)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += z.size(0)\n",
    "                correct += (predicted == z).sum().item()\n",
    "\n",
    "        bar.set_postfix({\"loss\": running_loss, \"acc\": correct / total})\n",
    "        print(f'Accuracy on test images: {correct / total:.2%}')\n",
    "    \n",
    "    return mini_model \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "727de6d3-05a0-4072-900c-2f6202ac83bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training model for 0 -> black\n",
      "Loading awa-train\n",
      "Loading awa-test\n",
      "Accuracy on test images: 88.31%\n",
      "Accuracy on test images: 89.61%\n",
      "Accuracy on test images: 89.87%\n",
      "Training model for 1 -> white\n",
      "Loading awa-train\n",
      "Loading awa-test\n",
      "Accuracy on test images: 89.66%\n",
      "Accuracy on test images: 90.96%\n",
      "Accuracy on test images: 91.19%\n",
      "Training model for 2 -> blue\n",
      "Loading awa-train\n",
      "Loading awa-test\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [70]\u001b[0m, in \u001b[0;36m<cell line: 5>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m85\u001b[39m):\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTraining model for \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mi\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m -> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mawads\u001b[38;5;241m.\u001b[39matt_idx_to_name[i]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 7\u001b[0m     mini_model_i \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_for_attribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[1;32m      9\u001b[0m     models\u001b[38;5;241m.\u001b[39mappend(mini_model_i)\n",
      "Input \u001b[0;32mIn [69]\u001b[0m, in \u001b[0;36mtrain_for_attribute\u001b[0;34m(att_index)\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m3\u001b[39m):\n\u001b[1;32m     18\u001b[0m     running_loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0.0\u001b[39m\n\u001b[0;32m---> 19\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m batch \u001b[38;5;129;01min\u001b[39;00m train_ld:\n\u001b[1;32m     20\u001b[0m         x, y, z  \u001b[38;5;241m=\u001b[39m batch\n\u001b[1;32m     22\u001b[0m         x, z \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mto(device), z\u001b[38;5;241m.\u001b[39mto(device)\n",
      "File \u001b[0;32m~/.local/share/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:652\u001b[0m, in \u001b[0;36m_BaseDataLoaderIter.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    649\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_sampler_iter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    650\u001b[0m     \u001b[38;5;66;03m# TODO(https://github.com/pytorch/pytorch/issues/76750)\u001b[39;00m\n\u001b[1;32m    651\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reset()  \u001b[38;5;66;03m# type: ignore[call-arg]\u001b[39;00m\n\u001b[0;32m--> 652\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_next_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    653\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m    654\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    655\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \\\n\u001b[1;32m    656\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_yielded \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_IterableDataset_len_called:\n",
      "File \u001b[0;32m~/.local/share/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1330\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1327\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_process_data(data)\n\u001b[1;32m   1329\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_shutdown \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m-> 1330\u001b[0m idx, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1331\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_tasks_outstanding \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n\u001b[1;32m   1332\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_dataset_kind \u001b[38;5;241m==\u001b[39m _DatasetKind\u001b[38;5;241m.\u001b[39mIterable:\n\u001b[1;32m   1333\u001b[0m     \u001b[38;5;66;03m# Check for _IterableDatasetStopIteration\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1296\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._get_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1292\u001b[0m     \u001b[38;5;66;03m# In this case, `self._data_queue` is a `queue.Queue`,. But we don't\u001b[39;00m\n\u001b[1;32m   1293\u001b[0m     \u001b[38;5;66;03m# need to call `.task_done()` because we don't use `.join()`.\u001b[39;00m\n\u001b[1;32m   1294\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1295\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m-> 1296\u001b[0m         success, data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_try_get_data\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1297\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m success:\n\u001b[1;32m   1298\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m data\n",
      "File \u001b[0;32m~/.local/share/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/utils/data/dataloader.py:1134\u001b[0m, in \u001b[0;36m_MultiProcessingDataLoaderIter._try_get_data\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m   1121\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_try_get_data\u001b[39m(\u001b[38;5;28mself\u001b[39m, timeout\u001b[38;5;241m=\u001b[39m_utils\u001b[38;5;241m.\u001b[39mMP_STATUS_CHECK_INTERVAL):\n\u001b[1;32m   1122\u001b[0m     \u001b[38;5;66;03m# Tries to fetch data from `self._data_queue` once for a given timeout.\u001b[39;00m\n\u001b[1;32m   1123\u001b[0m     \u001b[38;5;66;03m# This can also be used as inner loop of fetching without timeout, with\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m   1131\u001b[0m     \u001b[38;5;66;03m# Returns a 2-tuple:\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m     \u001b[38;5;66;03m#   (bool: whether successfully get data, any: data if successful else None)\u001b[39;00m\n\u001b[1;32m   1133\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1134\u001b[0m         data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_data_queue\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1135\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m (\u001b[38;5;28;01mTrue\u001b[39;00m, data)\n\u001b[1;32m   1136\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m   1137\u001b[0m         \u001b[38;5;66;03m# At timeout and error, we manually check whether any worker has\u001b[39;00m\n\u001b[1;32m   1138\u001b[0m         \u001b[38;5;66;03m# failed. Note that this is the only mechanism for Windows to detect\u001b[39;00m\n\u001b[1;32m   1139\u001b[0m         \u001b[38;5;66;03m# worker failures.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/anaconda3/envs/pytorch/lib/python3.9/multiprocessing/queues.py:122\u001b[0m, in \u001b[0;36mQueue.get\u001b[0;34m(self, block, timeout)\u001b[0m\n\u001b[1;32m    120\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_rlock\u001b[38;5;241m.\u001b[39mrelease()\n\u001b[1;32m    121\u001b[0m \u001b[38;5;66;03m# unserialize the data after having released the lock\u001b[39;00m\n\u001b[0;32m--> 122\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_ForkingPickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mloads\u001b[49m\u001b[43m(\u001b[49m\u001b[43mres\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/multiprocessing/reductions.py:297\u001b[0m, in \u001b[0;36mrebuild_storage_fd\u001b[0;34m(cls, df, size)\u001b[0m\n\u001b[1;32m    296\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mrebuild_storage_fd\u001b[39m(\u001b[38;5;28mcls\u001b[39m, df, size):\n\u001b[0;32m--> 297\u001b[0m     fd \u001b[38;5;241m=\u001b[39m \u001b[43mdf\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdetach\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    298\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    299\u001b[0m         storage \u001b[38;5;241m=\u001b[39m storage_from_cache(\u001b[38;5;28mcls\u001b[39m, fd_id(fd))\n",
      "File \u001b[0;32m~/.local/share/anaconda3/envs/pytorch/lib/python3.9/multiprocessing/resource_sharer.py:57\u001b[0m, in \u001b[0;36mDupFd.detach\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mdetach\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;124;03m'''Get the fd.  This should only be called once.'''\u001b[39;00m\n\u001b[0;32m---> 57\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43m_resource_sharer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget_connection\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_id\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m conn:\n\u001b[1;32m     58\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m reduction\u001b[38;5;241m.\u001b[39mrecv_handle(conn)\n",
      "File \u001b[0;32m~/.local/share/anaconda3/envs/pytorch/lib/python3.9/multiprocessing/resource_sharer.py:86\u001b[0m, in \u001b[0;36m_ResourceSharer.get_connection\u001b[0;34m(ident)\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mconnection\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m Client\n\u001b[1;32m     85\u001b[0m address, key \u001b[38;5;241m=\u001b[39m ident\n\u001b[0;32m---> 86\u001b[0m c \u001b[38;5;241m=\u001b[39m \u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43maddress\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mprocess\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_process\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mauthkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     87\u001b[0m c\u001b[38;5;241m.\u001b[39msend((key, os\u001b[38;5;241m.\u001b[39mgetpid()))\n\u001b[1;32m     88\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m c\n",
      "File \u001b[0;32m~/.local/share/anaconda3/envs/pytorch/lib/python3.9/multiprocessing/connection.py:514\u001b[0m, in \u001b[0;36mClient\u001b[0;34m(address, family, authkey)\u001b[0m\n\u001b[1;32m    512\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m authkey \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    513\u001b[0m     answer_challenge(c, authkey)\n\u001b[0;32m--> 514\u001b[0m     \u001b[43mdeliver_challenge\u001b[49m\u001b[43m(\u001b[49m\u001b[43mc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mauthkey\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    516\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m c\n",
      "File \u001b[0;32m~/.local/share/anaconda3/envs/pytorch/lib/python3.9/multiprocessing/connection.py:744\u001b[0m, in \u001b[0;36mdeliver_challenge\u001b[0;34m(connection, authkey)\u001b[0m\n\u001b[1;32m    742\u001b[0m message \u001b[38;5;241m=\u001b[39m os\u001b[38;5;241m.\u001b[39murandom(MESSAGE_LENGTH)\n\u001b[1;32m    743\u001b[0m connection\u001b[38;5;241m.\u001b[39msend_bytes(CHALLENGE \u001b[38;5;241m+\u001b[39m message)\n\u001b[0;32m--> 744\u001b[0m digest \u001b[38;5;241m=\u001b[39m \u001b[43mhmac\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnew\u001b[49m\u001b[43m(\u001b[49m\u001b[43mauthkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmessage\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mmd5\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mdigest()\n\u001b[1;32m    745\u001b[0m response \u001b[38;5;241m=\u001b[39m connection\u001b[38;5;241m.\u001b[39mrecv_bytes(\u001b[38;5;241m256\u001b[39m)        \u001b[38;5;66;03m# reject large message\u001b[39;00m\n\u001b[1;32m    746\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m response \u001b[38;5;241m==\u001b[39m digest:\n",
      "File \u001b[0;32m~/.local/share/anaconda3/envs/pytorch/lib/python3.9/hmac.py:170\u001b[0m, in \u001b[0;36mnew\u001b[0;34m(key, msg, digestmod)\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnew\u001b[39m(key, msg\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, digestmod\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    154\u001b[0m     \u001b[38;5;124;03m\"\"\"Create a new hashing object and return it.\u001b[39;00m\n\u001b[1;32m    155\u001b[0m \n\u001b[1;32m    156\u001b[0m \u001b[38;5;124;03m    key: bytes or buffer, The starting key for the hash.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    168\u001b[0m \u001b[38;5;124;03m    or hexdigest() methods.\u001b[39;00m\n\u001b[1;32m    169\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 170\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mHMAC\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmsg\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdigestmod\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/anaconda3/envs/pytorch/lib/python3.9/hmac.py:90\u001b[0m, in \u001b[0;36mHMAC.__init__\u001b[0;34m(self, key, msg, digestmod)\u001b[0m\n\u001b[1;32m     87\u001b[0m     key \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_digest_cons(key)\u001b[38;5;241m.\u001b[39mdigest()\n\u001b[1;32m     89\u001b[0m key \u001b[38;5;241m=\u001b[39m key\u001b[38;5;241m.\u001b[39mljust(blocksize, \u001b[38;5;124mb\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\0\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m---> 90\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_outer\u001b[38;5;241m.\u001b[39mupdate(\u001b[43mkey\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranslate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrans_5C\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner\u001b[38;5;241m.\u001b[39mupdate(key\u001b[38;5;241m.\u001b[39mtranslate(trans_36))\n\u001b[1;32m     92\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m msg \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "models = []\n",
    "\n",
    "awads = AwA(root=root)\n",
    "    \n",
    "for i in range(85):\n",
    "    print(f\"Training model for {i} -> {awads.att_idx_to_name[i]}\")\n",
    "    mini_model_i = train_for_attribute(i)\n",
    "    model.eval()\n",
    "    models.append(mini_model_i)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a6f998b0-2190-4c3b-b45c-574a6bfca0f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_ood.api import Detector\n",
    "from typing import List \n",
    "\n",
    "class EnsembleDetector(Detector):\n",
    "    \"\"\"\n",
    "    Ensemble of several OOD detectors \n",
    "    \"\"\"\n",
    "    \n",
    "    def __init__(self, detectors: List[Detector]):\n",
    "        self.detectors = detectors\n",
    "    \n",
    "    def predict(self, x):\n",
    "        with torch.no_grad():\n",
    "            scores = [d(x) for d in self.detectors]\n",
    "            # scores = [l.softmax(dim=1).max(dim=1).values for l in scores]\n",
    "            scores = torch.stack(scores, dim=1).mean(dim=1)\n",
    "            # scores = torch.stack(scores, dim=1).mean(dim=1)\n",
    "        return scores\n",
    "    \n",
    "    def fit():\n",
    "        pass \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7bca5b25-4230-477d-a9ae-4b0d56d2ff6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_ood.dataset.img import Textures, FractalDataset, FoolingImages, ImageNetA, ImageNetO, ImageNetR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "91259ea5-9517-47ae-965d-ed1d313492f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pytorch_ood\n",
    "from pytorch_ood.utils import ToRGB\n",
    "from pytorch_ood.detector import MaxSoftmax, EnergyBased, Mahalanobis\n",
    "from pytorch_ood.utils import OODMetrics, ToRGB, ToUnknown\n",
    "from pandas import DataFrame\n",
    "\n",
    "\n",
    "def ood_label(x):\n",
    "    if x == True:\n",
    "        return \"Normal\"\n",
    "    else:\n",
    "        return \"Anomaly\"\n",
    "\n",
    "\n",
    "def eval_on_loader(detector, loader):\n",
    "    scores = []\n",
    "    ys = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for x, y in loader:\n",
    "            x = x.to(device)\n",
    "            scores.append(detector(x).cpu())\n",
    "            ys.append(y)\n",
    "\n",
    "    scores = torch.cat(scores)\n",
    "    ys = torch.cat(ys)\n",
    "    return scores, ys\n",
    "\n",
    "    \n",
    "def evaluate(detector):\n",
    "    test_ds = load_dataset_features(\"awa\", root, subset=\"test\", full=False)\n",
    "    \n",
    "    metrics = []\n",
    "    \n",
    "    for dataset in [\"textures\", \"fractals\", \"fooling\", \"imagenet-a\", \"imagenet-o\", \"imagenet-r\"]:\n",
    "        print(f\"--> {dataset}\")\n",
    "        dataset_out_test = load_dataset_features(dataset, root, subset=\"test\")\n",
    "        loader = DataLoader(dataset_out_test +  test_ds, batch_size=16, shuffle=False, num_workers=10)\n",
    "\n",
    "        scores, ys = eval_on_loader(detector, loader)\n",
    "\n",
    "        m = OODMetrics()\n",
    "        m.update(scores, ys)\n",
    "\n",
    "        met = m.compute()\n",
    "        met[\"Dataset\"] = dataset\n",
    "        metrics.append(met)\n",
    "    \n",
    "    return metrics "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fae469d7-9940-49f4-b87e-f9786ab15c12",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> MaxSoftmax\n",
      "Loading awa-test\n",
      "--> textures\n",
      "Loading textures-test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1c41ad0eec234dc3bc066027b051e496",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/89 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> fractals\n",
      "Loading fractals-test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8db5744d90bc46848981ddada08449ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/223 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ki/.local/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:793: UserWarning: Corrupt EXIF data.  Expecting to read 4 bytes but only got 0. \n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> fooling\n",
      "Loading fooling-test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "889554ade84c423b84accf2b17c3448d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/157 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> imagenet-a\n",
      "Loading imagenet-a-test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "87a9cb07ffd14199af8fd4a38e324d87",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/117 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> imagenet-o\n",
      "Loading imagenet-o-test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f92980a66ae848fdb19a6b66ce797156",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/32 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ki/.local/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:771: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 4. Skipping tag 37381\n",
      "  warnings.warn(\n",
      "/home/ki/.local/lib/python3.9/site-packages/PIL/TiffImagePlugin.py:771: UserWarning: Possibly corrupt EXIF data.  Expecting to read 8 bytes but only got 0. Skipping tag 37386\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--> imagenet-r\n",
      "Loading imagenet-r-test\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f1bdd4f627cd4a5e99cc03523798fc0c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/469 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> EnergyBased\n",
      "Loading awa-test\n",
      "--> textures\n",
      "Loading textures-test\n",
      "--> fractals\n",
      "Loading fractals-test\n",
      "--> fooling\n",
      "Loading fooling-test\n",
      "--> imagenet-a\n",
      "Loading imagenet-a-test\n",
      "--> imagenet-o\n",
      "Loading imagenet-o-test\n",
      "--> imagenet-r\n",
      "Loading imagenet-r-test\n",
      "-> Ensemble\n",
      "Loading awa-test\n",
      "--> textures\n",
      "Loading textures-test\n",
      "--> fractals\n",
      "Loading fractals-test\n",
      "--> fooling\n",
      "Loading fooling-test\n",
      "--> imagenet-a\n",
      "Loading imagenet-a-test\n",
      "--> imagenet-o\n",
      "Loading imagenet-o-test\n",
      "--> imagenet-r\n",
      "Loading imagenet-r-test\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "seed_rep = 0\n",
    "\n",
    "from pytorch_ood.detector import MaxSoftmax, EnergyBased\n",
    "\n",
    "ensemble_detectors = [MaxSoftmax(m) for m in models]\n",
    "\n",
    "detectors = {\n",
    "    \"MaxSoftmax\": MaxSoftmax(mini_model),\n",
    "    \"EnergyBased\": EnergyBased(mini_model), \n",
    "    \"Ensemble\": EnsembleDetector(ensemble_detectors + [MaxSoftmax(mini_model)])\n",
    "}\n",
    "\n",
    "for name, detector in detectors.items():\n",
    "    print(f\"-> {name}\")\n",
    "    metrics = evaluate(detector)\n",
    "    for m in metrics:\n",
    "        m.update({\n",
    "            \"Method\": name,\n",
    "            \"Rep\": seed_rep\n",
    "        })\n",
    "    results += metrics\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "48db2cf3-1013-41fb-b45f-90bd64220c30",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "{} & AUROC & AUPR-IN & AUPR-OUT & FPR95TPR \\\\\n",
      "{} &  mean &    mean &     mean &     mean \\\\\n",
      "Method      &       &         &          &          \\\\\n",
      "\\midrule\n",
      "EnergyBased & 98.68 &   98.35 &    98.46 &     5.61 \\\\\n",
      "Ensemble    & 92.21 &   86.56 &    93.14 &    21.12 \\\\\n",
      "MaxSoftmax  & 97.70 &   97.05 &    97.32 &    10.06 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print((result_df.groupby(by=[\"Method\"]).agg([\"mean\"]) * 100)[[\"AUROC\", \"AUPR-IN\", \"AUPR-OUT\", \"FPR95TPR\"]].to_latex(float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6d433e0-d841-4d27-8a8d-ad44228106ee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUROC</th>\n",
       "      <th>AUPR-IN</th>\n",
       "      <th>AUPR-OUT</th>\n",
       "      <th>ACC95TPR</th>\n",
       "      <th>FPR95TPR</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Method</th>\n",
       "      <th>Rep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.969938</td>\n",
       "      <td>0.958632</td>\n",
       "      <td>0.976465</td>\n",
       "      <td>0.903564</td>\n",
       "      <td>0.132204</td>\n",
       "      <td>textures</td>\n",
       "      <td>MaxSoftmax</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.978644</td>\n",
       "      <td>0.987127</td>\n",
       "      <td>0.966914</td>\n",
       "      <td>0.935744</td>\n",
       "      <td>0.092051</td>\n",
       "      <td>fractals</td>\n",
       "      <td>MaxSoftmax</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.971794</td>\n",
       "      <td>0.976565</td>\n",
       "      <td>0.964718</td>\n",
       "      <td>0.918774</td>\n",
       "      <td>0.123873</td>\n",
       "      <td>fooling</td>\n",
       "      <td>MaxSoftmax</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.980464</td>\n",
       "      <td>0.978153</td>\n",
       "      <td>0.982724</td>\n",
       "      <td>0.933770</td>\n",
       "      <td>0.082901</td>\n",
       "      <td>imagenet-a</td>\n",
       "      <td>MaxSoftmax</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.978154</td>\n",
       "      <td>0.927726</td>\n",
       "      <td>0.993704</td>\n",
       "      <td>0.912787</td>\n",
       "      <td>0.097378</td>\n",
       "      <td>imagenet-o</td>\n",
       "      <td>MaxSoftmax</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.982902</td>\n",
       "      <td>0.995073</td>\n",
       "      <td>0.954836</td>\n",
       "      <td>0.945073</td>\n",
       "      <td>0.075116</td>\n",
       "      <td>imagenet-r</td>\n",
       "      <td>MaxSoftmax</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.980357</td>\n",
       "      <td>0.973539</td>\n",
       "      <td>0.984403</td>\n",
       "      <td>0.931415</td>\n",
       "      <td>0.082901</td>\n",
       "      <td>textures</td>\n",
       "      <td>EnergyBased</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.988655</td>\n",
       "      <td>0.993171</td>\n",
       "      <td>0.982605</td>\n",
       "      <td>0.951043</td>\n",
       "      <td>0.046982</td>\n",
       "      <td>fractals</td>\n",
       "      <td>EnergyBased</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.981863</td>\n",
       "      <td>0.985344</td>\n",
       "      <td>0.976183</td>\n",
       "      <td>0.938575</td>\n",
       "      <td>0.077028</td>\n",
       "      <td>fooling</td>\n",
       "      <td>EnergyBased</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.989205</td>\n",
       "      <td>0.988422</td>\n",
       "      <td>0.990036</td>\n",
       "      <td>0.950801</td>\n",
       "      <td>0.048484</td>\n",
       "      <td>imagenet-a</td>\n",
       "      <td>EnergyBased</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.988625</td>\n",
       "      <td>0.962496</td>\n",
       "      <td>0.996705</td>\n",
       "      <td>0.952478</td>\n",
       "      <td>0.046845</td>\n",
       "      <td>imagenet-o</td>\n",
       "      <td>EnergyBased</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.992052</td>\n",
       "      <td>0.997803</td>\n",
       "      <td>0.977532</td>\n",
       "      <td>0.953084</td>\n",
       "      <td>0.034280</td>\n",
       "      <td>imagenet-r</td>\n",
       "      <td>EnergyBased</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.914542</td>\n",
       "      <td>0.843862</td>\n",
       "      <td>0.946666</td>\n",
       "      <td>0.838991</td>\n",
       "      <td>0.246517</td>\n",
       "      <td>textures</td>\n",
       "      <td>Ensemble</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.922397</td>\n",
       "      <td>0.932837</td>\n",
       "      <td>0.912135</td>\n",
       "      <td>0.896477</td>\n",
       "      <td>0.207730</td>\n",
       "      <td>fractals</td>\n",
       "      <td>Ensemble</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.912421</td>\n",
       "      <td>0.898964</td>\n",
       "      <td>0.917671</td>\n",
       "      <td>0.868145</td>\n",
       "      <td>0.243649</td>\n",
       "      <td>fooling</td>\n",
       "      <td>Ensemble</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.930860</td>\n",
       "      <td>0.891307</td>\n",
       "      <td>0.948840</td>\n",
       "      <td>0.882071</td>\n",
       "      <td>0.187380</td>\n",
       "      <td>imagenet-a</td>\n",
       "      <td>Ensemble</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.924093</td>\n",
       "      <td>0.659408</td>\n",
       "      <td>0.980448</td>\n",
       "      <td>0.836516</td>\n",
       "      <td>0.194482</td>\n",
       "      <td>imagenet-o</td>\n",
       "      <td>Ensemble</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.928131</td>\n",
       "      <td>0.967396</td>\n",
       "      <td>0.882761</td>\n",
       "      <td>0.923048</td>\n",
       "      <td>0.187380</td>\n",
       "      <td>imagenet-r</td>\n",
       "      <td>Ensemble</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       AUROC   AUPR-IN  AUPR-OUT  ACC95TPR  FPR95TPR     Dataset       Method  \\\n",
       "0   0.969938  0.958632  0.976465  0.903564  0.132204    textures   MaxSoftmax   \n",
       "1   0.978644  0.987127  0.966914  0.935744  0.092051    fractals   MaxSoftmax   \n",
       "2   0.971794  0.976565  0.964718  0.918774  0.123873     fooling   MaxSoftmax   \n",
       "3   0.980464  0.978153  0.982724  0.933770  0.082901  imagenet-a   MaxSoftmax   \n",
       "4   0.978154  0.927726  0.993704  0.912787  0.097378  imagenet-o   MaxSoftmax   \n",
       "5   0.982902  0.995073  0.954836  0.945073  0.075116  imagenet-r   MaxSoftmax   \n",
       "6   0.980357  0.973539  0.984403  0.931415  0.082901    textures  EnergyBased   \n",
       "7   0.988655  0.993171  0.982605  0.951043  0.046982    fractals  EnergyBased   \n",
       "8   0.981863  0.985344  0.976183  0.938575  0.077028     fooling  EnergyBased   \n",
       "9   0.989205  0.988422  0.990036  0.950801  0.048484  imagenet-a  EnergyBased   \n",
       "10  0.988625  0.962496  0.996705  0.952478  0.046845  imagenet-o  EnergyBased   \n",
       "11  0.992052  0.997803  0.977532  0.953084  0.034280  imagenet-r  EnergyBased   \n",
       "12  0.914542  0.843862  0.946666  0.838991  0.246517    textures     Ensemble   \n",
       "13  0.922397  0.932837  0.912135  0.896477  0.207730    fractals     Ensemble   \n",
       "14  0.912421  0.898964  0.917671  0.868145  0.243649     fooling     Ensemble   \n",
       "15  0.930860  0.891307  0.948840  0.882071  0.187380  imagenet-a     Ensemble   \n",
       "16  0.924093  0.659408  0.980448  0.836516  0.194482  imagenet-o     Ensemble   \n",
       "17  0.928131  0.967396  0.882761  0.923048  0.187380  imagenet-r     Ensemble   \n",
       "\n",
       "    Rep  \n",
       "0     0  \n",
       "1     0  \n",
       "2     0  \n",
       "3     0  \n",
       "4     0  \n",
       "5     0  \n",
       "6     0  \n",
       "7     0  \n",
       "8     0  \n",
       "9     0  \n",
       "10    0  \n",
       "11    0  \n",
       "12    0  \n",
       "13    0  \n",
       "14    0  \n",
       "15    0  \n",
       "16    0  \n",
       "17    0  "
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "9931379c-b41a-4bb4-8295-18e5fb84a0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SemanticDetector(Detector):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\" \n",
    "    \n",
    "    def __init__(self, label_model, att_models: List[nn.Module], target_vectors, n=84):\n",
    "        \"\"\"\n",
    "        Models and target vectors \n",
    "        \"\"\"\n",
    "        self.label_model = label_model \n",
    "        self.att_model = att_models \n",
    "        self.target_vectors = target_vectors \n",
    "        self.n = n\n",
    "    \n",
    "    def predict(self, x):\n",
    "        values, labels = self.label_model(x).softmax(dim=1).max(dim=1)\n",
    "        outputs = [m(x).max(dim=1).indices for m in models]\n",
    "        att_vectors = torch.stack(outputs, dim=1).cpu()\n",
    "        \n",
    "        sat = []\n",
    "        \n",
    "        for l, att_vector in zip(labels, att_vectors):\n",
    "            s = att_vector == target_vectors[l]\n",
    "            sat.append(s.sum() >= self.n)\n",
    "        \n",
    "        sat = torch.tensor(sat).float()\n",
    "        # print(sat.sum())\n",
    "        return - values.cpu() * sat\n",
    "    \n",
    "    def fit():\n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "51dc4bd9-d2cf-4996-a819-149d07299d7e",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true,
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d9b7322551864f4aa485ccb45c5d366c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/86 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> 0\n",
      "Loading awa-test\n",
      "--> textures\n",
      "Loading textures-test\n",
      "--> fractals\n",
      "Loading fractals-test\n",
      "--> fooling\n",
      "Loading fooling-test\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [40]\u001b[0m, in \u001b[0;36m<cell line: 8>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m n, detector \u001b[38;5;129;01min\u001b[39;00m tqdm(\u001b[38;5;28menumerate\u001b[39m(detectors), total\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mlen\u001b[39m(detectors)):\n\u001b[1;32m      9\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-> \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mn\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 10\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdetector\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m m \u001b[38;5;129;01min\u001b[39;00m metrics:\n\u001b[1;32m     12\u001b[0m         m\u001b[38;5;241m.\u001b[39mupdate({\n\u001b[1;32m     13\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDetector\u001b[39m\u001b[38;5;124m\"\u001b[39m: n,\n\u001b[1;32m     14\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mRep\u001b[39m\u001b[38;5;124m\"\u001b[39m: seed_rep\n\u001b[1;32m     15\u001b[0m         })\n",
      "Input \u001b[0;32mIn [10]\u001b[0m, in \u001b[0;36mevaluate\u001b[0;34m(detector)\u001b[0m\n\u001b[1;32m     42\u001b[0m m \u001b[38;5;241m=\u001b[39m OODMetrics()\n\u001b[1;32m     43\u001b[0m m\u001b[38;5;241m.\u001b[39mupdate(scores, ys)\n\u001b[0;32m---> 45\u001b[0m met \u001b[38;5;241m=\u001b[39m \u001b[43mm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     46\u001b[0m met[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m dataset\n\u001b[1;32m     47\u001b[0m metrics\u001b[38;5;241m.\u001b[39mappend(met)\n",
      "File \u001b[0;32m~/.local/share/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_ood/utils/metrics.py:189\u001b[0m, in \u001b[0;36mOODMetrics.compute\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    186\u001b[0m aupr_out \u001b[38;5;241m=\u001b[39m torchmetrics\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mauc(r, p)\n\u001b[1;32m    188\u001b[0m acc \u001b[38;5;241m=\u001b[39m accuracy_at_tpr(scores, labels)\n\u001b[0;32m--> 189\u001b[0m fpr \u001b[38;5;241m=\u001b[39m \u001b[43mfpr_at_tpr\u001b[49m\u001b[43m(\u001b[49m\u001b[43mscores\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m    192\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAUROC\u001b[39m\u001b[38;5;124m\"\u001b[39m: auroc\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    193\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAUPR-IN\u001b[39m\u001b[38;5;124m\"\u001b[39m: aupr_in\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    196\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFPR95TPR\u001b[39m\u001b[38;5;124m\"\u001b[39m: fpr\u001b[38;5;241m.\u001b[39mitem(),\n\u001b[1;32m    197\u001b[0m }\n",
      "File \u001b[0;32m~/.local/share/anaconda3/envs/pytorch/lib/python3.9/site-packages/pytorch_ood/utils/metrics.py:88\u001b[0m, in \u001b[0;36mfpr_at_tpr\u001b[0;34m(pred, target, k)\u001b[0m\n\u001b[1;32m     86\u001b[0m fpr, tpr, thresholds \u001b[38;5;241m=\u001b[39m torchmetrics\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39mroc(pred, target)\n\u001b[1;32m     87\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fp, tp, t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(fpr, tpr, thresholds):\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mtp\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m>\u001b[39;49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m:\n\u001b[1;32m     89\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m fp\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mtensor(\u001b[38;5;241m1.0\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "results = []\n",
    "seed_rep = 0\n",
    "\n",
    "from pytorch_ood.detector import MaxSoftmax, EnergyBased\n",
    "\n",
    "detectors = [MaxSoftmax(m) for m in models] + [MaxSoftmax(mini_model)]\n",
    "\n",
    "for n, detector in tqdm(enumerate(detectors), total=len(detectors)):\n",
    "    print(f\"-> {n}\")\n",
    "    metrics = evaluate(detector)\n",
    "    for m in metrics:\n",
    "        m.update({\n",
    "            \"Detector\": n,\n",
    "            \"Rep\": seed_rep\n",
    "        })\n",
    "    results += metrics\n",
    "\n",
    "result_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "15c4c29b-ca6d-434c-9fb1-6e2524eb2087",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrr}\n",
      "\\toprule\n",
      "{} & AUROC & AUPR-IN & AUPR-OUT & FPR95TPR \\\\\n",
      "{} &  mean &    mean &     mean &     mean \\\\\n",
      "Detector &       &         &          &          \\\\\n",
      "\\midrule\n",
      "0        & 70.22 &   66.92 &    65.93 &    78.82 \\\\\n",
      "1        & 72.76 &   68.87 &    68.64 &    74.81 \\\\\n",
      "2        & 58.68 &   56.50 &    53.90 &    89.83 \\\\\n",
      "3        & 72.14 &   68.68 &    67.23 &    77.16 \\\\\n",
      "4        & 75.61 &   71.54 &    71.89 &    70.00 \\\\\n",
      "5        & 55.06 &   54.89 &    51.32 &    91.83 \\\\\n",
      "6        & 49.52 &   51.54 &    47.60 &    93.46 \\\\\n",
      "7        & 57.28 &   57.92 &    52.70 &    91.08 \\\\\n",
      "8        & 68.40 &   65.54 &    63.21 &    80.91 \\\\\n",
      "9        & 60.01 &   59.50 &    55.57 &    88.69 \\\\\n",
      "10       & 54.31 &   56.73 &    49.33 &    93.85 \\\\\n",
      "11       & 67.18 &   64.40 &    61.80 &    83.25 \\\\\n",
      "12       & 69.88 &   66.63 &    65.10 &    79.30 \\\\\n",
      "13       & 80.73 &   75.68 &    78.53 &    58.54 \\\\\n",
      "14       & 75.07 &   72.34 &    69.35 &    76.16 \\\\\n",
      "15       & 76.56 &   72.12 &    74.18 &    65.79 \\\\\n",
      "16       & 75.46 &   70.81 &    73.20 &    67.09 \\\\\n",
      "17       & 74.63 &   70.18 &    71.81 &    69.46 \\\\\n",
      "18       & 64.97 &   62.45 &    58.91 &    86.45 \\\\\n",
      "19       & 56.03 &   57.61 &    49.73 &    94.48 \\\\\n",
      "20       & 81.08 &   77.46 &    77.16 &    63.72 \\\\\n",
      "21       & 67.23 &   64.80 &    63.92 &    79.37 \\\\\n",
      "22       & 85.73 &   81.63 &    82.88 &    50.89 \\\\\n",
      "23       & 75.94 &   72.23 &    72.06 &    71.37 \\\\\n",
      "24       & 59.56 &   59.55 &    53.77 &    91.27 \\\\\n",
      "25       & 64.77 &   63.06 &    58.44 &    87.44 \\\\\n",
      "26       & 57.67 &   57.89 &    52.93 &    91.47 \\\\\n",
      "27       & 79.51 &   74.54 &    77.42 &    60.12 \\\\\n",
      "28       & 63.92 &   63.01 &    56.91 &    89.89 \\\\\n",
      "29       & 57.08 &   54.85 &    53.46 &    90.01 \\\\\n",
      "30       & 62.15 &   60.24 &    57.97 &    86.53 \\\\\n",
      "31       & 81.73 &   76.86 &    78.65 &    59.04 \\\\\n",
      "32       & 54.64 &   53.91 &    51.95 &    91.03 \\\\\n",
      "33       & 74.89 &   70.94 &    71.63 &    70.85 \\\\\n",
      "34       & 52.56 &   55.80 &    46.22 &    97.52 \\\\\n",
      "35       & 58.14 &   56.14 &    53.57 &    90.96 \\\\\n",
      "36       & 68.55 &   65.59 &    62.63 &    83.07 \\\\\n",
      "37       & 54.78 &   54.28 &    50.35 &    93.55 \\\\\n",
      "38       & 65.49 &   62.16 &    60.91 &    83.81 \\\\\n",
      "39       & 65.22 &   62.24 &    61.56 &    81.80 \\\\\n",
      "40       & 74.51 &   70.55 &    69.98 &    72.05 \\\\\n",
      "41       & 73.94 &   71.42 &    67.56 &    78.58 \\\\\n",
      "42       & 61.49 &   60.88 &    55.30 &    90.00 \\\\\n",
      "43       & 71.97 &   67.84 &    68.98 &    73.35 \\\\\n",
      "44       & 64.58 &   62.71 &    59.75 &    85.27 \\\\\n",
      "45       & 63.10 &   61.36 &    56.32 &    89.56 \\\\\n",
      "46       & 66.59 &   63.85 &    62.04 &    82.09 \\\\\n",
      "47       & 71.51 &   67.84 &    67.30 &    75.49 \\\\\n",
      "48       & 60.64 &   59.80 &    56.74 &    87.63 \\\\\n",
      "49       & 62.47 &   60.60 &    58.87 &    85.12 \\\\\n",
      "50       & 71.73 &   68.05 &    67.71 &    75.88 \\\\\n",
      "51       & 74.89 &   71.03 &    71.87 &    70.15 \\\\\n",
      "52       & 77.54 &   73.29 &    74.32 &    66.51 \\\\\n",
      "53       & 54.82 &   55.17 &    48.96 &    94.79 \\\\\n",
      "54       & 82.57 &   77.63 &    80.70 &    54.34 \\\\\n",
      "55       & 54.06 &   53.41 &    50.16 &    93.19 \\\\\n",
      "56       & 79.61 &   74.98 &    77.16 &    61.27 \\\\\n",
      "57       & 83.77 &   79.04 &    81.88 &    52.20 \\\\\n",
      "58       & 70.74 &   66.51 &    68.51 &    73.30 \\\\\n",
      "59       & 53.59 &   52.86 &    50.97 &    91.57 \\\\\n",
      "60       & 52.91 &   54.41 &    47.30 &    95.85 \\\\\n",
      "61       & 66.20 &   63.01 &    62.19 &    81.87 \\\\\n",
      "62       & 65.74 &   63.55 &    62.33 &    81.60 \\\\\n",
      "63       & 55.40 &   55.38 &    51.44 &    91.43 \\\\\n",
      "64       & 58.17 &   57.56 &    54.29 &    89.25 \\\\\n",
      "65       & 62.21 &   58.90 &    58.36 &    85.65 \\\\\n",
      "66       & 48.77 &   51.97 &    46.31 &    94.91 \\\\\n",
      "67       & 68.96 &   66.78 &    63.36 &    82.21 \\\\\n",
      "68       & 82.34 &   77.46 &    80.76 &    54.03 \\\\\n",
      "69       & 73.63 &   70.25 &    69.12 &    75.03 \\\\\n",
      "70       & 80.73 &   75.94 &    78.65 &    58.70 \\\\\n",
      "71       & 68.76 &   67.14 &    62.06 &    83.97 \\\\\n",
      "72       & 65.85 &   64.83 &    58.89 &    87.83 \\\\\n",
      "73       & 64.98 &   62.45 &    59.74 &    85.27 \\\\\n",
      "74       & 64.30 &   61.17 &    59.31 &    85.96 \\\\\n",
      "75       & 69.56 &   66.67 &    63.76 &    81.82 \\\\\n",
      "76       & 61.38 &   59.19 &    58.26 &    85.32 \\\\\n",
      "77       & 53.83 &   54.56 &    49.84 &    93.06 \\\\\n",
      "78       & 74.95 &   70.75 &    71.80 &    68.85 \\\\\n",
      "79       & 70.60 &   67.81 &    65.44 &    79.39 \\\\\n",
      "80       & 65.40 &   62.79 &    60.88 &    83.29 \\\\\n",
      "81       & 70.38 &   67.32 &    65.80 &    79.38 \\\\\n",
      "82       & 75.35 &   70.76 &    72.21 &    69.42 \\\\\n",
      "83       & 74.84 &   70.79 &    70.84 &    72.04 \\\\\n",
      "84       & 73.42 &   69.31 &    69.68 &    72.81 \\\\\n",
      "85       & 97.70 &   97.05 &    97.32 &    10.06 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print((result_df.groupby(by=[\"Detector\"]).agg([\"mean\"]) * 100)[[\"AUROC\", \"AUPR-IN\", \"AUPR-OUT\", \"FPR95TPR\"]].to_latex(float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "14fd2158-6ddc-4427-b97d-2a3bc8cb734f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 0, 0,  ..., 0, 0, 0],\n",
       "        [1, 0, 0,  ..., 1, 0, 0],\n",
       "        [1, 1, 0,  ..., 0, 0, 0],\n",
       "        ...,\n",
       "        [1, 1, 0,  ..., 1, 1, 0],\n",
       "        [1, 1, 0,  ..., 0, 0, 1],\n",
       "        [0, 1, 1,  ..., 0, 0, 1]])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "target_vectors = torch.tensor(awads.predicates.values).long()\n",
    "target_vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "f1878707-9e52-4043-a289-35544f11a1e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = SemanticDetector(mini_model, models, target_vectors=target_vectors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d1982b63-2485-444c-88b4-42b84ce4d53f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor(8.)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([False,  True, False,  True,  True,  True,  True, False,  True,  True,\n",
       "        False, False, False,  True,  True,  True,  True, False,  True,  True,\n",
       "         True, False,  True,  True,  True,  True])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "detector(x) == 0.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "db061a33-825e-4527-967e-98520ce31ccb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-> Semantic\n",
      "Loading awa-test\n",
      "--> textures\n",
      "Loading textures-test\n",
      "--> fractals\n",
      "Loading fractals-test\n",
      "--> fooling\n",
      "Loading fooling-test\n",
      "--> imagenet-a\n",
      "Loading imagenet-a-test\n",
      "--> imagenet-o\n",
      "Loading imagenet-o-test\n",
      "--> imagenet-r\n",
      "Loading imagenet-r-test\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "seed_rep = 0\n",
    "\n",
    "from pytorch_ood.detector import MaxSoftmax, EnergyBased\n",
    "\n",
    "ensemble_detectors = [MaxSoftmax(m) for m in models]\n",
    "\n",
    "detectors = {\n",
    "    \"Semantic\": SemanticDetector(mini_model, models, target_vectors=target_vectors)\n",
    "}\n",
    "\n",
    "for name, detector in detectors.items():\n",
    "    print(f\"-> {name}\")\n",
    "    metrics = evaluate(detector)\n",
    "    for m in metrics:\n",
    "        m.update({\n",
    "            \"Method\": name,\n",
    "            \"Rep\": seed_rep\n",
    "        })\n",
    "    results += metrics\n",
    "\n",
    "\n",
    "result_df = pd.DataFrame(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "488896b0-cc27-4ce9-8167-2d841a31d50b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>AUROC</th>\n",
       "      <th>AUPR-IN</th>\n",
       "      <th>AUPR-OUT</th>\n",
       "      <th>ACC95TPR</th>\n",
       "      <th>FPR95TPR</th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Method</th>\n",
       "      <th>Rep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.747917</td>\n",
       "      <td>0.801264</td>\n",
       "      <td>0.889107</td>\n",
       "      <td>0.715245</td>\n",
       "      <td>0.502868</td>\n",
       "      <td>textures</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.748114</td>\n",
       "      <td>0.896711</td>\n",
       "      <td>0.831838</td>\n",
       "      <td>0.828419</td>\n",
       "      <td>0.502868</td>\n",
       "      <td>fractals</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.747591</td>\n",
       "      <td>0.864790</td>\n",
       "      <td>0.850182</td>\n",
       "      <td>0.786168</td>\n",
       "      <td>0.502868</td>\n",
       "      <td>fooling</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.748114</td>\n",
       "      <td>0.834103</td>\n",
       "      <td>0.872030</td>\n",
       "      <td>0.750422</td>\n",
       "      <td>0.502868</td>\n",
       "      <td>imagenet-a</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.747831</td>\n",
       "      <td>0.674827</td>\n",
       "      <td>0.945548</td>\n",
       "      <td>0.604591</td>\n",
       "      <td>0.502868</td>\n",
       "      <td>imagenet-o</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.748203</td>\n",
       "      <td>0.945204</td>\n",
       "      <td>0.794732</td>\n",
       "      <td>0.900354</td>\n",
       "      <td>0.502868</td>\n",
       "      <td>imagenet-r</td>\n",
       "      <td>Semantic</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      AUROC   AUPR-IN  AUPR-OUT  ACC95TPR  FPR95TPR     Dataset    Method  Rep\n",
       "0  0.747917  0.801264  0.889107  0.715245  0.502868    textures  Semantic    0\n",
       "1  0.748114  0.896711  0.831838  0.828419  0.502868    fractals  Semantic    0\n",
       "2  0.747591  0.864790  0.850182  0.786168  0.502868     fooling  Semantic    0\n",
       "3  0.748114  0.834103  0.872030  0.750422  0.502868  imagenet-a  Semantic    0\n",
       "4  0.747831  0.674827  0.945548  0.604591  0.502868  imagenet-o  Semantic    0\n",
       "5  0.748203  0.945204  0.794732  0.900354  0.502868  imagenet-r  Semantic    0"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "result_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b96e82d-9cda-4150-8297-9f841bfa9bec",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
