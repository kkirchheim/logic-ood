{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09e68aaa-4b3b-46f8-a9bb-df16e1bd6e7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T13:06:46.478438331Z",
     "start_time": "2023-08-23T13:06:45.125681081Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, random_split\n",
    "import os \n",
    "from os.path import join \n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import sys\n",
    "logger = logging.getLogger()\n",
    "logger.addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "logger.info(\"abc\")\n",
    "\n",
    "device = \"cuda:0\"\n",
    "root = \"/home/ki/datasets/\"\n",
    "\n",
    "from detector import label_to_name, color_to_name\n",
    "\n",
    "class FruitDataset(Dataset):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    class_color_map = {\n",
    "        \"Apple Braeburn\": \"red\",\n",
    "        \"Apple Granny Smith\": \"green\",\n",
    "        \"Apricot\": \"orange\",\n",
    "        \"Avocado\": \"green\",\n",
    "        \"Banana\": \"yellow\",\n",
    "        \"Blueberry\": \"black\",\n",
    "        \"Cactus fruit\": \"green\",\n",
    "        \"Cantaloupe\": \"yellow\",\n",
    "        \"Cherry\": \"red\",\n",
    "        \"Clementine\": \"orange\",\n",
    "        \"Corn\": \"yellow\",\n",
    "        \"Cucumber Ripe\": \"brown\",\n",
    "        \"Grape Blue\": \"black\",\n",
    "        \"Kiwi\": \"brown\",\n",
    "        \"Lemon\": \"yellow\",\n",
    "        \"Limes\": \"green\",\n",
    "        \"Mango\": \"green\",\n",
    "        \"Onion White\": \"brown\",\n",
    "        \"Orange\": \"orange\",\n",
    "        \"Papaya\": \"green\",\n",
    "        \"Passion Fruit\": \"black\",\n",
    "        \"Peach\": \"orange\",\n",
    "        \"Pear\": \"green\", # ??\n",
    "        \"Pepper Green\": \"green\",\n",
    "        \"Pepper Red\": \"red\",\n",
    "        \"Pineapple\": \"brown\",\n",
    "        \"Plum\": \"red\",\n",
    "        \"Pomegranate\": \"red\",\n",
    "        \"Potato Red\": \"brown\",\n",
    "        \"Raspberry\": \"red\",\n",
    "        \"Strawberry\": \"red\",\n",
    "        \"Tomato\": \"red\",\n",
    "        \"Watermelon\": \"red\" \n",
    "    }\n",
    "    \n",
    "    def __init__(self, root=\"train\", transform=None, target_transform=None):\n",
    "        root = join(root, \"fruits\", \"train\", \"train\")\n",
    "\n",
    "        self.classes = os.listdir(root)\n",
    "        self.files = []\n",
    "        self.labels = []\n",
    "        self.colors = []\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform \n",
    "        \n",
    "        for c in self.classes:\n",
    "            fs = [join(root, c, f) for f in os.listdir(join(root, c))]\n",
    "            self.files += fs\n",
    "            self.labels += [c.lower().replace(\" \", \"_\")] * len(fs)\n",
    "            self.colors += [self.class_color_map[c]] * len(fs)\n",
    "\n",
    "        self.class_map = {c: n for n, c in enumerate(label_to_name)}\n",
    "        self.color_map = {c: n for n, c in enumerate(color_to_name)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.files[index]\n",
    "        y = self.class_map[self.labels[index]]\n",
    "        color = self.color_map[self.colors[index]]\n",
    "        \n",
    "        img = Image.open(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        y = torch.tensor([y, color]) \n",
    "        if self.target_transform is not None:\n",
    "            y = self.target_transform(y)\n",
    "        \n",
    "        return img, y \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20c8d208-b5cc-40cd-95d1-984635599b8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T13:06:46.510918683Z",
     "start_time": "2023-08-23T13:06:46.466329698Z"
    }
   },
   "outputs": [],
   "source": [
    "ds = FruitDataset(root=root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20e713eb-873e-4c92-b9a5-6d5d3a069707",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T13:06:46.529423077Z",
     "start_time": "2023-08-23T13:06:46.500492521Z"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_ood.utils import ToRGB\n",
    "from torchvision.transforms import ToTensor, Resize, Compose\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "trans = Compose([ToRGB(), ToTensor(), Resize((32, 32), antialias=True)])\n",
    "\n",
    "data = FruitDataset(root=root, transform=trans)\n",
    "train_data, val_data, test_data = random_split(data, [14000,1000, 1854], generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81f40b8b-621b-47cd-bf1a-2ad43081edf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T13:06:46.529664404Z",
     "start_time": "2023-08-23T13:06:46.528208613Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from pytorch_ood.model import WideResNet\n",
    "\n",
    "# def override \n",
    "def Model(num_classes=None, *args, **kwargs):\n",
    "    model = WideResNet(*args, num_classes=1000, pretrained=\"imagenet32\", **kwargs)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd36c49f-3254-4808-a751-1c79b5c0d4a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T13:06:46.576489430Z",
     "start_time": "2023-08-23T13:06:46.535161145Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "\n",
    "def train_model(att_index, num_classes):\n",
    "    \"\"\"\n",
    "    train a model for the given attribute index \n",
    "    \"\"\"\n",
    "    model = Model(num_classes=num_classes).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9, nesterov=True)\n",
    "\n",
    "    for epoch in range(1):\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        bar = tqdm(train_loader)\n",
    "        for inputs, y in bar:\n",
    "            labels = y[:, att_index]\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss = 0.8 * running_loss + 0.2 * loss.item()\n",
    "            bar.set_postfix({\"loss\": running_loss})\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            for inputs, y in test_loader:\n",
    "                labels = y[:, att_index]\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, dim=1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Accuracy of the network on the test images: {correct / total:.2%}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "from pytorch_ood.dataset.img import TinyImages300k\n",
    "from pytorch_ood.utils import is_known\n",
    "\n",
    "def train_fruit_model():\n",
    "    tiny = TinyImages300k(root=root, download=True, transform=trans, target_transform=ToUnknown())\n",
    "    data_train_out, data_test_out, _ = random_split(tiny, [50000, 10000, 240000], generator=torch.Generator().manual_seed(123))\n",
    "\n",
    "    data_noatt = FruitDataset(root=root, transform=trans, target_transform=lambda y: int(y[0]))\n",
    "    train_data_noatt, val_data_noatt, test_data_noatt = random_split(data_noatt, [14000,1000, 1854], generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "    new_loader = DataLoader(train_data_noatt + data_train_out, batch_size=32, shuffle=True, num_workers=10)\n",
    "    new_test_loader = DataLoader(test_data_noatt + data_test_out, batch_size=32, shuffle=False, num_workers=10)\n",
    "\n",
    "    model = Model(num_classes=2).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9, nesterov=True)\n",
    "\n",
    "    accs = []\n",
    "\n",
    "    for epoch in range(1):\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "\n",
    "        bar = tqdm(new_loader)\n",
    "        for inputs, y in bar:\n",
    "            labels = is_known(y).long()\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss = 0.8 * running_loss + 0.2 * loss.item()\n",
    "            bar.set_postfix({\"loss\": running_loss})\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            for inputs, y in new_test_loader:\n",
    "                labels = is_known(y).long()\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Accuracy of the shape network on the test images: {correct / total:.2%}')\n",
    "        accs.append(correct / total)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T13:11:45.499631367Z",
     "start_time": "2023-08-23T13:11:45.489319415Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1a02580-6d2c-4db2-a4b7-50c8dbbabf22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T13:11:45.898505204Z",
     "start_time": "2023-08-23T13:11:45.864854045Z"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_ood.dataset.img import (LSUNCrop, LSUNResize, Textures, TinyImageNetCrop, TinyImageNetResize)\n",
    "from pytorch_ood.detector import EnergyBased, MaxSoftmax, MaxLogit, Entropy, Mahalanobis, ViM, ReAct\n",
    "from pytorch_ood.utils import OODMetrics, ToUnknown\n",
    "from detector import EnsembleDetector, PrologOOD, Prologic, PrologOODT\n",
    "\n",
    "def evaluate(label_net, color_net, fruit_net):\n",
    "    _ = label_net.eval()\n",
    "    _ = color_net.eval()\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    detectors = {\n",
    "        \"ViM\": ViM(label_net.features, w=label_net.fc.weight, b=label_net.fc.bias, d=64),\n",
    "        \"Mahalanobis\": Mahalanobis(label_net.features),\n",
    "        \"Entropy\": Entropy(label_net),\n",
    "        \"LogicOOD+\": PrologOOD(\"kb.pl\", label_net, color_net, fruit_net),\n",
    "        \"Logic\": Prologic(\"kb.pl\", label_net, color_net),\n",
    "        \"Logic+\": Prologic(\"kb.pl\", label_net, color_net, fruit_net),\n",
    "        \"LogicOOD\": PrologOOD(\"kb.pl\", label_net, color_net),\n",
    "        \"LogicT\": PrologOODT(\"kb.pl\", label_net, color_net),\n",
    "        \"Ensemble\": EnsembleDetector(label_net, color_net),\n",
    "        \"MSP\": MaxSoftmax(label_net),\n",
    "        \"ReAct\": ReAct(label_net.features, label_net.fc),\n",
    "        \"Energy\": EnergyBased(label_net),\n",
    "        \"MaxLogit\": MaxLogit(label_net),\n",
    "    }\n",
    "\n",
    "\n",
    "    data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
    "    _ , data_fit_label, _ = random_split(data_fit_label, [14000, 1000, 1854], generator=torch.Generator().manual_seed(0))\n",
    "    data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
    "    _, data_fit_color, _ = random_split(data_fit_color, [14000, 1000, 1854], generator=torch.Generator().manual_seed(0))\n",
    "    data_fit_color = DataLoader(data_fit_color, batch_size=32, shuffle=False, num_workers=2)\n",
    "    data_fit_label = DataLoader(data_fit_label, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "    data = FruitDataset(root=root, transform=trans, target_transform=lambda y: int(y[0]))\n",
    "    data_in_train, data_in_val, data_in = random_split(data, [14000, 1000, 1854], generator=torch.Generator().manual_seed(0))\n",
    "    train_in_loader = DataLoader(data_in_train, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "    detectors[\"ViM\"].fit(train_in_loader, device=device)\n",
    "    detectors[\"LogicT\"].fit(data_fit_label, data_fit_color, device=device)\n",
    "    # detectors[\"Mahalanobis\"].fit(train_in_loader, device=device)\n",
    "\n",
    "    datasets = {d.__name__: d for d in (LSUNCrop, LSUNResize, Textures, TinyImageNetCrop, TinyImageNetResize)}\n",
    "    \n",
    "    for detector_name, detector in detectors.items():\n",
    "        for data_name, dataset_c in datasets.items():\n",
    "            print(data_name)\n",
    "            data_out = dataset_c(root=root, transform=trans, target_transform=ToUnknown(), download=True)\n",
    "            loader = DataLoader(data_in+data_out, batch_size=256, shuffle=False, num_workers=12)\n",
    "            \n",
    "            scores = []\n",
    "            ys = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for x, y in loader:\n",
    "                    scores.append(detector(x.to(device)))\n",
    "                    ys.append(y.to(device))\n",
    "                    \n",
    "                scores = torch.cat(scores, dim=0).cpu()\n",
    "                ys = torch.cat(ys, dim=0).cpu()\n",
    "            \n",
    "            metrics = OODMetrics()\n",
    "            metrics.update(scores, ys)\n",
    "            r = metrics.compute()\n",
    "            r.update({\n",
    "                \"Method\": detector_name,\n",
    "                \"Dataset\": data_name\n",
    "            })\n",
    "            print(r)\n",
    "            results.append(r)\n",
    "    \n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "930a358f-c6ef-4fe9-9b1a-c305a37ffb70",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2023-08-23T13:17:25.176674190Z",
     "start_time": "2023-08-23T13:11:46.337355625Z"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:12<00:00, 35.12it/s, loss=0.22]  \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 95.42%\n",
      "color\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:12<00:00, 34.64it/s, loss=0.0604]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 96.17%\n",
      "fruit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:57<00:00, 34.67it/s, loss=0.0263]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.88%\n",
      "Computing principal space ...\n",
      "Computing alpha ...\n",
      "self.alpha=4.4100\n",
      "Fitting with temperature scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-10-b3808c0e1ede>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-10-b3808c0e1ede>:29: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-10-b3808c0e1ede>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
      "<ipython-input-10-b3808c0e1ede>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial T/NLL: 1.000/0.155\n",
      "Optimal temperature: 0.8572290539741516\n",
      "NLL after scaling: 0.13'\n",
      "Initial T/NLL: 1.000/0.110\n",
      "Optimal temperature: 0.8130077123641968\n",
      "NLL after scaling: 0.10'\n",
      "self.scorer_label.t=Parameter containing:\n",
      "tensor(0.8572, requires_grad=True)\n",
      "self.scorer_color.t=Parameter containing:\n",
      "tensor(0.8130, requires_grad=True)\n",
      "LSUNCrop\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:84: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels).item()\n",
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:90: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels)\n",
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:97: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels).item()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUROC': 0.9957823753356934, 'AUPR-IN': 0.9992374777793884, 'AUPR-OUT': 0.9767327904701233, 'FPR95TPR': 0.004854368977248669, 'Method': 'ViM', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9999845027923584, 'AUPR-IN': 0.9999971985816956, 'AUPR-OUT': 0.9999152421951294, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9990158677101135, 'AUPR-IN': 0.9996914267539978, 'AUPR-OUT': 0.9967571496963501, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.998676061630249, 'AUPR-IN': 0.9997619390487671, 'AUPR-OUT': 0.9923152923583984, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9998490810394287, 'AUPR-IN': 0.9999725818634033, 'AUPR-OUT': 0.9991335272789001, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9773463010787964, 'AUPR-IN': 0.9958349466323853, 'AUPR-OUT': 0.9808893799781799, 'FPR95TPR': 0.04530744254589081, 'Method': 'LogicOOD+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9773463010787964, 'AUPR-IN': 0.9958349466323853, 'AUPR-OUT': 0.9808893799781799, 'FPR95TPR': 0.04530744254589081, 'Method': 'LogicOOD+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9764329791069031, 'AUPR-IN': 0.9923514723777771, 'AUPR-OUT': 0.9794517159461975, 'FPR95TPR': 0.04530744254589081, 'Method': 'LogicOOD+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9773463010787964, 'AUPR-IN': 0.9958349466323853, 'AUPR-OUT': 0.9808893799781799, 'FPR95TPR': 0.04530744254589081, 'Method': 'LogicOOD+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9773463010787964, 'AUPR-IN': 0.9958349466323853, 'AUPR-OUT': 0.9808893799781799, 'FPR95TPR': 0.04530744254589081, 'Method': 'LogicOOD+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.6515462398529053, 'AUPR-IN': 0.9372726678848267, 'AUPR-OUT': 0.5876960158348083, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.7486962676048279, 'AUPR-IN': 0.9566173553466797, 'AUPR-OUT': 0.6204132437705994, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.7083746194839478, 'AUPR-IN': 0.9178435802459717, 'AUPR-OUT': 0.6671722531318665, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.6442962288856506, 'AUPR-IN': 0.9356400966644287, 'AUPR-OUT': 0.5858591198921204, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.7446462512016296, 'AUPR-IN': 0.9558702707290649, 'AUPR-OUT': 0.6186540126800537, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.8145418167114258, 'AUPR-IN': 0.958274245262146, 'AUPR-OUT': 0.40986162424087524, 'FPR95TPR': 0.7605177760124207, 'Method': 'LogicOOD', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.8992488980293274, 'AUPR-IN': 0.9778962135314941, 'AUPR-OUT': 0.7034279704093933, 'FPR95TPR': 0.430960088968277, 'Method': 'LogicOOD', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.8352396488189697, 'AUPR-IN': 0.9407514929771423, 'AUPR-OUT': 0.5704926252365112, 'FPR95TPR': 0.7313916087150574, 'Method': 'LogicOOD', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.7912513017654419, 'AUPR-IN': 0.953644335269928, 'AUPR-OUT': 0.34613314270973206, 'FPR95TPR': 0.8160733580589294, 'Method': 'LogicOOD', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.8866801857948303, 'AUPR-IN': 0.9755480289459229, 'AUPR-OUT': 0.6303645968437195, 'FPR95TPR': 0.4994606375694275, 'Method': 'LogicOOD', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.8071846961975098, 'AUPR-IN': 0.9565461874008179, 'AUPR-OUT': 0.3969477415084839, 'FPR95TPR': 0.7718446850776672, 'Method': 'LogicT', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.8939119577407837, 'AUPR-IN': 0.9768375754356384, 'AUPR-OUT': 0.6866648197174072, 'FPR95TPR': 0.4590075612068176, 'Method': 'LogicT', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.8312889337539673, 'AUPR-IN': 0.9393956661224365, 'AUPR-OUT': 0.5632431507110596, 'FPR95TPR': 0.7346278429031372, 'Method': 'LogicT', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.7836207151412964, 'AUPR-IN': 0.9519066214561462, 'AUPR-OUT': 0.33371269702911377, 'FPR95TPR': 0.8252426981925964, 'Method': 'LogicT', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.8826426863670349, 'AUPR-IN': 0.974732518196106, 'AUPR-OUT': 0.6199457049369812, 'FPR95TPR': 0.517259955406189, 'Method': 'LogicT', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.8178167343139648, 'AUPR-IN': 0.9611713886260986, 'AUPR-OUT': 0.4074253439903259, 'FPR95TPR': 0.7605177760124207, 'Method': 'Ensemble', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.8618383407592773, 'AUPR-IN': 0.9676669836044312, 'AUPR-OUT': 0.6500527262687683, 'FPR95TPR': 0.4778856635093689, 'Method': 'Ensemble', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.8023319840431213, 'AUPR-IN': 0.9252882599830627, 'AUPR-OUT': 0.5340213775634766, 'FPR95TPR': 0.7481122016906738, 'Method': 'Ensemble', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.7970393896102905, 'AUPR-IN': 0.9576264023780823, 'AUPR-OUT': 0.3453179597854614, 'FPR95TPR': 0.8160733580589294, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.8523096442222595, 'AUPR-IN': 0.9661304354667664, 'AUPR-OUT': 0.5856469869613647, 'FPR95TPR': 0.5377562046051025, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.7912930846214294, 'AUPR-IN': 0.9515079259872437, 'AUPR-OUT': 0.3615049719810486, 'FPR95TPR': 0.7918015122413635, 'Method': 'MSP', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.8049109578132629, 'AUPR-IN': 0.9502328634262085, 'AUPR-OUT': 0.5200158357620239, 'FPR95TPR': 0.6429342031478882, 'Method': 'MSP', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.8028970956802368, 'AUPR-IN': 0.924152672290802, 'AUPR-OUT': 0.5075874328613281, 'FPR95TPR': 0.7777777910232544, 'Method': 'MSP', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.784988284111023, 'AUPR-IN': 0.9528003334999084, 'AUPR-OUT': 0.3183321952819824, 'FPR95TPR': 0.8743258118629456, 'Method': 'MSP', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.823284387588501, 'AUPR-IN': 0.9566861391067505, 'AUPR-OUT': 0.5124912858009338, 'FPR95TPR': 0.6321467161178589, 'Method': 'MSP', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.8968619704246521, 'AUPR-IN': 0.9769706130027771, 'AUPR-OUT': 0.720180869102478, 'FPR95TPR': 0.3996763825416565, 'Method': 'ReAct', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.7512013912200928, 'AUPR-IN': 0.9256159663200378, 'AUPR-OUT': 0.5347287654876709, 'FPR95TPR': 0.5760517716407776, 'Method': 'ReAct', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.8251652717590332, 'AUPR-IN': 0.9253968596458435, 'AUPR-OUT': 0.6494777202606201, 'FPR95TPR': 0.5507012009620667, 'Method': 'ReAct', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.8454989194869995, 'AUPR-IN': 0.9609158635139465, 'AUPR-OUT': 0.624547004699707, 'FPR95TPR': 0.4687162935733795, 'Method': 'ReAct', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.7982360124588013, 'AUPR-IN': 0.9436894655227661, 'AUPR-OUT': 0.5792022943496704, 'FPR95TPR': 0.5183387398719788, 'Method': 'ReAct', 'Dataset': 'TinyImageNetResize'}\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for trial in range(1):\n",
    "    print(\"label\")\n",
    "    label_net = train_model(att_index=0, num_classes=33)\n",
    "    print(\"color\")\n",
    "    color_net = train_model(att_index=1, num_classes=6)\n",
    "    print(\"fruit\")\n",
    "\n",
    "    fruit_net = train_fruit_model()\n",
    "\n",
    "    res = evaluate(label_net, color_net, fruit_net)\n",
    "    \n",
    "    for r in res:\n",
    "        r.update({\"Seed\": trial})\n",
    "    \n",
    "    results += res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c3e5287e-1bd0-4bfd-a73f-e427f369f3c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-23T13:22:57.338651622Z",
     "start_time": "2023-08-23T13:22:57.270259796Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{2}{l}{AUROC} & \\multicolumn{2}{l}{AUPR-IN} & \\multicolumn{2}{l}{AUPR-OUT} & \\multicolumn{2}{l}{FPR95TPR} \\\\\n",
      "{} &  mean &  sem &    mean &  sem &     mean &  sem &     mean &  sem \\\\\n",
      "Method    &       &      &         &      &          &      &          &      \\\\\n",
      "\\midrule\n",
      "Ensemble  & 82.63 & 1.31 &   95.56 & 0.78 &    50.45 & 5.63 &    66.81 & 6.71 \\\\\n",
      "Logic     & 69.95 & 2.22 &   94.06 & 0.72 &    61.60 & 1.48 &   100.00 & 0.00 \\\\\n",
      "LogicOOD  & 84.54 & 2.07 &   96.12 & 0.70 &    53.21 & 6.71 &    64.77 & 7.65 \\\\\n",
      "LogicOOD+ & 97.72 & 0.02 &   99.51 & 0.07 &    98.06 & 0.03 &     4.53 & 0.00 \\\\\n",
      "LogicT    & 83.97 & 2.13 &   95.99 & 0.71 &    52.01 & 6.69 &    66.16 & 7.29 \\\\\n",
      "MSP       & 80.15 & 0.66 &   94.71 & 0.58 &    44.40 & 4.31 &    74.38 & 4.64 \\\\\n",
      "ReAct     & 82.34 & 2.42 &   94.65 & 1.01 &    62.16 & 3.15 &    50.27 & 3.14 \\\\\n",
      "ViM       & 99.87 & 0.08 &   99.97 & 0.01 &    99.30 & 0.43 &     0.10 & 0.10 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "result_df = pd.DataFrame(results)\n",
    "print((result_df.groupby(by=\"Method\").agg([\"mean\", \"sem\"]) * 100)[[\"AUROC\", \"AUPR-IN\", \"AUPR-OUT\", \"FPR95TPR\"]].to_latex(float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "s = (result_df.groupby(by=\"Method\").agg([\"mean\", \"sem\"]) * 100)[[\"AUROC\", \"AUPR-IN\", \"AUPR-OUT\", \"FPR95TPR\"]].to_latex(float_format=\"%.2f\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T13:22:57.550100548Z",
     "start_time": "2023-08-23T13:22:57.507416425Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{2}{l}{AUROC} & \\multicolumn{2}{l}{AUPR-IN} & \\multicolumn{2}{l}{AUPR-OUT} & \\multicolumn{2}{l}{FPR95TPR} \\\\\n",
      "{} &  mean &  sem &    mean &  sem &     mean &  sem &     mean &  sem \\\\\n",
      "Method    &       &      &         &      &          &      &          &      \\\\\n",
      "\\midrule\n",
      "Ensemble  & 82.63 & \\pm 1.31 &   95.56 & \\pm 0.78 &    50.45 & 5.63 &    66.81 & 6.71 \\\\\n",
      "Logic     & 69.95 & 2.22 &   94.06 & \\pm 0.72 &    61.60 & \\pm 1.48 &   100.00 & \\pm 0.00 \\\\\n",
      "LogicOOD  & 84.54 & 2.07 &   96.12 & \\pm 0.70 &    53.21 & 6.71 &    64.77 & 7.65 \\\\\n",
      "LogicOOD+ & 97.72 & \\pm 0.02 &   99.51 & \\pm 0.07 &    98.06 & \\pm 0.03 &     4.53 & \\pm 0.00 \\\\\n",
      "LogicT    & 83.97 & 2.13 &   95.99 & \\pm 0.71 &    52.01 & 6.69 &    66.16 & 7.29 \\\\\n",
      "MSP       & 80.15 & \\pm 0.66 &   94.71 & \\pm 0.58 &    44.40 & 4.31 &    74.38 & 4.64 \\\\\n",
      "ReAct     & 82.34 & 2.42 &   94.65 & \\pm 1.01 &    62.16 & 3.15 &    50.27 & 3.14 \\\\\n",
      "ViM       & 99.87 & \\pm 0.08 &   99.97 & \\pm 0.01 &    99.30 & \\pm 0.43 &     0.10 & \\pm 0.10 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(s.replace(\"& 0.\", \"& \\pm 0.\").replace(\"& 1.\", \"& \\pm 1.\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-23T13:22:57.706469121Z",
     "start_time": "2023-08-23T13:22:57.660957495Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
