{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09e68aaa-4b3b-46f8-a9bb-df16e1bd6e7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T15:56:10.942927362Z",
     "start_time": "2023-08-25T15:56:09.071440435Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset, random_split\n",
    "import os \n",
    "from os.path import join \n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import sys\n",
    "logger = logging.getLogger()\n",
    "logger.addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "device = \"cuda:0\"\n",
    "root = \"/home/ki/datasets/\"\n",
    "\n",
    "from detector import label_to_name, color_to_name\n",
    "\n",
    "class FruitDataset(Dataset):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    class_color_map = {\n",
    "        \"Apple Braeburn\": \"red\",\n",
    "        \"Apple Granny Smith\": \"green\",\n",
    "        \"Apricot\": \"orange\",\n",
    "        \"Avocado\": \"green\",\n",
    "        \"Banana\": \"yellow\",\n",
    "        \"Blueberry\": \"black\",\n",
    "        \"Cactus fruit\": \"green\",\n",
    "        \"Cantaloupe\": \"yellow\",\n",
    "        \"Cherry\": \"red\",\n",
    "        \"Clementine\": \"orange\",\n",
    "        \"Corn\": \"yellow\",\n",
    "        \"Cucumber Ripe\": \"brown\",\n",
    "        \"Grape Blue\": \"black\",\n",
    "        \"Kiwi\": \"brown\",\n",
    "        \"Lemon\": \"yellow\",\n",
    "        \"Limes\": \"green\",\n",
    "        \"Mango\": \"green\",\n",
    "        \"Onion White\": \"brown\",\n",
    "        \"Orange\": \"orange\",\n",
    "        \"Papaya\": \"green\",\n",
    "        \"Passion Fruit\": \"black\",\n",
    "        \"Peach\": \"orange\",\n",
    "        \"Pear\": \"green\", # ??\n",
    "        \"Pepper Green\": \"green\",\n",
    "        \"Pepper Red\": \"red\",\n",
    "        \"Pineapple\": \"brown\",\n",
    "        \"Plum\": \"red\",\n",
    "        \"Pomegranate\": \"red\",\n",
    "        \"Potato Red\": \"brown\",\n",
    "        \"Raspberry\": \"red\",\n",
    "        \"Strawberry\": \"red\",\n",
    "        \"Tomato\": \"red\",\n",
    "        \"Watermelon\": \"red\" \n",
    "    }\n",
    "    \n",
    "    def __init__(self, root=\"train\", transform=None, target_transform=None):\n",
    "        root = join(root, \"fruits\", \"train\", \"train\")\n",
    "\n",
    "        self.classes = os.listdir(root)\n",
    "        self.files = []\n",
    "        self.labels = []\n",
    "        self.colors = []\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform \n",
    "        \n",
    "        for c in self.classes:\n",
    "            fs = [join(root, c, f) for f in os.listdir(join(root, c))]\n",
    "            self.files += fs\n",
    "            self.labels += [c.lower().replace(\" \", \"_\")] * len(fs)\n",
    "            self.colors += [self.class_color_map[c]] * len(fs)\n",
    "\n",
    "        self.class_map = {c: n for n, c in enumerate(label_to_name)}\n",
    "        self.color_map = {c: n for n, c in enumerate(color_to_name)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.files[index]\n",
    "        y = self.class_map[self.labels[index]]\n",
    "        color = self.color_map[self.colors[index]]\n",
    "        \n",
    "        img = Image.open(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        y = torch.tensor([y, color]) \n",
    "        if self.target_transform is not None:\n",
    "            y = self.target_transform(y)\n",
    "        \n",
    "        return img, y \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20c8d208-b5cc-40cd-95d1-984635599b8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T15:56:10.990498798Z",
     "start_time": "2023-08-25T15:56:10.943354538Z"
    }
   },
   "outputs": [],
   "source": [
    "ds = FruitDataset(root=root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20e713eb-873e-4c92-b9a5-6d5d3a069707",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T15:56:11.023254189Z",
     "start_time": "2023-08-25T15:56:10.993266248Z"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_ood.utils import ToRGB\n",
    "from torchvision.transforms import ToTensor, Resize, Compose\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "trans = Compose([ToRGB(), ToTensor(), Resize((32, 32), antialias=True)])\n",
    "\n",
    "data = FruitDataset(root=root, transform=trans)\n",
    "train_data, val_data, test_data = random_split(data, [14000,1000, 1854], generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81f40b8b-621b-47cd-bf1a-2ad43081edf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T15:56:11.023710484Z",
     "start_time": "2023-08-25T15:56:11.021503877Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from pytorch_ood.model import WideResNet\n",
    "\n",
    "# def override \n",
    "def Model(num_classes=None, *args, **kwargs):\n",
    "    model = WideResNet(*args, num_classes=1000, pretrained=\"imagenet32\", **kwargs)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd36c49f-3254-4808-a751-1c79b5c0d4a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T15:56:11.093273300Z",
     "start_time": "2023-08-25T15:56:11.028850462Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "\n",
    "def train_model(att_index, num_classes):\n",
    "    \"\"\"\n",
    "    train a model for the given attribute index \n",
    "    \"\"\"\n",
    "    model = Model(num_classes=num_classes).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9, nesterov=True)\n",
    "\n",
    "    for epoch in range(5):\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        bar = tqdm(train_loader)\n",
    "        for inputs, y in bar:\n",
    "            labels = y[:, att_index]\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss = 0.8 * running_loss + 0.2 * loss.item()\n",
    "            bar.set_postfix({\"loss\": running_loss})\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            for inputs, y in test_loader:\n",
    "                labels = y[:, att_index]\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, dim=1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Accuracy of the network on the test images: {correct / total:.2%}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from pytorch_ood.dataset.img import TinyImages300k\n",
    "from pytorch_ood.utils import is_known\n",
    "\n",
    "def train_fruit_model():\n",
    "    tiny = TinyImages300k(root=root, download=True, transform=trans, target_transform=ToUnknown())\n",
    "    data_train_out, data_test_out, _ = random_split(tiny, [50000, 10000, 240000], generator=torch.Generator().manual_seed(123))\n",
    "\n",
    "    data_noatt = FruitDataset(root=root, transform=trans, target_transform=lambda y: int(y[0]))\n",
    "    train_data_noatt, val_data_noatt, test_data_noatt = random_split(data_noatt, [14000,1000, 1854], generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "    new_loader = DataLoader(train_data_noatt + data_train_out, batch_size=32, shuffle=True, num_workers=10)\n",
    "    new_test_loader = DataLoader(test_data_noatt + data_test_out, batch_size=32, shuffle=False, num_workers=10)\n",
    "\n",
    "    model = Model(num_classes=2).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9, nesterov=True)\n",
    "\n",
    "    accs = []\n",
    "\n",
    "    for epoch in range(10):\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "\n",
    "        bar = tqdm(new_loader)\n",
    "        for inputs, y in bar:\n",
    "            labels = is_known(y).long()\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss = 0.8 * running_loss + 0.2 * loss.item()\n",
    "            bar.set_postfix({\"loss\": running_loss})\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            for inputs, y in new_test_loader:\n",
    "                labels = is_known(y).long()\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Accuracy of the shape network on the test images: {correct / total:.2%}')\n",
    "        accs.append(correct / total)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T15:56:11.094332039Z",
     "start_time": "2023-08-25T15:56:11.074567670Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1a02580-6d2c-4db2-a4b7-50c8dbbabf22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T15:56:11.094560432Z",
     "start_time": "2023-08-25T15:56:11.074782296Z"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_ood.dataset.img import (LSUNCrop, LSUNResize, Textures, TinyImageNetCrop, TinyImageNetResize)\n",
    "from pytorch_ood.detector import EnergyBased, MaxSoftmax, MaxLogit, Entropy, Mahalanobis, ViM, ReAct\n",
    "from pytorch_ood.utils import OODMetrics, ToUnknown\n",
    "from detector import EnsembleDetector, PrologOOD, Prologic, PrologOODT\n",
    "\n",
    "def evaluate(label_net, color_net, fruit_net):\n",
    "    _ = label_net.eval()\n",
    "    _ = color_net.eval()\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    detectors = {\n",
    "        \"ViM\": ViM(label_net.features, w=label_net.fc.weight, b=label_net.fc.bias, d=64),\n",
    "        \"Mahalanobis\": Mahalanobis(label_net.features),\n",
    "        \"Entropy\": Entropy(label_net),\n",
    "        \"LogicOOD+\": PrologOOD(\"kb.pl\", label_net, color_net, fruit_net),\n",
    "        \"Logic\": Prologic(\"kb.pl\", label_net, color_net),\n",
    "        \"Logic+\": Prologic(\"kb.pl\", label_net, color_net, fruit_net),\n",
    "        \"LogicOOD\": PrologOOD(\"kb.pl\", label_net, color_net),\n",
    "        \"LogicOODT\": PrologOODT(\"kb.pl\", label_net, color_net),\n",
    "        \"LogicOODT+\": PrologOODT(\"kb.pl\", label_net, color_net, fruit_net),\n",
    "        # \"LogicT+\": PrologOODT(\"kb.pl\", label_net, color_net, fruit_net), # this should be exactly the same\n",
    "        \"Ensemble\": EnsembleDetector(label_net, color_net),\n",
    "        \"MSP\": MaxSoftmax(label_net),\n",
    "        \"ReAct\": ReAct(label_net.features, label_net.fc),\n",
    "        \"Energy\": EnergyBased(label_net),\n",
    "        \"MaxLogit\": MaxLogit(label_net),\n",
    "    }\n",
    "\n",
    "\n",
    "    data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
    "    _ , data_fit_label, _ = random_split(data_fit_label, [14000, 1000, 1854], generator=torch.Generator().manual_seed(0))\n",
    "    data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
    "    _, data_fit_color, _ = random_split(data_fit_color, [14000, 1000, 1854], generator=torch.Generator().manual_seed(0))\n",
    "    data_fit_color = DataLoader(data_fit_color, batch_size=32, shuffle=False, num_workers=2)\n",
    "    data_fit_label = DataLoader(data_fit_label, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "    data = FruitDataset(root=root, transform=trans, target_transform=lambda y: int(y[0]))\n",
    "    data_in_train, data_in_val, data_in = random_split(data, [14000, 1000, 1854], generator=torch.Generator().manual_seed(0))\n",
    "    train_in_loader = DataLoader(data_in_train, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "    detectors[\"ViM\"].fit(train_in_loader, device=device)\n",
    "    detectors[\"LogicOODT\"].fit(data_fit_label, data_fit_color, device=device)\n",
    "    detectors[\"LogicOODT+\"].fit(data_fit_label, data_fit_color, device=device)\n",
    "    detectors[\"Mahalanobis\"].fit(train_in_loader, device=device)\n",
    "\n",
    "    datasets = {d.__name__: d for d in (LSUNCrop, LSUNResize, Textures, TinyImageNetCrop, TinyImageNetResize)}\n",
    "    \n",
    "    for detector_name, detector in detectors.items():\n",
    "        for data_name, dataset_c in datasets.items():\n",
    "            print(data_name)\n",
    "            data_out = dataset_c(root=root, transform=trans, target_transform=ToUnknown(), download=True)\n",
    "            loader = DataLoader(data_in+data_out, batch_size=256, shuffle=False, num_workers=12)\n",
    "            \n",
    "            scores = []\n",
    "            ys = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for x, y in loader:\n",
    "                    scores.append(detector(x.to(device)))\n",
    "                    ys.append(y.to(device))\n",
    "                    \n",
    "                scores = torch.cat(scores, dim=0).cpu()\n",
    "                ys = torch.cat(ys, dim=0).cpu()\n",
    "            \n",
    "            metrics = OODMetrics()\n",
    "            metrics.update(scores, ys)\n",
    "            r = metrics.compute()\n",
    "            r.update({\n",
    "                \"Method\": detector_name,\n",
    "                \"Dataset\": data_name\n",
    "            })\n",
    "            print(r)\n",
    "            results.append(r)\n",
    "    \n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "930a358f-c6ef-4fe9-9b1a-c305a37ffb70",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "ExecuteTime": {
     "end_time": "2023-08-25T19:35:37.220938583Z",
     "start_time": "2023-08-25T15:56:11.074962212Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 33.39it/s, loss=0.122] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 96.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 32.56it/s, loss=0.0174]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 31.68it/s, loss=0.0793] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 31.68it/s, loss=0.0155] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 31.75it/s, loss=0.00992]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.00%\n",
      "color\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 31.73it/s, loss=0.0279]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 31.71it/s, loss=0.0278] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 31.73it/s, loss=0.0183] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 31.53it/s, loss=0.0913] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 31.39it/s, loss=0.00969]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.00%\n",
      "fruit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:03<00:00, 31.58it/s, loss=0.0167]  \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:03<00:00, 31.35it/s, loss=0.000767]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:03<00:00, 31.39it/s, loss=0.00339] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:06<00:00, 30.23it/s, loss=0.000508]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:03<00:00, 31.30it/s, loss=0.000401]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:03<00:00, 31.54it/s, loss=5.19e-5] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:03<00:00, 31.41it/s, loss=0.000899]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:03<00:00, 31.43it/s, loss=2.92e-5] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:03<00:00, 31.39it/s, loss=2.07e-5] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:03<00:00, 31.35it/s, loss=0.0002]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.92%\n",
      "Computing principal space ...\n",
      "Computing alpha ...\n",
      "self.alpha=6.7493\n",
      "Fitting with temperature scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial T/NLL: 1.000/0.002\n",
      "Optimal temperature: 0.9287034273147583\n",
      "NLL after scaling: 0.00'\n",
      "Initial T/NLL: 1.000/0.002\n",
      "Optimal temperature: 0.8900877833366394\n",
      "NLL after scaling: 0.00'\n",
      "self.scorer_label.t=Parameter containing:\n",
      "tensor(0.9287, requires_grad=True)\n",
      "self.scorer_color.t=Parameter containing:\n",
      "tensor(0.8901, requires_grad=True)\n",
      "Fitting with temperature scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:84: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels).item()\n",
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:90: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels)\n",
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:97: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels).item()\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial T/NLL: 1.000/0.002\n",
      "Optimal temperature: 0.9287034273147583\n",
      "NLL after scaling: 0.00'\n",
      "Initial T/NLL: 1.000/0.002\n",
      "Optimal temperature: 0.8900877833366394\n",
      "NLL after scaling: 0.00'\n",
      "self.scorer_label.t=Parameter containing:\n",
      "tensor(0.9287, requires_grad=True)\n",
      "self.scorer_color.t=Parameter containing:\n",
      "tensor(0.8901, requires_grad=True)\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9955240488052368, 'AUPR-IN': 0.9991960525512695, 'AUPR-OUT': 0.9742469191551208, 'FPR95TPR': 0.006472492124885321, 'Method': 'ViM', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9999660849571228, 'AUPR-IN': 0.9999938011169434, 'AUPR-OUT': 0.999819278717041, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9997166395187378, 'AUPR-IN': 0.9999111294746399, 'AUPR-OUT': 0.9990662932395935, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9993784427642822, 'AUPR-IN': 0.9998874068260193, 'AUPR-OUT': 0.9963700771331787, 'FPR95TPR': 0.0016181230312213302, 'Method': 'ViM', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9999579787254333, 'AUPR-IN': 0.9999921917915344, 'AUPR-OUT': 0.9997791647911072, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9935081601142883, 'AUPR-IN': 0.9988241195678711, 'AUPR-OUT': 0.9650976657867432, 'FPR95TPR': 0.01779935322701931, 'Method': 'Mahalanobis', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9995629787445068, 'AUPR-IN': 0.9999189972877502, 'AUPR-OUT': 0.9977743625640869, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9991296529769897, 'AUPR-IN': 0.9997220039367676, 'AUPR-OUT': 0.9972711801528931, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9978535771369934, 'AUPR-IN': 0.9996137022972107, 'AUPR-OUT': 0.9874143004417419, 'FPR95TPR': 0.0021574972197413445, 'Method': 'Mahalanobis', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9996235370635986, 'AUPR-IN': 0.9999308586120605, 'AUPR-OUT': 0.9979695081710815, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.974128782749176, 'AUPR-IN': 0.995481550693512, 'AUPR-OUT': 0.8219413161277771, 'FPR95TPR': 0.1111111119389534, 'Method': 'Entropy', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9973080158233643, 'AUPR-IN': 0.9994874596595764, 'AUPR-OUT': 0.9883092641830444, 'FPR95TPR': 0.014563106931746006, 'Method': 'Entropy', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9801453948020935, 'AUPR-IN': 0.9940264821052551, 'AUPR-OUT': 0.9222509264945984, 'FPR95TPR': 0.06202804669737816, 'Method': 'Entropy', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.995293140411377, 'AUPR-IN': 0.9991511106491089, 'AUPR-OUT': 0.9706954956054688, 'FPR95TPR': 0.01564185507595539, 'Method': 'Entropy', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9965755939483643, 'AUPR-IN': 0.9993630051612854, 'AUPR-OUT': 0.98269122838974, 'FPR95TPR': 0.01564185507595539, 'Method': 'Entropy', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'LogicOOD+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'LogicOOD+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'LogicOOD+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'LogicOOD+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'LogicOOD+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.8222999572753906, 'AUPR-IN': 0.9722071886062622, 'AUPR-OUT': 0.6714127063751221, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.7466000318527222, 'AUPR-IN': 0.9603675603866577, 'AUPR-OUT': 0.6339208483695984, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.7166666984558105, 'AUPR-IN': 0.9299039244651794, 'AUPR-OUT': 0.6835643649101257, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.7782000303268433, 'AUPR-IN': 0.9653098583221436, 'AUPR-OUT': 0.6473767757415771, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.7517499923706055, 'AUPR-IN': 0.9611729979515076, 'AUPR-OUT': 0.6359436511993408, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'Logic+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'Logic+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9999113082885742, 'AUPR-IN': 0.9999780654907227, 'AUPR-OUT': 0.9997304677963257, 'FPR95TPR': 0.0, 'Method': 'Logic+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'Logic+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'Logic+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9973933696746826, 'AUPR-IN': 0.9995213747024536, 'AUPR-OUT': 0.9865597486495972, 'FPR95TPR': 0.007011866196990013, 'Method': 'LogicOOD', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9969408512115479, 'AUPR-IN': 0.9994280338287354, 'AUPR-OUT': 0.9857648611068726, 'FPR95TPR': 0.01887810230255127, 'Method': 'LogicOOD', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9863297939300537, 'AUPR-IN': 0.9957278370857239, 'AUPR-OUT': 0.9550846815109253, 'FPR95TPR': 0.0663430392742157, 'Method': 'LogicOOD', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9980071187019348, 'AUPR-IN': 0.9996349811553955, 'AUPR-OUT': 0.9893898963928223, 'FPR95TPR': 0.006472492124885321, 'Method': 'LogicOOD', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9965749979019165, 'AUPR-IN': 0.9993695616722107, 'AUPR-OUT': 0.9824432730674744, 'FPR95TPR': 0.014023732393980026, 'Method': 'LogicOOD', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9973359704017639, 'AUPR-IN': 0.9995108842849731, 'AUPR-OUT': 0.9862580299377441, 'FPR95TPR': 0.007011866196990013, 'Method': 'LogicOODT', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9969764351844788, 'AUPR-IN': 0.9994336366653442, 'AUPR-OUT': 0.9860585927963257, 'FPR95TPR': 0.01672060415148735, 'Method': 'LogicOODT', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9867240190505981, 'AUPR-IN': 0.995841383934021, 'AUPR-OUT': 0.9568028450012207, 'FPR95TPR': 0.06364616751670837, 'Method': 'LogicOODT', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9980489015579224, 'AUPR-IN': 0.9996421337127686, 'AUPR-OUT': 0.9896994233131409, 'FPR95TPR': 0.005933117587119341, 'Method': 'LogicOODT', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9966450929641724, 'AUPR-IN': 0.9993818402290344, 'AUPR-OUT': 0.9828606843948364, 'FPR95TPR': 0.01564185507595539, 'Method': 'LogicOODT', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 0.9999999403953552, 'FPR95TPR': 0.0, 'Method': 'LogicOODT+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 0.9999999403953552, 'FPR95TPR': 0.0, 'Method': 'LogicOODT+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 0.9999999403953552, 'FPR95TPR': 0.0, 'Method': 'LogicOODT+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 0.9999999403953552, 'FPR95TPR': 0.0, 'Method': 'LogicOODT+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 0.9999999403953552, 'FPR95TPR': 0.0, 'Method': 'LogicOODT+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.995974063873291, 'AUPR-IN': 0.9992489218711853, 'AUPR-OUT': 0.9811512231826782, 'FPR95TPR': 0.02103559859097004, 'Method': 'Ensemble', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9964115023612976, 'AUPR-IN': 0.999322235584259, 'AUPR-OUT': 0.9841747283935547, 'FPR95TPR': 0.02103559859097004, 'Method': 'Ensemble', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9852174520492554, 'AUPR-IN': 0.9953171014785767, 'AUPR-OUT': 0.9531456232070923, 'FPR95TPR': 0.06850054115056992, 'Method': 'Ensemble', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9974721074104309, 'AUPR-IN': 0.9995319843292236, 'AUPR-OUT': 0.9873853921890259, 'FPR95TPR': 0.012405609712004662, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9960547685623169, 'AUPR-IN': 0.9992669820785522, 'AUPR-OUT': 0.9807923436164856, 'FPR95TPR': 0.01887810230255127, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.974003791809082, 'AUPR-IN': 0.9954513907432556, 'AUPR-OUT': 0.8213374614715576, 'FPR95TPR': 0.10517799109220505, 'Method': 'MSP', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9968878030776978, 'AUPR-IN': 0.9994013905525208, 'AUPR-OUT': 0.9871162176132202, 'FPR95TPR': 0.017259977757930756, 'Method': 'MSP', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9799785614013672, 'AUPR-IN': 0.993939995765686, 'AUPR-OUT': 0.9224511981010437, 'FPR95TPR': 0.058252427726984024, 'Method': 'MSP', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9950487613677979, 'AUPR-IN': 0.9991012811660767, 'AUPR-OUT': 0.9699352979660034, 'FPR95TPR': 0.018338726833462715, 'Method': 'MSP', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9962692260742188, 'AUPR-IN': 0.9992998242378235, 'AUPR-OUT': 0.9819348454475403, 'FPR95TPR': 0.017259977757930756, 'Method': 'MSP', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.8262616395950317, 'AUPR-IN': 0.9670532941818237, 'AUPR-OUT': 0.3458539843559265, 'FPR95TPR': 0.9341963529586792, 'Method': 'ReAct', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9791673421859741, 'AUPR-IN': 0.9958999752998352, 'AUPR-OUT': 0.9118562340736389, 'FPR95TPR': 0.08683926612138748, 'Method': 'ReAct', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9286790490150452, 'AUPR-IN': 0.9764724969863892, 'AUPR-OUT': 0.757340669631958, 'FPR95TPR': 0.39428263902664185, 'Method': 'ReAct', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9131190776824951, 'AUPR-IN': 0.9820605516433716, 'AUPR-OUT': 0.6323565244674683, 'FPR95TPR': 0.49352750182151794, 'Method': 'ReAct', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.965218722820282, 'AUPR-IN': 0.9931831955909729, 'AUPR-OUT': 0.8507922887802124, 'FPR95TPR': 0.18230852484703064, 'Method': 'ReAct', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9789860248565674, 'AUPR-IN': 0.9962856769561768, 'AUPR-OUT': 0.8742179870605469, 'FPR95TPR': 0.10679611563682556, 'Method': 'Energy', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.997907280921936, 'AUPR-IN': 0.9996166229248047, 'AUPR-OUT': 0.9888945817947388, 'FPR95TPR': 0.005393743049353361, 'Method': 'Energy', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.979380190372467, 'AUPR-IN': 0.993809163570404, 'AUPR-OUT': 0.9230391979217529, 'FPR95TPR': 0.10571736842393875, 'Method': 'Energy', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9950852990150452, 'AUPR-IN': 0.9991205334663391, 'AUPR-OUT': 0.9709051847457886, 'FPR95TPR': 0.009708737954497337, 'Method': 'Energy', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9957178831100464, 'AUPR-IN': 0.9992284774780273, 'AUPR-OUT': 0.9745836853981018, 'FPR95TPR': 0.009708737954497337, 'Method': 'Energy', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9789908528327942, 'AUPR-IN': 0.9962865114212036, 'AUPR-OUT': 0.874208927154541, 'FPR95TPR': 0.10787486284971237, 'Method': 'MaxLogit', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9978959560394287, 'AUPR-IN': 0.9996142387390137, 'AUPR-OUT': 0.9888749718666077, 'FPR95TPR': 0.005393743049353361, 'Method': 'MaxLogit', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9794357419013977, 'AUPR-IN': 0.9938278198242188, 'AUPR-OUT': 0.9231774806976318, 'FPR95TPR': 0.10571736842393875, 'Method': 'MaxLogit', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9950924515724182, 'AUPR-IN': 0.9991216659545898, 'AUPR-OUT': 0.9709559082984924, 'FPR95TPR': 0.009708737954497337, 'Method': 'MaxLogit', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUROC': 0.9957358241081238, 'AUPR-IN': 0.9992316365242004, 'AUPR-OUT': 0.9746943116188049, 'FPR95TPR': 0.009708737954497337, 'Method': 'MaxLogit', 'Dataset': 'TinyImageNetResize'}\n",
      "label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 31.45it/s, loss=0.0772]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 91.59%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 31.48it/s, loss=0.0216]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 98.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 31.47it/s, loss=0.0511] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 95.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 31.30it/s, loss=0.0587] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 31.49it/s, loss=0.041]  \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.84%\n",
      "color\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 31.35it/s, loss=0.0743]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 98.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 31.54it/s, loss=0.00959]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 31.51it/s, loss=0.0325] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.27it/s, loss=0.00593]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.16it/s, loss=0.0109]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.84%\n",
      "fruit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:03<00:00, 31.35it/s, loss=0.00263] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:03<00:00, 31.36it/s, loss=0.00228] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:03<00:00, 31.38it/s, loss=0.00864] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.20it/s, loss=0.000299]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.24it/s, loss=0.00299] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.13it/s, loss=0.000368]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.23it/s, loss=0.000425]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:03<00:00, 31.27it/s, loss=0.000371]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.24it/s, loss=0.000137]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:03<00:00, 31.29it/s, loss=5.28e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 100.00%\n",
      "Computing principal space ...\n",
      "Computing alpha ...\n",
      "self.alpha=6.5695\n",
      "Fitting with temperature scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial T/NLL: 1.000/0.007\n",
      "Optimal temperature: 0.9019112586975098\n",
      "NLL after scaling: 0.00'\n",
      "Initial T/NLL: 1.000/0.003\n",
      "Optimal temperature: 0.9983204007148743\n",
      "NLL after scaling: 0.00'\n",
      "self.scorer_label.t=Parameter containing:\n",
      "tensor(0.9019, requires_grad=True)\n",
      "self.scorer_color.t=Parameter containing:\n",
      "tensor(0.9983, requires_grad=True)\n",
      "Fitting with temperature scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:84: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels).item()\n",
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:90: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels)\n",
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:97: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels).item()\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial T/NLL: 1.000/0.007\n",
      "Optimal temperature: 0.9019112586975098\n",
      "NLL after scaling: 0.00'\n",
      "Initial T/NLL: 1.000/0.003\n",
      "Optimal temperature: 0.9983204007148743\n",
      "NLL after scaling: 0.00'\n",
      "self.scorer_label.t=Parameter containing:\n",
      "tensor(0.9019, requires_grad=True)\n",
      "self.scorer_color.t=Parameter containing:\n",
      "tensor(0.9983, requires_grad=True)\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9995129108428955, 'AUPR-IN': 0.9999082088470459, 'AUPR-OUT': 0.9985712766647339, 'FPR95TPR': 0.0005393743049353361, 'Method': 'ViM', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9997077584266663, 'AUPR-IN': 0.9999456405639648, 'AUPR-OUT': 0.9998407363891602, 'FPR95TPR': 0.0005393743049353361, 'Method': 'ViM', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9996517896652222, 'AUPR-IN': 0.9998844265937805, 'AUPR-OUT': 0.999690592288971, 'FPR95TPR': 0.0005393743049353361, 'Method': 'ViM', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9995625615119934, 'AUPR-IN': 0.9999175071716309, 'AUPR-OUT': 0.9989448189735413, 'FPR95TPR': 0.0005393743049353361, 'Method': 'ViM', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9996263384819031, 'AUPR-IN': 0.9999306797981262, 'AUPR-OUT': 0.9993491768836975, 'FPR95TPR': 0.0005393743049353361, 'Method': 'ViM', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9994142055511475, 'AUPR-IN': 0.9998939633369446, 'AUPR-OUT': 0.9966421127319336, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9999496936798096, 'AUPR-IN': 0.9999909400939941, 'AUPR-OUT': 0.9997029304504395, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9998354315757751, 'AUPR-IN': 0.9999478459358215, 'AUPR-OUT': 0.9994657635688782, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9995867609977722, 'AUPR-IN': 0.9999239444732666, 'AUPR-OUT': 0.9977860450744629, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.999839186668396, 'AUPR-IN': 0.9999709129333496, 'AUPR-OUT': 0.9990605115890503, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9481156468391418, 'AUPR-IN': 0.9903486967086792, 'AUPR-OUT': 0.7364438772201538, 'FPR95TPR': 0.36192017793655396, 'Method': 'Entropy', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9732216000556946, 'AUPR-IN': 0.9947025775909424, 'AUPR-OUT': 0.8947136402130127, 'FPR95TPR': 0.1456310749053955, 'Method': 'Entropy', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9420382380485535, 'AUPR-IN': 0.9819962978363037, 'AUPR-OUT': 0.795377254486084, 'FPR95TPR': 0.4018338620662689, 'Method': 'Entropy', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9621483087539673, 'AUPR-IN': 0.9927911758422852, 'AUPR-OUT': 0.826103150844574, 'FPR95TPR': 0.21736785769462585, 'Method': 'Entropy', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9700291752815247, 'AUPR-IN': 0.9941962957382202, 'AUPR-OUT': 0.8629049062728882, 'FPR95TPR': 0.15857605636119843, 'Method': 'Entropy', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9991908669471741, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.9993175864219666, 'FPR95TPR': 0.0016181230312213302, 'Method': 'LogicOOD+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9991908669471741, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.9993175864219666, 'FPR95TPR': 0.0016181230312213302, 'Method': 'LogicOOD+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9991881251335144, 'AUPR-IN': 0.9997330904006958, 'AUPR-OUT': 0.9993826150894165, 'FPR95TPR': 0.0016181230312213302, 'Method': 'LogicOOD+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9991908669471741, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.9993175864219666, 'FPR95TPR': 0.0016181230312213302, 'Method': 'LogicOOD+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9991908669471741, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.9993175864219666, 'FPR95TPR': 0.0016181230312213302, 'Method': 'LogicOOD+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.6803409457206726, 'AUPR-IN': 0.9497172236442566, 'AUPR-OUT': 0.6117992401123047, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.6776909232139587, 'AUPR-IN': 0.9492965936660767, 'AUPR-OUT': 0.6110793352127075, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.7235703468322754, 'AUPR-IN': 0.9312200546264648, 'AUPR-OUT': 0.6859838366508484, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.6637409329414368, 'AUPR-IN': 0.9470793008804321, 'AUPR-OUT': 0.6074365973472595, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.6817909479141235, 'AUPR-IN': 0.9499472379684448, 'AUPR-OUT': 0.6121971011161804, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9991909265518188, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.999317467212677, 'FPR95TPR': 0.0016181230312213302, 'Method': 'Logic+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9991909265518188, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.999317467212677, 'FPR95TPR': 0.0016181230312213302, 'Method': 'Logic+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9988362789154053, 'AUPR-IN': 0.9996462464332581, 'AUPR-OUT': 0.9983129501342773, 'FPR95TPR': 0.0016181230312213302, 'Method': 'Logic+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9991909265518188, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.999317467212677, 'FPR95TPR': 0.0016181230312213302, 'Method': 'Logic+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9991909265518188, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.999317467212677, 'FPR95TPR': 0.0016181230312213302, 'Method': 'Logic+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.987162172794342, 'AUPR-IN': 0.9973532557487488, 'AUPR-OUT': 0.9475341439247131, 'FPR95TPR': 0.059331174939870834, 'Method': 'LogicOOD', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9897234439849854, 'AUPR-IN': 0.997804582118988, 'AUPR-OUT': 0.9614592790603638, 'FPR95TPR': 0.04314994439482689, 'Method': 'LogicOOD', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9776188135147095, 'AUPR-IN': 0.9927006959915161, 'AUPR-OUT': 0.9290883541107178, 'FPR95TPR': 0.1218985989689827, 'Method': 'LogicOOD', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9846006631851196, 'AUPR-IN': 0.9968280792236328, 'AUPR-OUT': 0.9371532797813416, 'FPR95TPR': 0.08306364715099335, 'Method': 'LogicOOD', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9878740310668945, 'AUPR-IN': 0.9974737763404846, 'AUPR-OUT': 0.9502354860305786, 'FPR95TPR': 0.05447680875658989, 'Method': 'LogicOOD', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9886711239814758, 'AUPR-IN': 0.9976142048835754, 'AUPR-OUT': 0.9560796022415161, 'FPR95TPR': 0.051779936999082565, 'Method': 'LogicOODT', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9906858801841736, 'AUPR-IN': 0.9979797601699829, 'AUPR-OUT': 0.9657940864562988, 'FPR95TPR': 0.03991369903087616, 'Method': 'LogicOODT', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.98003089427948, 'AUPR-IN': 0.99339359998703, 'AUPR-OUT': 0.938724160194397, 'FPR95TPR': 0.10517799109220505, 'Method': 'LogicOODT', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.986142098903656, 'AUPR-IN': 0.9971001148223877, 'AUPR-OUT': 0.9450072646141052, 'FPR95TPR': 0.07173678278923035, 'Method': 'LogicOODT', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9889302849769592, 'AUPR-IN': 0.9976643919944763, 'AUPR-OUT': 0.9556166529655457, 'FPR95TPR': 0.05070118606090546, 'Method': 'LogicOODT', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9991909265518188, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.999317467212677, 'FPR95TPR': 0.0016181230312213302, 'Method': 'LogicOODT+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9991909265518188, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.999317467212677, 'FPR95TPR': 0.0016181230312213302, 'Method': 'LogicOODT+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.999163806438446, 'AUPR-IN': 0.9997251629829407, 'AUPR-OUT': 0.9993067979812622, 'FPR95TPR': 0.0016181230312213302, 'Method': 'LogicOODT+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9991909265518188, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.999317467212677, 'FPR95TPR': 0.0016181230312213302, 'Method': 'LogicOODT+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9991909265518188, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.999317467212677, 'FPR95TPR': 0.0016181230312213302, 'Method': 'LogicOODT+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9868317246437073, 'AUPR-IN': 0.9973793029785156, 'AUPR-OUT': 0.9462571144104004, 'FPR95TPR': 0.06364616751670837, 'Method': 'Ensemble', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9898402690887451, 'AUPR-IN': 0.9979438781738281, 'AUPR-OUT': 0.9610873460769653, 'FPR95TPR': 0.04530744254589081, 'Method': 'Ensemble', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.97552490234375, 'AUPR-IN': 0.9920281171798706, 'AUPR-OUT': 0.9238711595535278, 'FPR95TPR': 0.1321467161178589, 'Method': 'Ensemble', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9845520853996277, 'AUPR-IN': 0.9969487190246582, 'AUPR-OUT': 0.9363572001457214, 'FPR95TPR': 0.08576051890850067, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9877076148986816, 'AUPR-IN': 0.9975461959838867, 'AUPR-OUT': 0.9489941596984863, 'FPR95TPR': 0.05447680875658989, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9460898637771606, 'AUPR-IN': 0.9896906614303589, 'AUPR-OUT': 0.7350361347198486, 'FPR95TPR': 0.3651564121246338, 'Method': 'MSP', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9724336862564087, 'AUPR-IN': 0.9943135976791382, 'AUPR-OUT': 0.8962463736534119, 'FPR95TPR': 0.1434735655784607, 'Method': 'MSP', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9411736726760864, 'AUPR-IN': 0.9813148975372314, 'AUPR-OUT': 0.7969081401824951, 'FPR95TPR': 0.3991369903087616, 'Method': 'MSP', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9605837464332581, 'AUPR-IN': 0.9922665953636169, 'AUPR-OUT': 0.8256150484085083, 'FPR95TPR': 0.21682848036289215, 'Method': 'MSP', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9690685272216797, 'AUPR-IN': 0.9937998056411743, 'AUPR-OUT': 0.8645534515380859, 'FPR95TPR': 0.15318231284618378, 'Method': 'MSP', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.8212741613388062, 'AUPR-IN': 0.9603978395462036, 'AUPR-OUT': 0.42674487829208374, 'FPR95TPR': 0.762135922908783, 'Method': 'ReAct', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.8720811605453491, 'AUPR-IN': 0.9674898386001587, 'AUPR-OUT': 0.7066637873649597, 'FPR95TPR': 0.4137001037597656, 'Method': 'ReAct', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.8763440251350403, 'AUPR-IN': 0.9540117979049683, 'AUPR-OUT': 0.6904088854789734, 'FPR95TPR': 0.5668824315071106, 'Method': 'ReAct', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.8905940055847168, 'AUPR-IN': 0.9750178456306458, 'AUPR-OUT': 0.6719011068344116, 'FPR95TPR': 0.46278315782546997, 'Method': 'ReAct', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.8943473100662231, 'AUPR-IN': 0.9750851392745972, 'AUPR-OUT': 0.7119766473770142, 'FPR95TPR': 0.40399137139320374, 'Method': 'ReAct', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9512245059013367, 'AUPR-IN': 0.9910913705825806, 'AUPR-OUT': 0.7466518878936768, 'FPR95TPR': 0.35922330617904663, 'Method': 'Energy', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9566409587860107, 'AUPR-IN': 0.9915525317192078, 'AUPR-OUT': 0.8123210072517395, 'FPR95TPR': 0.24217906594276428, 'Method': 'Energy', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9342072010040283, 'AUPR-IN': 0.979788064956665, 'AUPR-OUT': 0.7684139609336853, 'FPR95TPR': 0.47357064485549927, 'Method': 'Energy', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9554721117019653, 'AUPR-IN': 0.9916517734527588, 'AUPR-OUT': 0.7845401763916016, 'FPR95TPR': 0.2777777910232544, 'Method': 'Energy', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9562484622001648, 'AUPR-IN': 0.9916430115699768, 'AUPR-OUT': 0.7885265946388245, 'FPR95TPR': 0.2588996887207031, 'Method': 'Energy', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9511017799377441, 'AUPR-IN': 0.9910591244697571, 'AUPR-OUT': 0.746557354927063, 'FPR95TPR': 0.35922330617904663, 'Method': 'MaxLogit', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9577279090881348, 'AUPR-IN': 0.9918164610862732, 'AUPR-OUT': 0.8141869902610779, 'FPR95TPR': 0.24056094884872437, 'Method': 'MaxLogit', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9345536828041077, 'AUPR-IN': 0.9799222350120544, 'AUPR-OUT': 0.7689082026481628, 'FPR95TPR': 0.47411003708839417, 'Method': 'MaxLogit', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9557846784591675, 'AUPR-IN': 0.9917206168174744, 'AUPR-OUT': 0.7851945757865906, 'FPR95TPR': 0.2772383987903595, 'Method': 'MaxLogit', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUROC': 0.957070529460907, 'AUPR-IN': 0.9918356537818909, 'AUPR-OUT': 0.7900141477584839, 'FPR95TPR': 0.2588996887207031, 'Method': 'MaxLogit', 'Dataset': 'TinyImageNetResize'}\n",
      "label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.24it/s, loss=0.0489]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 97.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.23it/s, loss=0.0448]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 98.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.18it/s, loss=0.0296] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 97.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.24it/s, loss=0.0191] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.20it/s, loss=0.0186] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.84%\n",
      "color\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.19it/s, loss=0.0133]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 94.77%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 30.96it/s, loss=0.014]  \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.14it/s, loss=0.00532]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.09it/s, loss=0.00395]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 30.99it/s, loss=0.00158]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.00%\n",
      "fruit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.24it/s, loss=0.00825] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.06it/s, loss=0.000496]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.18it/s, loss=0.00046] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.13it/s, loss=0.000369]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.13it/s, loss=5.74e-5] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.18it/s, loss=0.000123]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.09it/s, loss=0.000142]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.13it/s, loss=0.000108]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.11it/s, loss=0.000221]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.13it/s, loss=7.96e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.96%\n",
      "Computing principal space ...\n",
      "Computing alpha ...\n",
      "self.alpha=6.2339\n",
      "Fitting with temperature scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial T/NLL: 1.000/0.009\n",
      "Optimal temperature: 0.8969682455062866\n",
      "NLL after scaling: 0.01'\n",
      "Initial T/NLL: 1.000/0.002\n",
      "Optimal temperature: 0.8671324849128723\n",
      "NLL after scaling: 0.00'\n",
      "self.scorer_label.t=Parameter containing:\n",
      "tensor(0.8970, requires_grad=True)\n",
      "self.scorer_color.t=Parameter containing:\n",
      "tensor(0.8671, requires_grad=True)\n",
      "Fitting with temperature scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:84: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels).item()\n",
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:90: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels)\n",
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:97: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels).item()\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial T/NLL: 1.000/0.009\n",
      "Optimal temperature: 0.8969682455062866\n",
      "NLL after scaling: 0.01'\n",
      "Initial T/NLL: 1.000/0.002\n",
      "Optimal temperature: 0.8671324849128723\n",
      "NLL after scaling: 0.00'\n",
      "self.scorer_label.t=Parameter containing:\n",
      "tensor(0.8970, requires_grad=True)\n",
      "self.scorer_color.t=Parameter containing:\n",
      "tensor(0.8671, requires_grad=True)\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9960405826568604, 'AUPR-IN': 0.9992837905883789, 'AUPR-OUT': 0.9781866073608398, 'FPR95TPR': 0.0021574972197413445, 'Method': 'ViM', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9997240900993347, 'AUPR-IN': 0.9999487996101379, 'AUPR-OUT': 0.9999136328697205, 'FPR95TPR': 0.0005393743049353361, 'Method': 'ViM', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9988812208175659, 'AUPR-IN': 0.9996422529220581, 'AUPR-OUT': 0.9971420764923096, 'FPR95TPR': 0.0005393743049353361, 'Method': 'ViM', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9990718364715576, 'AUPR-IN': 0.9998298287391663, 'AUPR-OUT': 0.9957214593887329, 'FPR95TPR': 0.0005393743049353361, 'Method': 'ViM', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.999711811542511, 'AUPR-IN': 0.9999464154243469, 'AUPR-OUT': 0.9998164176940918, 'FPR95TPR': 0.0005393743049353361, 'Method': 'ViM', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9953465461730957, 'AUPR-IN': 0.9991620779037476, 'AUPR-OUT': 0.9736899733543396, 'FPR95TPR': 0.007011866196990013, 'Method': 'Mahalanobis', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9999299049377441, 'AUPR-IN': 0.9999869465827942, 'AUPR-OUT': 0.9996362924575806, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9979566335678101, 'AUPR-IN': 0.9993600845336914, 'AUPR-OUT': 0.9933404922485352, 'FPR95TPR': 0.0016181230312213302, 'Method': 'Mahalanobis', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.99917072057724, 'AUPR-IN': 0.999850332736969, 'AUPR-OUT': 0.9951915144920349, 'FPR95TPR': 0.0010787486098706722, 'Method': 'Mahalanobis', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9999206066131592, 'AUPR-IN': 0.9999853372573853, 'AUPR-OUT': 0.9995737075805664, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.954403281211853, 'AUPR-IN': 0.9915785193443298, 'AUPR-OUT': 0.7625861763954163, 'FPR95TPR': 0.3193095922470093, 'Method': 'Entropy', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.986853837966919, 'AUPR-IN': 0.9974570274353027, 'AUPR-OUT': 0.9457696676254272, 'FPR95TPR': 0.06364616751670837, 'Method': 'Entropy', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9412622451782227, 'AUPR-IN': 0.9817191362380981, 'AUPR-OUT': 0.7897113561630249, 'FPR95TPR': 0.41639697551727295, 'Method': 'Entropy', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9794853925704956, 'AUPR-IN': 0.9962561726570129, 'AUPR-OUT': 0.8842527270317078, 'FPR95TPR': 0.10302049666643143, 'Method': 'Entropy', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.988165557384491, 'AUPR-IN': 0.9977767467498779, 'AUPR-OUT': 0.9365875124931335, 'FPR95TPR': 0.04584681615233421, 'Method': 'Entropy', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'LogicOOD+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'LogicOOD+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9999403953552246, 'AUPR-IN': 0.9999808073043823, 'AUPR-OUT': 0.9998133182525635, 'FPR95TPR': 0.0, 'Method': 'LogicOOD+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'LogicOOD+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'LogicOOD+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.8700500130653381, 'AUPR-IN': 0.9796754717826843, 'AUPR-OUT': 0.7081742286682129, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.7669000029563904, 'AUPR-IN': 0.9635424613952637, 'AUPR-OUT': 0.6422652006149292, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.7420212626457214, 'AUPR-IN': 0.9361766576766968, 'AUPR-OUT': 0.6945843696594238, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.7542999982833862, 'AUPR-IN': 0.9615718126296997, 'AUPR-OUT': 0.6369680762290955, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.7638499736785889, 'AUPR-IN': 0.963065505027771, 'AUPR-OUT': 0.6409457325935364, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'Logic+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'Logic+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9979609847068787, 'AUPR-IN': 0.9994955658912659, 'AUPR-OUT': 0.9938732385635376, 'FPR95TPR': 0.0, 'Method': 'Logic+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'Logic+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'Logic+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.997276246547699, 'AUPR-IN': 0.9994987845420837, 'AUPR-OUT': 0.9840850830078125, 'FPR95TPR': 0.010248112492263317, 'Method': 'LogicOOD', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9958716630935669, 'AUPR-IN': 0.9992181062698364, 'AUPR-OUT': 0.9812805652618408, 'FPR95TPR': 0.01779935322701931, 'Method': 'LogicOOD', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9772192239761353, 'AUPR-IN': 0.9930054545402527, 'AUPR-OUT': 0.9140953421592712, 'FPR95TPR': 0.11866234987974167, 'Method': 'LogicOOD', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9937615990638733, 'AUPR-IN': 0.9988552331924438, 'AUPR-OUT': 0.9615440964698792, 'FPR95TPR': 0.01887810230255127, 'Method': 'LogicOOD', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9948795437812805, 'AUPR-IN': 0.9990534782409668, 'AUPR-OUT': 0.9701821208000183, 'FPR95TPR': 0.01779935322701931, 'Method': 'LogicOOD', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9969805479049683, 'AUPR-IN': 0.9994423389434814, 'AUPR-OUT': 0.9828199148178101, 'FPR95TPR': 0.012944984249770641, 'Method': 'LogicOODT', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9953807592391968, 'AUPR-IN': 0.9991183876991272, 'AUPR-OUT': 0.9796996116638184, 'FPR95TPR': 0.018338726833462715, 'Method': 'LogicOODT', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9761598110198975, 'AUPR-IN': 0.9926518797874451, 'AUPR-OUT': 0.9116194248199463, 'FPR95TPR': 0.12351672351360321, 'Method': 'LogicOODT', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9932124614715576, 'AUPR-IN': 0.9987452626228333, 'AUPR-OUT': 0.9596222639083862, 'FPR95TPR': 0.020496224984526634, 'Method': 'LogicOODT', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9944502711296082, 'AUPR-IN': 0.998965322971344, 'AUPR-OUT': 0.968946099281311, 'FPR95TPR': 0.01887810230255127, 'Method': 'LogicOODT', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'LogicOODT+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'LogicOODT+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9996633529663086, 'AUPR-IN': 0.9999035000801086, 'AUPR-OUT': 0.9981521964073181, 'FPR95TPR': 0.0, 'Method': 'LogicOODT+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'LogicOODT+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'LogicOODT+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9927886724472046, 'AUPR-IN': 0.9985624551773071, 'AUPR-OUT': 0.9692464470863342, 'FPR95TPR': 0.02535059303045273, 'Method': 'Ensemble', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9945687651634216, 'AUPR-IN': 0.9989120960235596, 'AUPR-OUT': 0.978940486907959, 'FPR95TPR': 0.018338726833462715, 'Method': 'Ensemble', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9735813736915588, 'AUPR-IN': 0.9915537238121033, 'AUPR-OUT': 0.9066855907440186, 'FPR95TPR': 0.1337648332118988, 'Method': 'Ensemble', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9921449422836304, 'AUPR-IN': 0.9985054731369019, 'AUPR-OUT': 0.9571071267127991, 'FPR95TPR': 0.02535059303045273, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9935109615325928, 'AUPR-IN': 0.9987366795539856, 'AUPR-OUT': 0.9676161408424377, 'FPR95TPR': 0.01995684951543808, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9511147141456604, 'AUPR-IN': 0.9906767010688782, 'AUPR-OUT': 0.7559716701507568, 'FPR95TPR': 0.3317151963710785, 'Method': 'MSP', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9844019412994385, 'AUPR-IN': 0.9967578649520874, 'AUPR-OUT': 0.9431465864181519, 'FPR95TPR': 0.06957928836345673, 'Method': 'MSP', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9390836954116821, 'AUPR-IN': 0.9804702997207642, 'AUPR-OUT': 0.7877273559570312, 'FPR95TPR': 0.41639697551727295, 'Method': 'MSP', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9770188331604004, 'AUPR-IN': 0.995622992515564, 'AUPR-OUT': 0.8801875710487366, 'FPR95TPR': 0.11218985915184021, 'Method': 'MSP', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9858132004737854, 'AUPR-IN': 0.9971391558647156, 'AUPR-OUT': 0.9338618516921997, 'FPR95TPR': 0.051779936999082565, 'Method': 'MSP', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.8643395304679871, 'AUPR-IN': 0.974394679069519, 'AUPR-OUT': 0.38798588514328003, 'FPR95TPR': 0.8964401483535767, 'Method': 'ReAct', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9264237880706787, 'AUPR-IN': 0.9768667221069336, 'AUPR-OUT': 0.8397928476333618, 'FPR95TPR': 0.20496223866939545, 'Method': 'ReAct', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.8667675852775574, 'AUPR-IN': 0.9484155774116516, 'AUPR-OUT': 0.6371167302131653, 'FPR95TPR': 0.6208198666572571, 'Method': 'ReAct', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.8938062787055969, 'AUPR-IN': 0.9761217832565308, 'AUPR-OUT': 0.5528818964958191, 'FPR95TPR': 0.5679611563682556, 'Method': 'ReAct', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9170657992362976, 'AUPR-IN': 0.9758175611495972, 'AUPR-OUT': 0.7761385440826416, 'FPR95TPR': 0.2562028169631958, 'Method': 'ReAct', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9682177305221558, 'AUPR-IN': 0.994273841381073, 'AUPR-OUT': 0.8243028521537781, 'FPR95TPR': 0.22006472945213318, 'Method': 'Energy', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9869304895401001, 'AUPR-IN': 0.9975727796554565, 'AUPR-OUT': 0.9363677501678467, 'FPR95TPR': 0.07173678278923035, 'Method': 'Energy', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9419493675231934, 'AUPR-IN': 0.9821406006813049, 'AUPR-OUT': 0.7898287773132324, 'FPR95TPR': 0.4217907190322876, 'Method': 'Energy', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9849662184715271, 'AUPR-IN': 0.9973210096359253, 'AUPR-OUT': 0.9052273035049438, 'FPR95TPR': 0.06580366939306259, 'Method': 'Energy', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9890308380126953, 'AUPR-IN': 0.9980173707008362, 'AUPR-OUT': 0.9302568435668945, 'FPR95TPR': 0.04207119718194008, 'Method': 'Energy', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9678979516029358, 'AUPR-IN': 0.994202196598053, 'AUPR-OUT': 0.8236467838287354, 'FPR95TPR': 0.2211434692144394, 'Method': 'MaxLogit', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9869093298912048, 'AUPR-IN': 0.9975634217262268, 'AUPR-OUT': 0.9364416599273682, 'FPR95TPR': 0.06957928836345673, 'Method': 'MaxLogit', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9420697689056396, 'AUPR-IN': 0.9821820855140686, 'AUPR-OUT': 0.7899958491325378, 'FPR95TPR': 0.4223301112651825, 'Method': 'MaxLogit', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9847757816314697, 'AUPR-IN': 0.9972808957099915, 'AUPR-OUT': 0.9047489166259766, 'FPR95TPR': 0.06580366939306259, 'Method': 'MaxLogit', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUROC': 0.9889322519302368, 'AUPR-IN': 0.9979936480522156, 'AUPR-OUT': 0.9301491975784302, 'FPR95TPR': 0.04368932172656059, 'Method': 'MaxLogit', 'Dataset': 'TinyImageNetResize'}\n",
      "label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.15it/s, loss=0.107] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 98.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.15it/s, loss=0.0304]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 98.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.10it/s, loss=0.0465] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 30.96it/s, loss=0.0342] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.15it/s, loss=0.0207] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.08%\n",
      "color\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.10it/s, loss=0.0585]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 97.09%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 30.99it/s, loss=0.0126] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 98.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 30.94it/s, loss=0.00569]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.04it/s, loss=0.0225] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.14it/s, loss=0.00937]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.00%\n",
      "fruit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.19it/s, loss=0.000924]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.88%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.05it/s, loss=0.00161] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.15it/s, loss=0.00106] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.11it/s, loss=0.000218]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.17it/s, loss=0.000189]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.20it/s, loss=0.000275]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.13it/s, loss=0.00037] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.08it/s, loss=6.54e-5] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.09it/s, loss=0.000449]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.09it/s, loss=3.01e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.94%\n",
      "Computing principal space ...\n",
      "Computing alpha ...\n",
      "self.alpha=6.4951\n",
      "Fitting with temperature scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial T/NLL: 1.000/0.039\n",
      "Optimal temperature: 1.0174537897109985\n",
      "NLL after scaling: 0.04'\n",
      "Initial T/NLL: 1.000/0.003\n",
      "Optimal temperature: 0.8663231730461121\n",
      "NLL after scaling: 0.00'\n",
      "self.scorer_label.t=Parameter containing:\n",
      "tensor(1.0175, requires_grad=True)\n",
      "self.scorer_color.t=Parameter containing:\n",
      "tensor(0.8663, requires_grad=True)\n",
      "Fitting with temperature scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:84: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels).item()\n",
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:90: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels)\n",
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:97: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels).item()\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial T/NLL: 1.000/0.039\n",
      "Optimal temperature: 1.0174537897109985\n",
      "NLL after scaling: 0.04'\n",
      "Initial T/NLL: 1.000/0.003\n",
      "Optimal temperature: 0.8663231730461121\n",
      "NLL after scaling: 0.00'\n",
      "self.scorer_label.t=Parameter containing:\n",
      "tensor(1.0175, requires_grad=True)\n",
      "self.scorer_color.t=Parameter containing:\n",
      "tensor(0.8663, requires_grad=True)\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9995220899581909, 'AUPR-IN': 0.9999120235443115, 'AUPR-OUT': 0.997461199760437, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9999998211860657, 'AUPR-IN': 1.0, 'AUPR-OUT': 0.9999993443489075, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9998899698257446, 'AUPR-IN': 0.9999653100967407, 'AUPR-OUT': 0.9996384978294373, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9998918771743774, 'AUPR-IN': 0.9999801516532898, 'AUPR-OUT': 0.9994147419929504, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9999977946281433, 'AUPR-IN': 0.9999996423721313, 'AUPR-OUT': 0.999988317489624, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9991468787193298, 'AUPR-IN': 0.9998428225517273, 'AUPR-OUT': 0.995545506477356, 'FPR95TPR': 0.0005393743049353361, 'Method': 'Mahalanobis', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.999999463558197, 'AUPR-IN': 0.9999998807907104, 'AUPR-OUT': 0.9999974370002747, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9999234080314636, 'AUPR-IN': 0.9999749660491943, 'AUPR-OUT': 0.9997676610946655, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9998216032981873, 'AUPR-IN': 0.9999672770500183, 'AUPR-OUT': 0.9990261793136597, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.999993085861206, 'AUPR-IN': 0.9999987483024597, 'AUPR-OUT': 0.9999626278877258, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9849072098731995, 'AUPR-IN': 0.9972105026245117, 'AUPR-OUT': 0.9214993119239807, 'FPR95TPR': 0.0668824166059494, 'Method': 'Entropy', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9912130832672119, 'AUPR-IN': 0.9983469843864441, 'AUPR-OUT': 0.952558159828186, 'FPR95TPR': 0.033980581909418106, 'Method': 'Entropy', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9576707482337952, 'AUPR-IN': 0.9865928292274475, 'AUPR-OUT': 0.8566855788230896, 'FPR95TPR': 0.25188779830932617, 'Method': 'Entropy', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9894822835922241, 'AUPR-IN': 0.998065173625946, 'AUPR-OUT': 0.9420924186706543, 'FPR95TPR': 0.03667745366692543, 'Method': 'Entropy', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.99122154712677, 'AUPR-IN': 0.9983718395233154, 'AUPR-OUT': 0.9505106806755066, 'FPR95TPR': 0.032901834696531296, 'Method': 'Entropy', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'LogicOOD+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'LogicOOD+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9999976754188538, 'AUPR-IN': 0.9999991655349731, 'AUPR-OUT': 0.9999929666519165, 'FPR95TPR': 0.0, 'Method': 'LogicOOD+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'LogicOOD+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'LogicOOD+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.7469000220298767, 'AUPR-IN': 0.9604144096374512, 'AUPR-OUT': 0.6340370178222656, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.7589499950408936, 'AUPR-IN': 0.962299108505249, 'AUPR-OUT': 0.6388764381408691, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.7084220051765442, 'AUPR-IN': 0.9278641939163208, 'AUPR-OUT': 0.6802449822425842, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.7267999649047852, 'AUPR-IN': 0.9572707414627075, 'AUPR-OUT': 0.6266739368438721, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.7588000297546387, 'AUPR-IN': 0.9622756242752075, 'AUPR-OUT': 0.6388140320777893, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'Logic+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'Logic+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9991134405136108, 'AUPR-IN': 0.9997806549072266, 'AUPR-OUT': 0.9973175525665283, 'FPR95TPR': 0.0, 'Method': 'Logic+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'Logic+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'Logic+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.980309009552002, 'AUPR-IN': 0.9964532852172852, 'AUPR-OUT': 0.8851540088653564, 'FPR95TPR': 0.10841424018144608, 'Method': 'LogicOOD', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9951577186584473, 'AUPR-IN': 0.9991049766540527, 'AUPR-OUT': 0.9745051860809326, 'FPR95TPR': 0.01779935322701931, 'Method': 'LogicOOD', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.97089684009552, 'AUPR-IN': 0.9909683465957642, 'AUPR-OUT': 0.8945606350898743, 'FPR95TPR': 0.18176914751529694, 'Method': 'LogicOOD', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9871530532836914, 'AUPR-IN': 0.9976911544799805, 'AUPR-OUT': 0.9202315211296082, 'FPR95TPR': 0.05447680875658989, 'Method': 'LogicOOD', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9931001663208008, 'AUPR-IN': 0.9987464547157288, 'AUPR-OUT': 0.9562574028968811, 'FPR95TPR': 0.02319309674203396, 'Method': 'LogicOOD', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9826213121414185, 'AUPR-IN': 0.9968632459640503, 'AUPR-OUT': 0.9005576372146606, 'FPR95TPR': 0.09277238696813583, 'Method': 'LogicOODT', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9953727722167969, 'AUPR-IN': 0.9991443753242493, 'AUPR-OUT': 0.9750357270240784, 'FPR95TPR': 0.016181230545043945, 'Method': 'LogicOODT', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.972812294960022, 'AUPR-IN': 0.9915682077407837, 'AUPR-OUT': 0.9030749797821045, 'FPR95TPR': 0.15803667902946472, 'Method': 'LogicOODT', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9886828660964966, 'AUPR-IN': 0.9979639053344727, 'AUPR-OUT': 0.9308452606201172, 'FPR95TPR': 0.035059332847595215, 'Method': 'LogicOODT', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9939151406288147, 'AUPR-IN': 0.9988926649093628, 'AUPR-OUT': 0.9617554545402527, 'FPR95TPR': 0.018338726833462715, 'Method': 'LogicOODT', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'LogicOODT+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'LogicOODT+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9998078346252441, 'AUPR-IN': 0.9999456405639648, 'AUPR-OUT': 0.9991527795791626, 'FPR95TPR': 0.0, 'Method': 'LogicOODT+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'LogicOODT+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'LogicOODT+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9794026613235474, 'AUPR-IN': 0.9962488412857056, 'AUPR-OUT': 0.8836674690246582, 'FPR95TPR': 0.10841424018144608, 'Method': 'Ensemble', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9941850900650024, 'AUPR-IN': 0.9988972544670105, 'AUPR-OUT': 0.9716991186141968, 'FPR95TPR': 0.022114347666502, 'Method': 'Ensemble', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9687345027923584, 'AUPR-IN': 0.9900678396224976, 'AUPR-OUT': 0.8913716673851013, 'FPR95TPR': 0.18338726460933685, 'Method': 'Ensemble', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9863778948783875, 'AUPR-IN': 0.9975192546844482, 'AUPR-OUT': 0.9185895919799805, 'FPR95TPR': 0.06040992587804794, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9917070865631104, 'AUPR-IN': 0.9984528422355652, 'AUPR-OUT': 0.9523539543151855, 'FPR95TPR': 0.0334412083029747, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9837460517883301, 'AUPR-IN': 0.9969317317008972, 'AUPR-OUT': 0.9188461899757385, 'FPR95TPR': 0.07011866569519043, 'Method': 'MSP', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9901952743530273, 'AUPR-IN': 0.9980685710906982, 'AUPR-OUT': 0.9518120884895325, 'FPR95TPR': 0.03775620460510254, 'Method': 'MSP', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9569165706634521, 'AUPR-IN': 0.9860116839408875, 'AUPR-OUT': 0.8575630187988281, 'FPR95TPR': 0.24973031878471375, 'Method': 'MSP', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.988353967666626, 'AUPR-IN': 0.9977775812149048, 'AUPR-OUT': 0.9406434297561646, 'FPR95TPR': 0.039374325424432755, 'Method': 'MSP', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9902893304824829, 'AUPR-IN': 0.9981200098991394, 'AUPR-OUT': 0.9501046538352966, 'FPR95TPR': 0.0334412083029747, 'Method': 'MSP', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.8557148575782776, 'AUPR-IN': 0.9724694490432739, 'AUPR-OUT': 0.38887912034988403, 'FPR95TPR': 0.8764832615852356, 'Method': 'ReAct', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9193759560585022, 'AUPR-IN': 0.9815906882286072, 'AUPR-OUT': 0.7511904835700989, 'FPR95TPR': 0.323085218667984, 'Method': 'ReAct', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.8797062039375305, 'AUPR-IN': 0.9599062204360962, 'AUPR-OUT': 0.6005911827087402, 'FPR95TPR': 0.6747573018074036, 'Method': 'ReAct', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.8813806772232056, 'AUPR-IN': 0.9747152924537659, 'AUPR-OUT': 0.5264907479286194, 'FPR95TPR': 0.5177993774414062, 'Method': 'ReAct', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9168766736984253, 'AUPR-IN': 0.9815899729728699, 'AUPR-OUT': 0.708288311958313, 'FPR95TPR': 0.3327939510345459, 'Method': 'ReAct', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9879730343818665, 'AUPR-IN': 0.9978257417678833, 'AUPR-OUT': 0.9320167899131775, 'FPR95TPR': 0.049622438848018646, 'Method': 'Energy', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9887683987617493, 'AUPR-IN': 0.9979832172393799, 'AUPR-OUT': 0.9267526865005493, 'FPR95TPR': 0.041531823575496674, 'Method': 'Energy', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9531997442245483, 'AUPR-IN': 0.985607922077179, 'AUPR-OUT': 0.8377007246017456, 'FPR95TPR': 0.3193095922470093, 'Method': 'Energy', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9911791086196899, 'AUPR-IN': 0.9984303712844849, 'AUPR-OUT': 0.9439053535461426, 'FPR95TPR': 0.024811219424009323, 'Method': 'Energy', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9902182817459106, 'AUPR-IN': 0.9982543587684631, 'AUPR-OUT': 0.9354563355445862, 'FPR95TPR': 0.029126213863492012, 'Method': 'Energy', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9878724813461304, 'AUPR-IN': 0.9978037476539612, 'AUPR-OUT': 0.9318572282791138, 'FPR95TPR': 0.04908306524157524, 'Method': 'MaxLogit', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9887597560882568, 'AUPR-IN': 0.9979779720306396, 'AUPR-OUT': 0.9270282983779907, 'FPR95TPR': 0.041531823575496674, 'Method': 'MaxLogit', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9532076120376587, 'AUPR-IN': 0.9855974912643433, 'AUPR-OUT': 0.8378468751907349, 'FPR95TPR': 0.31877022981643677, 'Method': 'MaxLogit', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9910975694656372, 'AUPR-IN': 0.9984127283096313, 'AUPR-OUT': 0.9437803626060486, 'FPR95TPR': 0.02535059303045273, 'Method': 'MaxLogit', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUROC': 0.9901741147041321, 'AUPR-IN': 0.9982433915138245, 'AUPR-OUT': 0.9355196356773376, 'FPR95TPR': 0.027508091181516647, 'Method': 'MaxLogit', 'Dataset': 'TinyImageNetResize'}\n",
      "label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 31.40it/s, loss=0.0605]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 91.75%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.11it/s, loss=0.0387]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 98.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 31.45it/s, loss=0.0235] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 96.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.24it/s, loss=0.0162] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 31.32it/s, loss=0.00761]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.73%\n",
      "color\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 30.92it/s, loss=0.0773]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 98.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.08it/s, loss=0.00571]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.14%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.07it/s, loss=0.0126] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 30.84it/s, loss=0.0332] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.73%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 30.97it/s, loss=0.00571]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.00%\n",
      "fruit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.10it/s, loss=0.0123]  \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.21it/s, loss=0.000241]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.11it/s, loss=0.000581]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.10it/s, loss=0.000642]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.14it/s, loss=0.000139]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 30.96it/s, loss=0.00183] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.13it/s, loss=0.00344] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.16it/s, loss=0.000106]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.15it/s, loss=0.000631]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.15it/s, loss=7.67e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n",
      "Computing principal space ...\n",
      "Computing alpha ...\n",
      "self.alpha=6.7105\n",
      "Fitting with temperature scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial T/NLL: 1.000/0.005\n",
      "Optimal temperature: 0.9203759431838989\n",
      "NLL after scaling: 0.00'\n",
      "Initial T/NLL: 1.000/0.009\n",
      "Optimal temperature: 0.8025617599487305\n",
      "NLL after scaling: 0.01'\n",
      "self.scorer_label.t=Parameter containing:\n",
      "tensor(0.9204, requires_grad=True)\n",
      "self.scorer_color.t=Parameter containing:\n",
      "tensor(0.8026, requires_grad=True)\n",
      "Fitting with temperature scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:84: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels).item()\n",
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:90: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels)\n",
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:97: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels).item()\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial T/NLL: 1.000/0.005\n",
      "Optimal temperature: 0.9203759431838989\n",
      "NLL after scaling: 0.00'\n",
      "Initial T/NLL: 1.000/0.009\n",
      "Optimal temperature: 0.8025617599487305\n",
      "NLL after scaling: 0.01'\n",
      "self.scorer_label.t=Parameter containing:\n",
      "tensor(0.9204, requires_grad=True)\n",
      "self.scorer_color.t=Parameter containing:\n",
      "tensor(0.8026, requires_grad=True)\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9999513626098633, 'AUPR-IN': 0.9999909996986389, 'AUPR-OUT': 0.999741792678833, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.999996542930603, 'AUPR-IN': 0.9999992847442627, 'AUPR-OUT': 0.9999821186065674, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9999930262565613, 'AUPR-IN': 0.9999977350234985, 'AUPR-OUT': 0.9999791383743286, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.999966561794281, 'AUPR-IN': 0.9999937415122986, 'AUPR-OUT': 0.9998236894607544, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9999889135360718, 'AUPR-IN': 0.9999979734420776, 'AUPR-OUT': 0.9999406337738037, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9998777508735657, 'AUPR-IN': 0.999977707862854, 'AUPR-OUT': 0.99933260679245, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9999929070472717, 'AUPR-IN': 0.9999986290931702, 'AUPR-OUT': 0.9999628067016602, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9999720454216003, 'AUPR-IN': 0.9999907612800598, 'AUPR-OUT': 0.9999164938926697, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9998921751976013, 'AUPR-IN': 0.999980092048645, 'AUPR-OUT': 0.99942547082901, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9999828338623047, 'AUPR-IN': 0.9999967813491821, 'AUPR-OUT': 0.9999101758003235, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9778453707695007, 'AUPR-IN': 0.9959259033203125, 'AUPR-OUT': 0.8768194913864136, 'FPR95TPR': 0.11650485545396805, 'Method': 'Entropy', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9714268445968628, 'AUPR-IN': 0.9946290850639343, 'AUPR-OUT': 0.8415791988372803, 'FPR95TPR': 0.14185544848442078, 'Method': 'Entropy', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9546464085578918, 'AUPR-IN': 0.9860272407531738, 'AUPR-OUT': 0.8209261894226074, 'FPR95TPR': 0.2918015122413635, 'Method': 'Entropy', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9832789301872253, 'AUPR-IN': 0.9969031810760498, 'AUPR-OUT': 0.9113187789916992, 'FPR95TPR': 0.077130526304245, 'Method': 'Entropy', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9696031212806702, 'AUPR-IN': 0.9942002892494202, 'AUPR-OUT': 0.8522011637687683, 'FPR95TPR': 0.18338726460933685, 'Method': 'Entropy', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9986515045166016, 'AUPR-IN': 0.9997501373291016, 'AUPR-OUT': 0.9988624453544617, 'FPR95TPR': 0.0026968715246766806, 'Method': 'LogicOOD+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9986515045166016, 'AUPR-IN': 0.9997501373291016, 'AUPR-OUT': 0.9988624453544617, 'FPR95TPR': 0.0026968715246766806, 'Method': 'LogicOOD+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.998647928237915, 'AUPR-IN': 0.9995558261871338, 'AUPR-OUT': 0.9989737868309021, 'FPR95TPR': 0.0026968715246766806, 'Method': 'LogicOOD+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9986515045166016, 'AUPR-IN': 0.9997501373291016, 'AUPR-OUT': 0.9988624453544617, 'FPR95TPR': 0.0026968715246766806, 'Method': 'LogicOOD+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9986515045166016, 'AUPR-IN': 0.9997501373291016, 'AUPR-OUT': 0.9988624453544617, 'FPR95TPR': 0.0026968715246766806, 'Method': 'LogicOOD+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.7440515756607056, 'AUPR-IN': 0.959670901298523, 'AUPR-OUT': 0.6320565938949585, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.7619015574455261, 'AUPR-IN': 0.9624972343444824, 'AUPR-OUT': 0.6392786502838135, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.7419139742851257, 'AUPR-IN': 0.9355741739273071, 'AUPR-OUT': 0.6938217878341675, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.7795015573501587, 'AUPR-IN': 0.9652796983718872, 'AUPR-OUT': 0.6472097039222717, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.7365015745162964, 'AUPR-IN': 0.9584739804267883, 'AUPR-OUT': 0.6292206048965454, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9986515641212463, 'AUPR-IN': 0.9997501373291016, 'AUPR-OUT': 0.9988624453544617, 'FPR95TPR': 0.0026968715246766806, 'Method': 'Logic+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9986515641212463, 'AUPR-IN': 0.9997501373291016, 'AUPR-OUT': 0.9988624453544617, 'FPR95TPR': 0.0026968715246766806, 'Method': 'Logic+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9985628724098206, 'AUPR-IN': 0.999535083770752, 'AUPR-OUT': 0.9987149238586426, 'FPR95TPR': 0.0026968715246766806, 'Method': 'Logic+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9986515641212463, 'AUPR-IN': 0.9997501373291016, 'AUPR-OUT': 0.9988624453544617, 'FPR95TPR': 0.0026968715246766806, 'Method': 'Logic+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9986515641212463, 'AUPR-IN': 0.9997501373291016, 'AUPR-OUT': 0.9988624453544617, 'FPR95TPR': 0.0026968715246766806, 'Method': 'Logic+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9866989254951477, 'AUPR-IN': 0.9972628951072693, 'AUPR-OUT': 0.9484810829162598, 'FPR95TPR': 0.08090614527463913, 'Method': 'LogicOOD', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9895512461662292, 'AUPR-IN': 0.9978424906730652, 'AUPR-OUT': 0.9584258794784546, 'FPR95TPR': 0.04638619348406792, 'Method': 'LogicOOD', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9738305807113647, 'AUPR-IN': 0.9914549589157104, 'AUPR-OUT': 0.9085819721221924, 'FPR95TPR': 0.14185544848442078, 'Method': 'LogicOOD', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9924416542053223, 'AUPR-IN': 0.9984177350997925, 'AUPR-OUT': 0.9701682329177856, 'FPR95TPR': 0.025889968499541283, 'Method': 'LogicOOD', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9883359670639038, 'AUPR-IN': 0.9975575804710388, 'AUPR-OUT': 0.9541176557540894, 'FPR95TPR': 0.058252427726984024, 'Method': 'LogicOOD', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9851316213607788, 'AUPR-IN': 0.996974527835846, 'AUPR-OUT': 0.9409752488136292, 'FPR95TPR': 0.08683926612138748, 'Method': 'LogicOODT', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9886003732681274, 'AUPR-IN': 0.9976674318313599, 'AUPR-OUT': 0.9528806209564209, 'FPR95TPR': 0.04746494069695473, 'Method': 'LogicOODT', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9720460772514343, 'AUPR-IN': 0.9909597635269165, 'AUPR-OUT': 0.898894190788269, 'FPR95TPR': 0.14023733139038086, 'Method': 'LogicOODT', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9919289350509644, 'AUPR-IN': 0.9983218312263489, 'AUPR-OUT': 0.9676195383071899, 'FPR95TPR': 0.02427184395492077, 'Method': 'LogicOODT', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.987690806388855, 'AUPR-IN': 0.9974300265312195, 'AUPR-OUT': 0.9513486623764038, 'FPR95TPR': 0.06040992587804794, 'Method': 'LogicOODT', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9986515641212463, 'AUPR-IN': 0.9997501373291016, 'AUPR-OUT': 0.9988624453544617, 'FPR95TPR': 0.0026968715246766806, 'Method': 'LogicOODT+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9986515641212463, 'AUPR-IN': 0.9997501373291016, 'AUPR-OUT': 0.9988624453544617, 'FPR95TPR': 0.0026968715246766806, 'Method': 'LogicOODT+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9986053705215454, 'AUPR-IN': 0.9995430111885071, 'AUPR-OUT': 0.9988219738006592, 'FPR95TPR': 0.0026968715246766806, 'Method': 'LogicOODT+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9986515641212463, 'AUPR-IN': 0.9997501373291016, 'AUPR-OUT': 0.9988624453544617, 'FPR95TPR': 0.0026968715246766806, 'Method': 'LogicOODT+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9986515641212463, 'AUPR-IN': 0.9997501373291016, 'AUPR-OUT': 0.9988624453544617, 'FPR95TPR': 0.0026968715246766806, 'Method': 'LogicOODT+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9847753047943115, 'AUPR-IN': 0.9969922304153442, 'AUPR-OUT': 0.9426536560058594, 'FPR95TPR': 0.09061488509178162, 'Method': 'Ensemble', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9856460690498352, 'AUPR-IN': 0.9970299005508423, 'AUPR-OUT': 0.9485559463500977, 'FPR95TPR': 0.07605177909135818, 'Method': 'Ensemble', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9718309640884399, 'AUPR-IN': 0.9908424615859985, 'AUPR-OUT': 0.9044013023376465, 'FPR95TPR': 0.14617043733596802, 'Method': 'Ensemble', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9910727143287659, 'AUPR-IN': 0.9982317686080933, 'AUPR-OUT': 0.9645732641220093, 'FPR95TPR': 0.04692556709051132, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9863787889480591, 'AUPR-IN': 0.997208833694458, 'AUPR-OUT': 0.9491628408432007, 'FPR95TPR': 0.07443365454673767, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9755188822746277, 'AUPR-IN': 0.9953356385231018, 'AUPR-OUT': 0.8729940056800842, 'FPR95TPR': 0.1229773461818695, 'Method': 'MSP', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9692584872245789, 'AUPR-IN': 0.9938390254974365, 'AUPR-OUT': 0.8413063287734985, 'FPR95TPR': 0.1456310749053955, 'Method': 'MSP', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9531130790710449, 'AUPR-IN': 0.9849447011947632, 'AUPR-OUT': 0.8218616843223572, 'FPR95TPR': 0.2880258858203888, 'Method': 'MSP', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.981147050857544, 'AUPR-IN': 0.9963141083717346, 'AUPR-OUT': 0.9086760878562927, 'FPR95TPR': 0.08522114157676697, 'Method': 'MSP', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9678753614425659, 'AUPR-IN': 0.9935077428817749, 'AUPR-OUT': 0.8525871634483337, 'FPR95TPR': 0.17259977757930756, 'Method': 'MSP', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.8974465131759644, 'AUPR-IN': 0.9746302962303162, 'AUPR-OUT': 0.6119784712791443, 'FPR95TPR': 0.5167205929756165, 'Method': 'ReAct', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.856174647808075, 'AUPR-IN': 0.9559459090232849, 'AUPR-OUT': 0.6162923574447632, 'FPR95TPR': 0.454692542552948, 'Method': 'ReAct', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.8560119867324829, 'AUPR-IN': 0.936556339263916, 'AUPR-OUT': 0.6998197436332703, 'FPR95TPR': 0.5674217939376831, 'Method': 'ReAct', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.8996467590332031, 'AUPR-IN': 0.9744927287101746, 'AUPR-OUT': 0.6635739207267761, 'FPR95TPR': 0.3662351667881012, 'Method': 'ReAct', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.8693186044692993, 'AUPR-IN': 0.9632587432861328, 'AUPR-OUT': 0.5984967350959778, 'FPR95TPR': 0.4687162935733795, 'Method': 'ReAct', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.985654890537262, 'AUPR-IN': 0.9974412322044373, 'AUPR-OUT': 0.9084774255752563, 'FPR95TPR': 0.0663430392742157, 'Method': 'Energy', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.973272979259491, 'AUPR-IN': 0.9952588081359863, 'AUPR-OUT': 0.8294646739959717, 'FPR95TPR': 0.15318231284618378, 'Method': 'Energy', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9540053606033325, 'AUPR-IN': 0.986056923866272, 'AUPR-OUT': 0.8236470818519592, 'FPR95TPR': 0.33764833211898804, 'Method': 'Energy', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9867628812789917, 'AUPR-IN': 0.997631847858429, 'AUPR-OUT': 0.9210535287857056, 'FPR95TPR': 0.06256742030382156, 'Method': 'Energy', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.969414234161377, 'AUPR-IN': 0.9944450855255127, 'AUPR-OUT': 0.8357722759246826, 'FPR95TPR': 0.20981661975383759, 'Method': 'Energy', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9855086803436279, 'AUPR-IN': 0.997408926486969, 'AUPR-OUT': 0.9084756374359131, 'FPR95TPR': 0.0674217939376831, 'Method': 'MaxLogit', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9730265736579895, 'AUPR-IN': 0.9952008128166199, 'AUPR-OUT': 0.8292715549468994, 'FPR95TPR': 0.15210355818271637, 'Method': 'MaxLogit', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.953906774520874, 'AUPR-IN': 0.9860020875930786, 'AUPR-OUT': 0.8237130641937256, 'FPR95TPR': 0.33764833211898804, 'Method': 'MaxLogit', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.986603856086731, 'AUPR-IN': 0.9975977540016174, 'AUPR-OUT': 0.9208060503005981, 'FPR95TPR': 0.06364616751670837, 'Method': 'MaxLogit', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUROC': 0.969254732131958, 'AUPR-IN': 0.9944020509719849, 'AUPR-OUT': 0.835799515247345, 'FPR95TPR': 0.20819848775863647, 'Method': 'MaxLogit', 'Dataset': 'TinyImageNetResize'}\n",
      "label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.16it/s, loss=0.0492]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 96.17%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 30.89it/s, loss=0.0181]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 31.32it/s, loss=0.0142] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 31.33it/s, loss=0.00502]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.02it/s, loss=0.00876]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 97.63%\n",
      "color\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 30.93it/s, loss=0.0159]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 96.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.26it/s, loss=0.016]  \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.28it/s, loss=0.00596]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 30.93it/s, loss=0.027]   \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 30.97it/s, loss=0.0027]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.84%\n",
      "fruit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.15it/s, loss=0.0121]  \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.85%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.11it/s, loss=0.00143] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.07it/s, loss=0.0014]  \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.07it/s, loss=0.000217]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.05it/s, loss=0.00193] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.12it/s, loss=0.00158] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.05it/s, loss=0.00317] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.13it/s, loss=0.00243] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.10it/s, loss=0.000112]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.10it/s, loss=2.83e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n",
      "Computing principal space ...\n",
      "Computing alpha ...\n",
      "self.alpha=6.3791\n",
      "Fitting with temperature scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial T/NLL: 1.000/0.047\n",
      "Optimal temperature: 1.0090967416763306\n",
      "NLL after scaling: 0.05'\n",
      "Initial T/NLL: 1.000/0.004\n",
      "Optimal temperature: 0.9979324340820312\n",
      "NLL after scaling: 0.00'\n",
      "self.scorer_label.t=Parameter containing:\n",
      "tensor(1.0091, requires_grad=True)\n",
      "self.scorer_color.t=Parameter containing:\n",
      "tensor(0.9979, requires_grad=True)\n",
      "Fitting with temperature scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:84: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels).item()\n",
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:90: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels)\n",
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:97: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels).item()\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial T/NLL: 1.000/0.047\n",
      "Optimal temperature: 1.0090967416763306\n",
      "NLL after scaling: 0.05'\n",
      "Initial T/NLL: 1.000/0.004\n",
      "Optimal temperature: 0.9979324340820312\n",
      "NLL after scaling: 0.00'\n",
      "self.scorer_label.t=Parameter containing:\n",
      "tensor(1.0091, requires_grad=True)\n",
      "self.scorer_color.t=Parameter containing:\n",
      "tensor(0.9979, requires_grad=True)\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9998435974121094, 'AUPR-IN': 0.9999709129333496, 'AUPR-OUT': 0.9992103576660156, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0000001192092896, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9999663829803467, 'AUPR-IN': 0.9999891519546509, 'AUPR-OUT': 0.9998971223831177, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9999666213989258, 'AUPR-IN': 0.9999938011169434, 'AUPR-OUT': 0.9998228549957275, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9999988675117493, 'AUPR-IN': 0.9999998211860657, 'AUPR-OUT': 0.9999935626983643, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.99959796667099, 'AUPR-IN': 0.9999262690544128, 'AUPR-OUT': 0.9978267550468445, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9999988079071045, 'AUPR-IN': 0.9999997019767761, 'AUPR-OUT': 0.9999935626983643, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9995664954185486, 'AUPR-IN': 0.9998606443405151, 'AUPR-OUT': 0.9986488819122314, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9996964335441589, 'AUPR-IN': 0.9999448657035828, 'AUPR-OUT': 0.9982707500457764, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9999881982803345, 'AUPR-IN': 0.9999979138374329, 'AUPR-OUT': 0.9999361634254456, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9588544368743896, 'AUPR-IN': 0.9925827383995056, 'AUPR-OUT': 0.7217026948928833, 'FPR95TPR': 0.22923408448696136, 'Method': 'Entropy', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9791938066482544, 'AUPR-IN': 0.9957340359687805, 'AUPR-OUT': 0.9320223331451416, 'FPR95TPR': 0.09816613048315048, 'Method': 'Entropy', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9222139716148376, 'AUPR-IN': 0.9748019576072693, 'AUPR-OUT': 0.7351363897323608, 'FPR95TPR': 0.43959006667137146, 'Method': 'Entropy', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9750893712043762, 'AUPR-IN': 0.9953532218933105, 'AUPR-OUT': 0.8602714538574219, 'FPR95TPR': 0.11380798369646072, 'Method': 'Entropy', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9801222085952759, 'AUPR-IN': 0.9960992336273193, 'AUPR-OUT': 0.9121999144554138, 'FPR95TPR': 0.09439050406217575, 'Method': 'Entropy', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9991909861564636, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.9993174076080322, 'FPR95TPR': 0.0016181230312213302, 'Method': 'LogicOOD+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9991909861564636, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.9993174076080322, 'FPR95TPR': 0.0016181230312213302, 'Method': 'LogicOOD+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9990540742874146, 'AUPR-IN': 0.9996888637542725, 'AUPR-OUT': 0.9989593625068665, 'FPR95TPR': 0.0016181230312213302, 'Method': 'LogicOOD+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9991909861564636, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.9993174076080322, 'FPR95TPR': 0.0016181230312213302, 'Method': 'LogicOOD+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9991909861564636, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.9993174076080322, 'FPR95TPR': 0.0016181230312213302, 'Method': 'LogicOOD+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.6806909441947937, 'AUPR-IN': 0.9497727751731873, 'AUPR-OUT': 0.6118950247764587, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.7356409430503845, 'AUPR-IN': 0.9584630131721497, 'AUPR-OUT': 0.6292669177055359, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.6806625723838806, 'AUPR-IN': 0.9204649925231934, 'AUPR-OUT': 0.6693947315216064, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.7161909341812134, 'AUPR-IN': 0.955392599105835, 'AUPR-OUT': 0.6225367188453674, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.7138409614562988, 'AUPR-IN': 0.9550212621688843, 'AUPR-OUT': 0.6217705011367798, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9991909265518188, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.999317467212677, 'FPR95TPR': 0.0016181230312213302, 'Method': 'Logic+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9991909265518188, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.999317467212677, 'FPR95TPR': 0.0016181230312213302, 'Method': 'Logic+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9967085719108582, 'AUPR-IN': 0.9991187453269958, 'AUPR-OUT': 0.9919403195381165, 'FPR95TPR': 0.0016181230312213302, 'Method': 'Logic+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9991909265518188, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.999317467212677, 'FPR95TPR': 0.0016181230312213302, 'Method': 'Logic+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9991909265518188, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.999317467212677, 'FPR95TPR': 0.0016181230312213302, 'Method': 'Logic+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9793968796730042, 'AUPR-IN': 0.995851457118988, 'AUPR-OUT': 0.9128390550613403, 'FPR95TPR': 0.10463862121105194, 'Method': 'LogicOOD', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9869563579559326, 'AUPR-IN': 0.9972778558731079, 'AUPR-OUT': 0.9560524225234985, 'FPR95TPR': 0.06256742030382156, 'Method': 'LogicOOD', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9540668725967407, 'AUPR-IN': 0.984589695930481, 'AUPR-OUT': 0.8613215088844299, 'FPR95TPR': 0.2680690288543701, 'Method': 'LogicOOD', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.986454963684082, 'AUPR-IN': 0.9972643256187439, 'AUPR-OUT': 0.9401871562004089, 'FPR95TPR': 0.052858684211969376, 'Method': 'LogicOOD', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9858599901199341, 'AUPR-IN': 0.9970724582672119, 'AUPR-OUT': 0.9460269212722778, 'FPR95TPR': 0.06364616751670837, 'Method': 'LogicOOD', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9791302680969238, 'AUPR-IN': 0.9958081841468811, 'AUPR-OUT': 0.9107291102409363, 'FPR95TPR': 0.10409924387931824, 'Method': 'LogicOODT', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9869751930236816, 'AUPR-IN': 0.9972831010818481, 'AUPR-OUT': 0.9560235142707825, 'FPR95TPR': 0.06256742030382156, 'Method': 'LogicOODT', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9538232684135437, 'AUPR-IN': 0.9845301508903503, 'AUPR-OUT': 0.860088050365448, 'FPR95TPR': 0.268608421087265, 'Method': 'LogicOODT', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.986414909362793, 'AUPR-IN': 0.9972593784332275, 'AUPR-OUT': 0.9396523237228394, 'FPR95TPR': 0.05231931060552597, 'Method': 'LogicOODT', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9859002828598022, 'AUPR-IN': 0.9970822930335999, 'AUPR-OUT': 0.946121871471405, 'FPR95TPR': 0.06310679763555527, 'Method': 'LogicOODT', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9991909265518188, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.9993175268173218, 'FPR95TPR': 0.0016181230312213302, 'Method': 'LogicOODT+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9991909265518188, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.9993175268173218, 'FPR95TPR': 0.0016181230312213302, 'Method': 'LogicOODT+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9988903403282166, 'AUPR-IN': 0.999638020992279, 'AUPR-OUT': 0.9984108209609985, 'FPR95TPR': 0.0016181230312213302, 'Method': 'LogicOODT+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9991909265518188, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.9993175268173218, 'FPR95TPR': 0.0016181230312213302, 'Method': 'LogicOODT+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9991909265518188, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.9993175268173218, 'FPR95TPR': 0.0016181230312213302, 'Method': 'LogicOODT+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9794629216194153, 'AUPR-IN': 0.9959987998008728, 'AUPR-OUT': 0.9121279120445251, 'FPR95TPR': 0.10463862121105194, 'Method': 'Ensemble', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9827560186386108, 'AUPR-IN': 0.9961309432983398, 'AUPR-OUT': 0.9510313868522644, 'FPR95TPR': 0.06796116381883621, 'Method': 'Ensemble', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9498245716094971, 'AUPR-IN': 0.9826251268386841, 'AUPR-OUT': 0.8536995649337769, 'FPR95TPR': 0.27130529284477234, 'Method': 'Ensemble', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9850150346755981, 'AUPR-IN': 0.9970452785491943, 'AUPR-OUT': 0.9312665462493896, 'FPR95TPR': 0.06094929948449135, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9830130338668823, 'AUPR-IN': 0.9964132308959961, 'AUPR-OUT': 0.9376121759414673, 'FPR95TPR': 0.06850054115056992, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9548026919364929, 'AUPR-IN': 0.9914993047714233, 'AUPR-OUT': 0.7172278165817261, 'FPR95TPR': 0.23732469975948334, 'Method': 'MSP', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9730159640312195, 'AUPR-IN': 0.993626058101654, 'AUPR-OUT': 0.9268369078636169, 'FPR95TPR': 0.10194174945354462, 'Method': 'MSP', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9178569912910461, 'AUPR-IN': 0.9718058109283447, 'AUPR-OUT': 0.7338761687278748, 'FPR95TPR': 0.4358144700527191, 'Method': 'MSP', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.970272421836853, 'AUPR-IN': 0.993986189365387, 'AUPR-OUT': 0.8556257486343384, 'FPR95TPR': 0.11542610824108124, 'Method': 'MSP', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9745336771011353, 'AUPR-IN': 0.9943408370018005, 'AUPR-OUT': 0.907655656337738, 'FPR95TPR': 0.09870550036430359, 'Method': 'MSP', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9360640048980713, 'AUPR-IN': 0.9878775477409363, 'AUPR-OUT': 0.715660572052002, 'FPR95TPR': 0.3975188732147217, 'Method': 'ReAct', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.8534955978393555, 'AUPR-IN': 0.9658315777778625, 'AUPR-OUT': 0.6535642147064209, 'FPR95TPR': 0.4913700222969055, 'Method': 'ReAct', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.8101364374160767, 'AUPR-IN': 0.9278540015220642, 'AUPR-OUT': 0.599452018737793, 'FPR95TPR': 0.6720604300498962, 'Method': 'ReAct', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9294971823692322, 'AUPR-IN': 0.9855029582977295, 'AUPR-OUT': 0.7523635625839233, 'FPR95TPR': 0.353829562664032, 'Method': 'ReAct', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.8179619312286377, 'AUPR-IN': 0.9555235505104065, 'AUPR-OUT': 0.591507613658905, 'FPR95TPR': 0.5593311786651611, 'Method': 'ReAct', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9699450135231018, 'AUPR-IN': 0.9947519898414612, 'AUPR-OUT': 0.7818955183029175, 'FPR95TPR': 0.17421790957450867, 'Method': 'Energy', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9818135499954224, 'AUPR-IN': 0.9965207576751709, 'AUPR-OUT': 0.920557975769043, 'FPR95TPR': 0.10355987399816513, 'Method': 'Energy', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9192042350769043, 'AUPR-IN': 0.9749577641487122, 'AUPR-OUT': 0.7163953185081482, 'FPR95TPR': 0.5264293551445007, 'Method': 'Energy', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9804810285568237, 'AUPR-IN': 0.9964991211891174, 'AUPR-OUT': 0.8804848194122314, 'FPR95TPR': 0.09708737581968307, 'Method': 'Energy', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9788737893104553, 'AUPR-IN': 0.9960589408874512, 'AUPR-OUT': 0.8894477486610413, 'FPR95TPR': 0.11866234987974167, 'Method': 'Energy', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9695069193840027, 'AUPR-IN': 0.9946634769439697, 'AUPR-OUT': 0.780847430229187, 'FPR95TPR': 0.17475728690624237, 'Method': 'MaxLogit', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9813543558120728, 'AUPR-IN': 0.9964384436607361, 'AUPR-OUT': 0.9194459319114685, 'FPR95TPR': 0.10571736842393875, 'Method': 'MaxLogit', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9188594222068787, 'AUPR-IN': 0.9748191833496094, 'AUPR-OUT': 0.7161751985549927, 'FPR95TPR': 0.5269687175750732, 'Method': 'MaxLogit', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9801678657531738, 'AUPR-IN': 0.9964375495910645, 'AUPR-OUT': 0.8796717524528503, 'FPR95TPR': 0.0997842475771904, 'Method': 'MaxLogit', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUROC': 0.9785869121551514, 'AUPR-IN': 0.9960114359855652, 'AUPR-OUT': 0.8887437582015991, 'FPR95TPR': 0.11704422533512115, 'Method': 'MaxLogit', 'Dataset': 'TinyImageNetResize'}\n",
      "label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 31.47it/s, loss=0.081] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 92.18%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.25it/s, loss=0.0294]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 98.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.18it/s, loss=0.014]  \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 98.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.20it/s, loss=0.0116] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 98.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.14it/s, loss=0.00924]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.73%\n",
      "color\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 30.96it/s, loss=0.0301] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 98.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.07it/s, loss=0.00404]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.23it/s, loss=0.005]  \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.14it/s, loss=0.00557]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.07it/s, loss=0.00622]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.00%\n",
      "fruit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.17it/s, loss=0.00466] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.01it/s, loss=0.00165] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.12it/s, loss=0.00151] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.13it/s, loss=8.18e-5] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.14it/s, loss=0.000499]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.05it/s, loss=0.000146]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.07it/s, loss=0.000418]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.02it/s, loss=4.18e-5] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.09it/s, loss=0.000443]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.10it/s, loss=5.98e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n",
      "Computing principal space ...\n",
      "Computing alpha ...\n",
      "self.alpha=6.4571\n",
      "Fitting with temperature scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial T/NLL: 1.000/0.012\n",
      "Optimal temperature: 0.906321108341217\n",
      "NLL after scaling: 0.01'\n",
      "Initial T/NLL: 1.000/0.002\n",
      "Optimal temperature: 0.8881919980049133\n",
      "NLL after scaling: 0.00'\n",
      "self.scorer_label.t=Parameter containing:\n",
      "tensor(0.9063, requires_grad=True)\n",
      "self.scorer_color.t=Parameter containing:\n",
      "tensor(0.8882, requires_grad=True)\n",
      "Fitting with temperature scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:84: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels).item()\n",
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:90: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels)\n",
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:97: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels).item()\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial T/NLL: 1.000/0.012\n",
      "Optimal temperature: 0.906321108341217\n",
      "NLL after scaling: 0.01'\n",
      "Initial T/NLL: 1.000/0.002\n",
      "Optimal temperature: 0.8881919980049133\n",
      "NLL after scaling: 0.00'\n",
      "self.scorer_label.t=Parameter containing:\n",
      "tensor(0.9063, requires_grad=True)\n",
      "self.scorer_color.t=Parameter containing:\n",
      "tensor(0.8882, requires_grad=True)\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9963555932044983, 'AUPR-IN': 0.9993355870246887, 'AUPR-OUT': 0.9805370569229126, 'FPR95TPR': 0.008629988878965378, 'Method': 'ViM', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9998252391815186, 'AUPR-IN': 0.9999682307243347, 'AUPR-OUT': 0.9989974498748779, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9996610283851624, 'AUPR-IN': 0.9998939037322998, 'AUPR-OUT': 0.998875617980957, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9974033832550049, 'AUPR-IN': 0.9995311498641968, 'AUPR-OUT': 0.9853241443634033, 'FPR95TPR': 0.0021574972197413445, 'Method': 'ViM', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9994121789932251, 'AUPR-IN': 0.9998941421508789, 'AUPR-OUT': 0.9965444803237915, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.990707516670227, 'AUPR-IN': 0.9983184933662415, 'AUPR-OUT': 0.9487606883049011, 'FPR95TPR': 0.04099244996905327, 'Method': 'Mahalanobis', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9991918802261353, 'AUPR-IN': 0.9998525977134705, 'AUPR-OUT': 0.9955651760101318, 'FPR95TPR': 0.0010787486098706722, 'Method': 'Mahalanobis', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9988948702812195, 'AUPR-IN': 0.9996485710144043, 'AUPR-OUT': 0.9965002536773682, 'FPR95TPR': 0.0010787486098706722, 'Method': 'Mahalanobis', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9915852546691895, 'AUPR-IN': 0.9984710216522217, 'AUPR-OUT': 0.9540560841560364, 'FPR95TPR': 0.028047464787960052, 'Method': 'Mahalanobis', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9978674650192261, 'AUPR-IN': 0.9996165037155151, 'AUPR-OUT': 0.9875996112823486, 'FPR95TPR': 0.0016181230312213302, 'Method': 'Mahalanobis', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9326968193054199, 'AUPR-IN': 0.9867439866065979, 'AUPR-OUT': 0.7523201107978821, 'FPR95TPR': 0.40668824315071106, 'Method': 'Entropy', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.96958327293396, 'AUPR-IN': 0.9935789108276367, 'AUPR-OUT': 0.8918325304985046, 'FPR95TPR': 0.1321467161178589, 'Method': 'Entropy', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.931670606136322, 'AUPR-IN': 0.9769437909126282, 'AUPR-OUT': 0.8051748275756836, 'FPR95TPR': 0.4088457524776459, 'Method': 'Entropy', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9613370299339294, 'AUPR-IN': 0.9924628138542175, 'AUPR-OUT': 0.8407462239265442, 'FPR95TPR': 0.2211434692144394, 'Method': 'Entropy', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9651390314102173, 'AUPR-IN': 0.9928985238075256, 'AUPR-OUT': 0.867344856262207, 'FPR95TPR': 0.17044228315353394, 'Method': 'Entropy', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9986515045166016, 'AUPR-IN': 0.9997501373291016, 'AUPR-OUT': 0.9988624453544617, 'FPR95TPR': 0.0026968715246766806, 'Method': 'LogicOOD+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9986515045166016, 'AUPR-IN': 0.9997501373291016, 'AUPR-OUT': 0.9988624453544617, 'FPR95TPR': 0.0026968715246766806, 'Method': 'LogicOOD+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9983820915222168, 'AUPR-IN': 0.9994686841964722, 'AUPR-OUT': 0.9981028437614441, 'FPR95TPR': 0.0026968715246766806, 'Method': 'LogicOOD+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9986515045166016, 'AUPR-IN': 0.9997501373291016, 'AUPR-OUT': 0.9988624453544617, 'FPR95TPR': 0.0026968715246766806, 'Method': 'LogicOOD+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9986515045166016, 'AUPR-IN': 0.9997501373291016, 'AUPR-OUT': 0.9988624453544617, 'FPR95TPR': 0.0026968715246766806, 'Method': 'LogicOOD+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.835001528263092, 'AUPR-IN': 0.9740333557128906, 'AUPR-OUT': 0.6793583631515503, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.7089515328407288, 'AUPR-IN': 0.9540964365005493, 'AUPR-OUT': 0.6198228001594543, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.7397863268852234, 'AUPR-IN': 0.9350398182868958, 'AUPR-OUT': 0.6928412914276123, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.7715016007423401, 'AUPR-IN': 0.9640153646469116, 'AUPR-OUT': 0.6434963345527649, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.7256515622138977, 'AUPR-IN': 0.9567519426345825, 'AUPR-OUT': 0.6253503561019897, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9986515641212463, 'AUPR-IN': 0.9997501373291016, 'AUPR-OUT': 0.9988624453544617, 'FPR95TPR': 0.0026968715246766806, 'Method': 'Logic+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9986515641212463, 'AUPR-IN': 0.9997501373291016, 'AUPR-OUT': 0.9988624453544617, 'FPR95TPR': 0.0026968715246766806, 'Method': 'Logic+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9945735335350037, 'AUPR-IN': 0.9985446333885193, 'AUPR-OUT': 0.986847996711731, 'FPR95TPR': 0.0026968715246766806, 'Method': 'Logic+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9986515641212463, 'AUPR-IN': 0.9997501373291016, 'AUPR-OUT': 0.9988624453544617, 'FPR95TPR': 0.0026968715246766806, 'Method': 'Logic+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9986515641212463, 'AUPR-IN': 0.9997501373291016, 'AUPR-OUT': 0.9988624453544617, 'FPR95TPR': 0.0026968715246766806, 'Method': 'Logic+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9925327897071838, 'AUPR-IN': 0.9984985589981079, 'AUPR-OUT': 0.97059166431427, 'FPR95TPR': 0.03182308375835419, 'Method': 'LogicOOD', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9903548955917358, 'AUPR-IN': 0.9978750944137573, 'AUPR-OUT': 0.9661665558815002, 'FPR95TPR': 0.03883495181798935, 'Method': 'LogicOOD', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9776393175125122, 'AUPR-IN': 0.9923734664916992, 'AUPR-OUT': 0.9379858374595642, 'FPR95TPR': 0.1326860785484314, 'Method': 'LogicOOD', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9915933609008789, 'AUPR-IN': 0.9982500076293945, 'AUPR-OUT': 0.9668334722518921, 'FPR95TPR': 0.029665587469935417, 'Method': 'LogicOOD', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9904177188873291, 'AUPR-IN': 0.9979308843612671, 'AUPR-OUT': 0.9654699563980103, 'FPR95TPR': 0.038295578211545944, 'Method': 'LogicOOD', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9922272562980652, 'AUPR-IN': 0.9984406232833862, 'AUPR-OUT': 0.9693796038627625, 'FPR95TPR': 0.03451995551586151, 'Method': 'LogicOODT', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9898791909217834, 'AUPR-IN': 0.9977788925170898, 'AUPR-OUT': 0.9646232724189758, 'FPR95TPR': 0.03991369903087616, 'Method': 'LogicOODT', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9769623875617981, 'AUPR-IN': 0.9921424388885498, 'AUPR-OUT': 0.9363402128219604, 'FPR95TPR': 0.13538295030593872, 'Method': 'LogicOODT', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9912474155426025, 'AUPR-IN': 0.9981836676597595, 'AUPR-OUT': 0.9653955101966858, 'FPR95TPR': 0.03182308375835419, 'Method': 'LogicOODT', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.989979088306427, 'AUPR-IN': 0.9978439807891846, 'AUPR-OUT': 0.9639298319816589, 'FPR95TPR': 0.03991369903087616, 'Method': 'LogicOODT', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9986516237258911, 'AUPR-IN': 0.9997501373291016, 'AUPR-OUT': 0.9988625049591064, 'FPR95TPR': 0.0026968715246766806, 'Method': 'LogicOODT+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9986516237258911, 'AUPR-IN': 0.9997501373291016, 'AUPR-OUT': 0.9988625049591064, 'FPR95TPR': 0.0026968715246766806, 'Method': 'LogicOODT+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9978356957435608, 'AUPR-IN': 0.9993088841438293, 'AUPR-OUT': 0.9959157109260559, 'FPR95TPR': 0.0026968715246766806, 'Method': 'LogicOODT+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9986516237258911, 'AUPR-IN': 0.9997501373291016, 'AUPR-OUT': 0.9988625049591064, 'FPR95TPR': 0.0026968715246766806, 'Method': 'LogicOODT+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9986516237258911, 'AUPR-IN': 0.9997501373291016, 'AUPR-OUT': 0.9988625049591064, 'FPR95TPR': 0.0026968715246766806, 'Method': 'LogicOODT+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9890086054801941, 'AUPR-IN': 0.9978247284889221, 'AUPR-OUT': 0.9588993191719055, 'FPR95TPR': 0.057173676788806915, 'Method': 'Ensemble', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.990246057510376, 'AUPR-IN': 0.9980285167694092, 'AUPR-OUT': 0.9642308950424194, 'FPR95TPR': 0.04099244996905327, 'Method': 'Ensemble', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9751179218292236, 'AUPR-IN': 0.9915813207626343, 'AUPR-OUT': 0.9329800605773926, 'FPR95TPR': 0.1445523202419281, 'Method': 'Ensemble', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9884653687477112, 'AUPR-IN': 0.9977350831031799, 'AUPR-OUT': 0.9547727704048157, 'FPR95TPR': 0.057173676788806915, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9893338084220886, 'AUPR-IN': 0.9978671073913574, 'AUPR-OUT': 0.9604095220565796, 'FPR95TPR': 0.049622438848018646, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9311255812644958, 'AUPR-IN': 0.9863467812538147, 'AUPR-OUT': 0.7485492825508118, 'FPR95TPR': 0.4099244773387909, 'Method': 'MSP', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9683001041412354, 'AUPR-IN': 0.9932719469070435, 'AUPR-OUT': 0.8892470002174377, 'FPR95TPR': 0.13646170496940613, 'Method': 'MSP', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9305486083030701, 'AUPR-IN': 0.976377010345459, 'AUPR-OUT': 0.8036785125732422, 'FPR95TPR': 0.4023732542991638, 'Method': 'MSP', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9597244262695312, 'AUPR-IN': 0.9920530319213867, 'AUPR-OUT': 0.8371852040290833, 'FPR95TPR': 0.22707659006118774, 'Method': 'MSP', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9637455940246582, 'AUPR-IN': 0.9925521612167358, 'AUPR-OUT': 0.8644099831581116, 'FPR95TPR': 0.17367853224277496, 'Method': 'MSP', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.8898460865020752, 'AUPR-IN': 0.978132963180542, 'AUPR-OUT': 0.5604807138442993, 'FPR95TPR': 0.6677454113960266, 'Method': 'ReAct', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.954704761505127, 'AUPR-IN': 0.9894412159919739, 'AUPR-OUT': 0.8723040819168091, 'FPR95TPR': 0.18284790217876434, 'Method': 'ReAct', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.8975688815116882, 'AUPR-IN': 0.9650560617446899, 'AUPR-OUT': 0.6843029260635376, 'FPR95TPR': 0.6116504669189453, 'Method': 'ReAct', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9454526305198669, 'AUPR-IN': 0.9887939691543579, 'AUPR-OUT': 0.7914109826087952, 'FPR95TPR': 0.2756202816963196, 'Method': 'ReAct', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9545468091964722, 'AUPR-IN': 0.9898557066917419, 'AUPR-OUT': 0.8600921630859375, 'FPR95TPR': 0.19633226096630096, 'Method': 'ReAct', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9375444650650024, 'AUPR-IN': 0.9878610968589783, 'AUPR-OUT': 0.7682900428771973, 'FPR95TPR': 0.3861920237541199, 'Method': 'Energy', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9706631302833557, 'AUPR-IN': 0.9938811659812927, 'AUPR-OUT': 0.8882958889007568, 'FPR95TPR': 0.12998920679092407, 'Method': 'Energy', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9287856817245483, 'AUPR-IN': 0.9763979911804199, 'AUPR-OUT': 0.7945363521575928, 'FPR95TPR': 0.4358144700527191, 'Method': 'Energy', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9648635983467102, 'AUPR-IN': 0.9933006167411804, 'AUPR-OUT': 0.8464282751083374, 'FPR95TPR': 0.21682848036289215, 'Method': 'Energy', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9662636518478394, 'AUPR-IN': 0.99322509765625, 'AUPR-OUT': 0.8632816672325134, 'FPR95TPR': 0.17421790957450867, 'Method': 'Energy', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9375412464141846, 'AUPR-IN': 0.987849771976471, 'AUPR-OUT': 0.7683955430984497, 'FPR95TPR': 0.3861920237541199, 'Method': 'MaxLogit', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9707788228988647, 'AUPR-IN': 0.9939638376235962, 'AUPR-OUT': 0.8885172605514526, 'FPR95TPR': 0.12891046702861786, 'Method': 'MaxLogit', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9290460348129272, 'AUPR-IN': 0.976550281047821, 'AUPR-OUT': 0.7949461936950684, 'FPR95TPR': 0.4358144700527191, 'Method': 'MaxLogit', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9648485779762268, 'AUPR-IN': 0.9932934045791626, 'AUPR-OUT': 0.8465662002563477, 'FPR95TPR': 0.21521036326885223, 'Method': 'MaxLogit', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUROC': 0.966401219367981, 'AUPR-IN': 0.9932879209518433, 'AUPR-OUT': 0.863513171672821, 'FPR95TPR': 0.17421790957450867, 'Method': 'MaxLogit', 'Dataset': 'TinyImageNetResize'}\n",
      "label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 31.38it/s, loss=0.103] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 97.57%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 31.53it/s, loss=0.0414]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 96.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.27it/s, loss=0.044]  \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.19%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.29it/s, loss=0.00835]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.20it/s, loss=0.00917]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.00%\n",
      "color\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.13it/s, loss=0.0156] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.24it/s, loss=0.0124] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 98.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 30.72it/s, loss=0.0286] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 30.85it/s, loss=0.00309]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.02it/s, loss=0.00116] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.00%\n",
      "fruit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.04it/s, loss=0.00196] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.07it/s, loss=0.000294]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.12it/s, loss=0.00241] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.16it/s, loss=8.12e-5] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.08it/s, loss=0.000159]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.22it/s, loss=7.99e-5] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.09it/s, loss=0.000129]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.14it/s, loss=0.000109]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.12it/s, loss=0.000394]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.11it/s, loss=5.51e-5] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n",
      "Computing principal space ...\n",
      "Computing alpha ...\n",
      "self.alpha=6.5053\n",
      "Fitting with temperature scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial T/NLL: 1.000/0.003\n",
      "Optimal temperature: 0.9173693656921387\n",
      "NLL after scaling: 0.00'\n",
      "Initial T/NLL: 1.000/0.002\n",
      "Optimal temperature: 0.8749767541885376\n",
      "NLL after scaling: 0.00'\n",
      "self.scorer_label.t=Parameter containing:\n",
      "tensor(0.9174, requires_grad=True)\n",
      "self.scorer_color.t=Parameter containing:\n",
      "tensor(0.8750, requires_grad=True)\n",
      "Fitting with temperature scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:84: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels).item()\n",
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:90: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels)\n",
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:97: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels).item()\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial T/NLL: 1.000/0.003\n",
      "Optimal temperature: 0.9173693656921387\n",
      "NLL after scaling: 0.00'\n",
      "Initial T/NLL: 1.000/0.002\n",
      "Optimal temperature: 0.8749767541885376\n",
      "NLL after scaling: 0.00'\n",
      "self.scorer_label.t=Parameter containing:\n",
      "tensor(0.9174, requires_grad=True)\n",
      "self.scorer_color.t=Parameter containing:\n",
      "tensor(0.8750, requires_grad=True)\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9996520280838013, 'AUPR-IN': 0.9999321699142456, 'AUPR-OUT': 0.9995356798171997, 'FPR95TPR': 0.0005393743049353361, 'Method': 'ViM', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.999730110168457, 'AUPR-IN': 0.9999499320983887, 'AUPR-OUT': 0.9999982714653015, 'FPR95TPR': 0.0005393743049353361, 'Method': 'ViM', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9997249841690063, 'AUPR-IN': 0.9999090433120728, 'AUPR-OUT': 0.9999778270721436, 'FPR95TPR': 0.0005393743049353361, 'Method': 'ViM', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9996945858001709, 'AUPR-IN': 0.9999422430992126, 'AUPR-OUT': 0.9997771382331848, 'FPR95TPR': 0.0005393743049353361, 'Method': 'ViM', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9997285604476929, 'AUPR-IN': 0.99994957447052, 'AUPR-OUT': 0.9999864101409912, 'FPR95TPR': 0.0005393743049353361, 'Method': 'ViM', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9984131455421448, 'AUPR-IN': 0.9996986985206604, 'AUPR-OUT': 0.9930906295776367, 'FPR95TPR': 0.008629988878965378, 'Method': 'Mahalanobis', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9999164938926697, 'AUPR-IN': 0.9999845027923584, 'AUPR-OUT': 0.999575138092041, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9996360540390015, 'AUPR-IN': 0.9998807311058044, 'AUPR-OUT': 0.9989211559295654, 'FPR95TPR': 0.0005393743049353361, 'Method': 'Mahalanobis', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9992554783821106, 'AUPR-IN': 0.9998603463172913, 'AUPR-OUT': 0.9964843988418579, 'FPR95TPR': 0.0016181230312213302, 'Method': 'Mahalanobis', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9998864531517029, 'AUPR-IN': 0.9999788999557495, 'AUPR-OUT': 0.9994145631790161, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9894329309463501, 'AUPR-IN': 0.9979994297027588, 'AUPR-OUT': 0.9511476755142212, 'FPR95TPR': 0.06903991103172302, 'Method': 'Entropy', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9861979484558105, 'AUPR-IN': 0.9971905946731567, 'AUPR-OUT': 0.9507131576538086, 'FPR95TPR': 0.077130526304245, 'Method': 'Entropy', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9726436138153076, 'AUPR-IN': 0.991435170173645, 'AUPR-OUT': 0.899642288684845, 'FPR95TPR': 0.13160733878612518, 'Method': 'Entropy', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9882346987724304, 'AUPR-IN': 0.9977890849113464, 'AUPR-OUT': 0.9387224912643433, 'FPR95TPR': 0.058252427726984024, 'Method': 'Entropy', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9885106086730957, 'AUPR-IN': 0.9977529048919678, 'AUPR-OUT': 0.9501947164535522, 'FPR95TPR': 0.058252427726984024, 'Method': 'Entropy', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'LogicOOD+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'LogicOOD+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9999986290931702, 'AUPR-IN': 0.9999995231628418, 'AUPR-OUT': 0.9999960064888, 'FPR95TPR': 0.0, 'Method': 'LogicOOD+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'LogicOOD+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'LogicOOD+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.8326500058174133, 'AUPR-IN': 0.973825991153717, 'AUPR-OUT': 0.6782349348068237, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.8568500280380249, 'AUPR-IN': 0.9776109457015991, 'AUPR-OUT': 0.6965231895446777, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.7456560134887695, 'AUPR-IN': 0.9370758533477783, 'AUPR-OUT': 0.6962735652923584, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.7520999908447266, 'AUPR-IN': 0.9612277746200562, 'AUPR-OUT': 0.6360833644866943, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.821150004863739, 'AUPR-IN': 0.9720273613929749, 'AUPR-OUT': 0.6706867814064026, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'Logic+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'Logic+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9997340440750122, 'AUPR-IN': 0.999934196472168, 'AUPR-OUT': 0.9991922378540039, 'FPR95TPR': 0.0, 'Method': 'Logic+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'Logic+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'Logic+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.996891975402832, 'AUPR-IN': 0.9994224309921265, 'AUPR-OUT': 0.9849675297737122, 'FPR95TPR': 0.013484357856214046, 'Method': 'LogicOOD', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9967502355575562, 'AUPR-IN': 0.9994111657142639, 'AUPR-OUT': 0.9811365008354187, 'FPR95TPR': 0.009169363416731358, 'Method': 'LogicOOD', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.985975444316864, 'AUPR-IN': 0.9956793189048767, 'AUPR-OUT': 0.946767270565033, 'FPR95TPR': 0.05771305412054062, 'Method': 'LogicOOD', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9957689046859741, 'AUPR-IN': 0.9992192983627319, 'AUPR-OUT': 0.9783098697662354, 'FPR95TPR': 0.017259977757930756, 'Method': 'LogicOOD', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9950863122940063, 'AUPR-IN': 0.9991148114204407, 'AUPR-OUT': 0.9700089693069458, 'FPR95TPR': 0.014563106931746006, 'Method': 'LogicOOD', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9966920614242554, 'AUPR-IN': 0.9993839859962463, 'AUPR-OUT': 0.9841867685317993, 'FPR95TPR': 0.015102481469511986, 'Method': 'LogicOODT', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9967368245124817, 'AUPR-IN': 0.9994072914123535, 'AUPR-OUT': 0.9813587069511414, 'FPR95TPR': 0.010248112492263317, 'Method': 'LogicOODT', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9855492115020752, 'AUPR-IN': 0.9955469965934753, 'AUPR-OUT': 0.9454222917556763, 'FPR95TPR': 0.057173676788806915, 'Method': 'LogicOODT', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9955723285675049, 'AUPR-IN': 0.9991817474365234, 'AUPR-OUT': 0.9774006605148315, 'FPR95TPR': 0.01672060415148735, 'Method': 'LogicOODT', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9950921535491943, 'AUPR-IN': 0.9991132020950317, 'AUPR-OUT': 0.9706257581710815, 'FPR95TPR': 0.015102481469511986, 'Method': 'LogicOODT', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 0.9999999403953552, 'FPR95TPR': 0.0, 'Method': 'LogicOODT+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 0.9999999403953552, 'FPR95TPR': 0.0, 'Method': 'LogicOODT+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.999942421913147, 'AUPR-IN': 0.9999828338623047, 'AUPR-OUT': 0.9997895956039429, 'FPR95TPR': 0.0, 'Method': 'LogicOODT+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 0.9999999403953552, 'FPR95TPR': 0.0, 'Method': 'LogicOODT+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 0.9999999403953552, 'FPR95TPR': 0.0, 'Method': 'LogicOODT+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9953670501708984, 'AUPR-IN': 0.9991152882575989, 'AUPR-OUT': 0.9805752038955688, 'FPR95TPR': 0.028047464787960052, 'Method': 'Ensemble', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9879813194274902, 'AUPR-IN': 0.997513473033905, 'AUPR-OUT': 0.9588629603385925, 'FPR95TPR': 0.049622438848018646, 'Method': 'Ensemble', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9845219850540161, 'AUPR-IN': 0.9951106309890747, 'AUPR-OUT': 0.9445614218711853, 'FPR95TPR': 0.06040992587804794, 'Method': 'Ensemble', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9944945573806763, 'AUPR-IN': 0.998957097530365, 'AUPR-OUT': 0.9748138189315796, 'FPR95TPR': 0.020496224984526634, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9898672103881836, 'AUPR-IN': 0.9980040192604065, 'AUPR-OUT': 0.9567919373512268, 'FPR95TPR': 0.042610570788383484, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9889633655548096, 'AUPR-IN': 0.9978879690170288, 'AUPR-OUT': 0.9504032135009766, 'FPR95TPR': 0.06903991103172302, 'Method': 'MSP', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9854077100753784, 'AUPR-IN': 0.9969725608825684, 'AUPR-OUT': 0.949480414390564, 'FPR95TPR': 0.07874865084886551, 'Method': 'MSP', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9725596904754639, 'AUPR-IN': 0.9913148283958435, 'AUPR-OUT': 0.9009250402450562, 'FPR95TPR': 0.13160733878612518, 'Method': 'MSP', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9879070520401001, 'AUPR-IN': 0.9977008700370789, 'AUPR-OUT': 0.9386133551597595, 'FPR95TPR': 0.05663430318236351, 'Method': 'MSP', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9879889488220215, 'AUPR-IN': 0.9976083040237427, 'AUPR-OUT': 0.949833333492279, 'FPR95TPR': 0.05771305412054062, 'Method': 'MSP', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9042578935623169, 'AUPR-IN': 0.9818076491355896, 'AUPR-OUT': 0.5296933650970459, 'FPR95TPR': 0.5496224164962769, 'Method': 'ReAct', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.8968959450721741, 'AUPR-IN': 0.9781068563461304, 'AUPR-OUT': 0.6730402708053589, 'FPR95TPR': 0.4417475759983063, 'Method': 'ReAct', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.8381850719451904, 'AUPR-IN': 0.9442369937896729, 'AUPR-OUT': 0.5418516993522644, 'FPR95TPR': 0.7114347219467163, 'Method': 'ReAct', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.8996904492378235, 'AUPR-IN': 0.9801350831985474, 'AUPR-OUT': 0.587049126625061, 'FPR95TPR': 0.5113268494606018, 'Method': 'ReAct', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.8784241080284119, 'AUPR-IN': 0.9742212891578674, 'AUPR-OUT': 0.6274052262306213, 'FPR95TPR': 0.5032362341880798, 'Method': 'ReAct', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9877508878707886, 'AUPR-IN': 0.9977153539657593, 'AUPR-OUT': 0.9397531747817993, 'FPR95TPR': 0.07065803557634354, 'Method': 'Energy', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9827893972396851, 'AUPR-IN': 0.996567964553833, 'AUPR-OUT': 0.9336176514625549, 'FPR95TPR': 0.07551240921020508, 'Method': 'Energy', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9592185616493225, 'AUPR-IN': 0.987434983253479, 'AUPR-OUT': 0.8443040251731873, 'FPR95TPR': 0.24811218678951263, 'Method': 'Energy', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9838482141494751, 'AUPR-IN': 0.997001051902771, 'AUPR-OUT': 0.9135653972625732, 'FPR95TPR': 0.07497303187847137, 'Method': 'Energy', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9841996431350708, 'AUPR-IN': 0.9969748258590698, 'AUPR-OUT': 0.926087498664856, 'FPR95TPR': 0.07119741290807724, 'Method': 'Energy', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9877946972846985, 'AUPR-IN': 0.9977238178253174, 'AUPR-OUT': 0.9399073123931885, 'FPR95TPR': 0.06957928836345673, 'Method': 'MaxLogit', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9829814434051514, 'AUPR-IN': 0.9966121912002563, 'AUPR-OUT': 0.9339195489883423, 'FPR95TPR': 0.07551240921020508, 'Method': 'MaxLogit', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9596689939498901, 'AUPR-IN': 0.987610936164856, 'AUPR-OUT': 0.8450219631195068, 'FPR95TPR': 0.24811218678951263, 'Method': 'MaxLogit', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9840361475944519, 'AUPR-IN': 0.9970413446426392, 'AUPR-OUT': 0.9139369130134583, 'FPR95TPR': 0.07497303187847137, 'Method': 'MaxLogit', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUROC': 0.9844142198562622, 'AUPR-IN': 0.9970226287841797, 'AUPR-OUT': 0.9265136122703552, 'FPR95TPR': 0.07119741290807724, 'Method': 'MaxLogit', 'Dataset': 'TinyImageNetResize'}\n",
      "label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.12it/s, loss=0.119] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 96.06%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 31.49it/s, loss=0.0484]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 98.65%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 31.31it/s, loss=0.0209] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 97.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 30.87it/s, loss=0.0172] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 97.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.01it/s, loss=0.011]  \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.73%\n",
      "color\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.20it/s, loss=0.0418]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 98.33%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.03it/s, loss=0.0136] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 95.20%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.23it/s, loss=0.0192] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.17it/s, loss=0.00464]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.03it/s, loss=0.0125]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.89%\n",
      "fruit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.11it/s, loss=0.00152] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.13it/s, loss=0.00317] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.14it/s, loss=0.000212]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.20it/s, loss=0.000315]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.11it/s, loss=0.000425]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.07it/s, loss=0.00257] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.09it/s, loss=3.68e-5] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.06it/s, loss=0.000205]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.04it/s, loss=0.000199]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.15it/s, loss=0.0047]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.95%\n",
      "Computing principal space ...\n",
      "Computing alpha ...\n",
      "self.alpha=6.5715\n",
      "Fitting with temperature scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial T/NLL: 1.000/0.011\n",
      "Optimal temperature: 0.9057022333145142\n",
      "NLL after scaling: 0.01'\n",
      "Initial T/NLL: 1.000/0.009\n",
      "Optimal temperature: 0.8244858980178833\n",
      "NLL after scaling: 0.01'\n",
      "self.scorer_label.t=Parameter containing:\n",
      "tensor(0.9057, requires_grad=True)\n",
      "self.scorer_color.t=Parameter containing:\n",
      "tensor(0.8245, requires_grad=True)\n",
      "Fitting with temperature scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:84: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels).item()\n",
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:90: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels)\n",
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:97: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels).item()\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial T/NLL: 1.000/0.011\n",
      "Optimal temperature: 0.9057022333145142\n",
      "NLL after scaling: 0.01'\n",
      "Initial T/NLL: 1.000/0.009\n",
      "Optimal temperature: 0.8244858980178833\n",
      "NLL after scaling: 0.01'\n",
      "self.scorer_label.t=Parameter containing:\n",
      "tensor(0.9057, requires_grad=True)\n",
      "self.scorer_color.t=Parameter containing:\n",
      "tensor(0.8245, requires_grad=True)\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9976770281791687, 'AUPR-IN': 0.9995774030685425, 'AUPR-OUT': 0.9872215986251831, 'FPR95TPR': 0.0026968715246766806, 'Method': 'ViM', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9999550580978394, 'AUPR-IN': 0.9999918341636658, 'AUPR-OUT': 0.9997434616088867, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9997258186340332, 'AUPR-IN': 0.9999125003814697, 'AUPR-OUT': 0.9991300106048584, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9988915920257568, 'AUPR-IN': 0.9997973442077637, 'AUPR-OUT': 0.9940083026885986, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9998320937156677, 'AUPR-IN': 0.9999694228172302, 'AUPR-OUT': 0.9990519881248474, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9923864603042603, 'AUPR-IN': 0.9985532164573669, 'AUPR-OUT': 0.9659882187843323, 'FPR95TPR': 0.028586838394403458, 'Method': 'Mahalanobis', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9995590448379517, 'AUPR-IN': 0.9999189972877502, 'AUPR-OUT': 0.9976476430892944, 'FPR95TPR': 0.0005393743049353361, 'Method': 'Mahalanobis', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9974877834320068, 'AUPR-IN': 0.9991903305053711, 'AUPR-OUT': 0.9924637675285339, 'FPR95TPR': 0.010248112492263317, 'Method': 'Mahalanobis', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.994488000869751, 'AUPR-IN': 0.9989535212516785, 'AUPR-OUT': 0.9759231805801392, 'FPR95TPR': 0.02319309674203396, 'Method': 'Mahalanobis', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9991189241409302, 'AUPR-IN': 0.9998379349708557, 'AUPR-OUT': 0.9953905344009399, 'FPR95TPR': 0.0016181230312213302, 'Method': 'Mahalanobis', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.7944454550743103, 'AUPR-IN': 0.9562592506408691, 'AUPR-OUT': 0.43598034977912903, 'FPR95TPR': 0.7281553149223328, 'Method': 'Entropy', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9864320158958435, 'AUPR-IN': 0.997306227684021, 'AUPR-OUT': 0.9469171166419983, 'FPR95TPR': 0.05771305412054062, 'Method': 'Entropy', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9363276362419128, 'AUPR-IN': 0.9788708686828613, 'AUPR-OUT': 0.7937982082366943, 'FPR95TPR': 0.3543689250946045, 'Method': 'Entropy', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.951485276222229, 'AUPR-IN': 0.9906126260757446, 'AUPR-OUT': 0.7822823524475098, 'FPR95TPR': 0.2982740104198456, 'Method': 'Entropy', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9768120050430298, 'AUPR-IN': 0.9954202175140381, 'AUPR-OUT': 0.8927527666091919, 'FPR95TPR': 0.09385113418102264, 'Method': 'Entropy', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9991908073425293, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.9993174076080322, 'FPR95TPR': 0.0016181230312213302, 'Method': 'LogicOOD+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9991908073425293, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.9993174076080322, 'FPR95TPR': 0.0016181230312213302, 'Method': 'LogicOOD+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9989829659461975, 'AUPR-IN': 0.9996665716171265, 'AUPR-OUT': 0.9987108111381531, 'FPR95TPR': 0.0016181230312213302, 'Method': 'LogicOOD+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9991908073425293, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.9993174076080322, 'FPR95TPR': 0.0016181230312213302, 'Method': 'LogicOOD+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9991908073425293, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.9993174076080322, 'FPR95TPR': 0.0016181230312213302, 'Method': 'LogicOOD+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.7817909121513367, 'AUPR-IN': 0.9657328128814697, 'AUPR-OUT': 0.6486157774925232, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.71639084815979, 'AUPR-IN': 0.9554242491722107, 'AUPR-OUT': 0.6226024031639099, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.7320809960365295, 'AUPR-IN': 0.933347225189209, 'AUPR-OUT': 0.6896665692329407, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.7000909447669983, 'AUPR-IN': 0.9528468251228333, 'AUPR-OUT': 0.61747145652771, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.7310409545898438, 'AUPR-IN': 0.9577373266220093, 'AUPR-OUT': 0.6276096701622009, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9991909265518188, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.999317467212677, 'FPR95TPR': 0.0016181230312213302, 'Method': 'Logic+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9991909265518188, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.999317467212677, 'FPR95TPR': 0.0016181230312213302, 'Method': 'Logic+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.995290219783783, 'AUPR-IN': 0.998767077922821, 'AUPR-OUT': 0.9877815842628479, 'FPR95TPR': 0.0016181230312213302, 'Method': 'Logic+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9991909265518188, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.999317467212677, 'FPR95TPR': 0.0016181230312213302, 'Method': 'Logic+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9991909265518188, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.999317467212677, 'FPR95TPR': 0.0016181230312213302, 'Method': 'Logic+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9752162098884583, 'AUPR-IN': 0.9952415227890015, 'AUPR-OUT': 0.8824068307876587, 'FPR95TPR': 0.1218985989689827, 'Method': 'LogicOOD', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9872544407844543, 'AUPR-IN': 0.9973430037498474, 'AUPR-OUT': 0.9539545774459839, 'FPR95TPR': 0.06256742030382156, 'Method': 'LogicOOD', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9642861485481262, 'AUPR-IN': 0.9883062839508057, 'AUPR-OUT': 0.881704568862915, 'FPR95TPR': 0.188241645693779, 'Method': 'LogicOOD', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9762915372848511, 'AUPR-IN': 0.9952332973480225, 'AUPR-OUT': 0.9056733846664429, 'FPR95TPR': 0.12405609339475632, 'Method': 'LogicOOD', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9832921028137207, 'AUPR-IN': 0.9966354370117188, 'AUPR-OUT': 0.9301831126213074, 'FPR95TPR': 0.07659115642309189, 'Method': 'LogicOOD', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9736438989639282, 'AUPR-IN': 0.9949648976325989, 'AUPR-OUT': 0.8703773021697998, 'FPR95TPR': 0.12729233503341675, 'Method': 'LogicOODT', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9873182773590088, 'AUPR-IN': 0.9973379373550415, 'AUPR-OUT': 0.9549952149391174, 'FPR95TPR': 0.06202804669737816, 'Method': 'LogicOODT', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.964272677898407, 'AUPR-IN': 0.9882887601852417, 'AUPR-OUT': 0.8796446323394775, 'FPR95TPR': 0.188241645693779, 'Method': 'LogicOODT', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.975757896900177, 'AUPR-IN': 0.9951194524765015, 'AUPR-OUT': 0.9025302529335022, 'FPR95TPR': 0.12405609339475632, 'Method': 'LogicOODT', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.983553946018219, 'AUPR-IN': 0.9966684579849243, 'AUPR-OUT': 0.9321920275688171, 'FPR95TPR': 0.07497303187847137, 'Method': 'LogicOODT', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9991908669471741, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.9993174076080322, 'FPR95TPR': 0.0016181230312213302, 'Method': 'LogicOODT+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9991908669471741, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.9993174076080322, 'FPR95TPR': 0.0016181230312213302, 'Method': 'LogicOODT+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9984906315803528, 'AUPR-IN': 0.9995208978652954, 'AUPR-OUT': 0.9968052506446838, 'FPR95TPR': 0.0016181230312213302, 'Method': 'LogicOODT+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9991908669471741, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.9993174076080322, 'FPR95TPR': 0.0016181230312213302, 'Method': 'LogicOODT+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9991908669471741, 'AUPR-IN': 0.9998500347137451, 'AUPR-OUT': 0.9993174076080322, 'FPR95TPR': 0.0016181230312213302, 'Method': 'LogicOODT+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9584138989448547, 'AUPR-IN': 0.9909092783927917, 'AUPR-OUT': 0.851934552192688, 'FPR95TPR': 0.18015103042125702, 'Method': 'Ensemble', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9813398718833923, 'AUPR-IN': 0.9959747791290283, 'AUPR-OUT': 0.9408863186836243, 'FPR95TPR': 0.07928802818059921, 'Method': 'Ensemble', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9509152173995972, 'AUPR-IN': 0.9827695488929749, 'AUPR-OUT': 0.8605999946594238, 'FPR95TPR': 0.23732469975948334, 'Method': 'Ensemble', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9713539481163025, 'AUPR-IN': 0.9940680861473083, 'AUPR-OUT': 0.8958913683891296, 'FPR95TPR': 0.1343042105436325, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9746840000152588, 'AUPR-IN': 0.9945638179779053, 'AUPR-OUT': 0.9135130643844604, 'FPR95TPR': 0.10463862121105194, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.7875128984451294, 'AUPR-IN': 0.9543750286102295, 'AUPR-OUT': 0.4199282228946686, 'FPR95TPR': 0.747033417224884, 'Method': 'MSP', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9845238327980042, 'AUPR-IN': 0.996664822101593, 'AUPR-OUT': 0.9452770948410034, 'FPR95TPR': 0.059331174939870834, 'Method': 'MSP', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.934188723564148, 'AUPR-IN': 0.9775198101997375, 'AUPR-OUT': 0.7900020480155945, 'FPR95TPR': 0.35976266860961914, 'Method': 'MSP', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9494740962982178, 'AUPR-IN': 0.9899179935455322, 'AUPR-OUT': 0.7766144871711731, 'FPR95TPR': 0.3015102446079254, 'Method': 'MSP', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9751464128494263, 'AUPR-IN': 0.9947988390922546, 'AUPR-OUT': 0.8911382555961609, 'FPR95TPR': 0.09654800593852997, 'Method': 'MSP', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.4403149485588074, 'AUPR-IN': 0.8480811715126038, 'AUPR-OUT': 0.13230189681053162, 'FPR95TPR': 0.9498381614685059, 'Method': 'ReAct', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.948388934135437, 'AUPR-IN': 0.9892035722732544, 'AUPR-OUT': 0.7928518652915955, 'FPR95TPR': 0.2696871757507324, 'Method': 'ReAct', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.8750543594360352, 'AUPR-IN': 0.9569637775421143, 'AUPR-OUT': 0.641716480255127, 'FPR95TPR': 0.6812297701835632, 'Method': 'ReAct', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.7931983470916748, 'AUPR-IN': 0.9519684314727783, 'AUPR-OUT': 0.3688119053840637, 'FPR95TPR': 0.8403452038764954, 'Method': 'ReAct', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9307359457015991, 'AUPR-IN': 0.9860720038414001, 'AUPR-OUT': 0.7081186175346375, 'FPR95TPR': 0.3797195255756378, 'Method': 'ReAct', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.8288862705230713, 'AUPR-IN': 0.9625055193901062, 'AUPR-OUT': 0.5405849814414978, 'FPR95TPR': 0.608953595161438, 'Method': 'Energy', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.984736442565918, 'AUPR-IN': 0.99692702293396, 'AUPR-OUT': 0.9366026520729065, 'FPR95TPR': 0.058252427726984024, 'Method': 'Energy', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9388943314552307, 'AUPR-IN': 0.979571521282196, 'AUPR-OUT': 0.807090163230896, 'FPR95TPR': 0.3236245810985565, 'Method': 'Energy', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9505482316017151, 'AUPR-IN': 0.990308403968811, 'AUPR-OUT': 0.7865170240402222, 'FPR95TPR': 0.29557713866233826, 'Method': 'Energy', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9743354320526123, 'AUPR-IN': 0.994879424571991, 'AUPR-OUT': 0.8838624954223633, 'FPR95TPR': 0.09924487769603729, 'Method': 'Energy', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.8288378119468689, 'AUPR-IN': 0.9626016020774841, 'AUPR-OUT': 0.5404202938079834, 'FPR95TPR': 0.6084142327308655, 'Method': 'MaxLogit', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9851561784744263, 'AUPR-IN': 0.9970697164535522, 'AUPR-OUT': 0.9367654323577881, 'FPR95TPR': 0.059331174939870834, 'Method': 'MaxLogit', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9393028020858765, 'AUPR-IN': 0.979839563369751, 'AUPR-OUT': 0.8075613379478455, 'FPR95TPR': 0.3236245810985565, 'Method': 'MaxLogit', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9509010910987854, 'AUPR-IN': 0.9904495477676392, 'AUPR-OUT': 0.7866715788841248, 'FPR95TPR': 0.29557713866233826, 'Method': 'MaxLogit', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUROC': 0.9747579097747803, 'AUPR-IN': 0.9950323700904846, 'AUPR-OUT': 0.8840968608856201, 'FPR95TPR': 0.09762675315141678, 'Method': 'MaxLogit', 'Dataset': 'TinyImageNetResize'}\n",
      "label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 30.90it/s, loss=0.0432]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 92.45%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.21it/s, loss=0.0512]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 98.38%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.14it/s, loss=0.021]  \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 98.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 31.40it/s, loss=0.0113] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 30.94it/s, loss=0.0133] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.30%\n",
      "color\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.08it/s, loss=0.0409]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 88.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 30.89it/s, loss=0.0173] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.10it/s, loss=0.00342]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 96.39%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.25it/s, loss=0.00335]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 97.25%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 31.18it/s, loss=0.00157] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.35%\n",
      "fruit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.22it/s, loss=0.00194] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.17it/s, loss=0.000451]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.13it/s, loss=0.000835]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.86%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:03<00:00, 31.25it/s, loss=0.00017] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.11it/s, loss=0.00164] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.11it/s, loss=0.000219]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.12it/s, loss=0.000679]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.07it/s, loss=0.000878]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.20it/s, loss=0.000191]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [01:04<00:00, 31.12it/s, loss=0.000159]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n",
      "Computing principal space ...\n",
      "Computing alpha ...\n",
      "self.alpha=6.5099\n",
      "Fitting with temperature scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial T/NLL: 1.000/0.015\n",
      "Optimal temperature: 0.943420946598053\n",
      "NLL after scaling: 0.01'\n",
      "Initial T/NLL: 1.000/0.030\n",
      "Optimal temperature: 0.8378126621246338\n",
      "NLL after scaling: 0.03'\n",
      "self.scorer_label.t=Parameter containing:\n",
      "tensor(0.9434, requires_grad=True)\n",
      "self.scorer_color.t=Parameter containing:\n",
      "tensor(0.8378, requires_grad=True)\n",
      "Fitting with temperature scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:84: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels).item()\n",
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:90: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels)\n",
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:97: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels).item()\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial T/NLL: 1.000/0.015\n",
      "Optimal temperature: 0.943420946598053\n",
      "NLL after scaling: 0.01'\n",
      "Initial T/NLL: 1.000/0.030\n",
      "Optimal temperature: 0.8378126621246338\n",
      "NLL after scaling: 0.03'\n",
      "self.scorer_label.t=Parameter containing:\n",
      "tensor(0.9434, requires_grad=True)\n",
      "self.scorer_color.t=Parameter containing:\n",
      "tensor(0.8378, requires_grad=True)\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9992287158966064, 'AUPR-IN': 0.9998577833175659, 'AUPR-OUT': 0.9959626197814941, 'FPR95TPR': 0.0005393743049353361, 'Method': 'ViM', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9999430179595947, 'AUPR-IN': 0.9999895095825195, 'AUPR-OUT': 0.9996877908706665, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9999796748161316, 'AUPR-IN': 0.9999932646751404, 'AUPR-OUT': 0.999937891960144, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9993524551391602, 'AUPR-IN': 0.999880313873291, 'AUPR-OUT': 0.9966566562652588, 'FPR95TPR': 0.0005393743049353361, 'Method': 'ViM', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.99988853931427, 'AUPR-IN': 0.9999793767929077, 'AUPR-OUT': 0.9994058609008789, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9968070983886719, 'AUPR-IN': 0.9994296431541443, 'AUPR-OUT': 0.9811626672744751, 'FPR95TPR': 0.0005393743049353361, 'Method': 'Mahalanobis', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.999819278717041, 'AUPR-IN': 0.9999673366546631, 'AUPR-OUT': 0.9989420175552368, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9997597932815552, 'AUPR-IN': 0.9999229907989502, 'AUPR-OUT': 0.9992428421974182, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9969192743301392, 'AUPR-IN': 0.9994434118270874, 'AUPR-OUT': 0.9827474355697632, 'FPR95TPR': 0.0016181230312213302, 'Method': 'Mahalanobis', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9995676875114441, 'AUPR-IN': 0.9999213814735413, 'AUPR-OUT': 0.9975711107254028, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9593039751052856, 'AUPR-IN': 0.9926040768623352, 'AUPR-OUT': 0.7597197890281677, 'FPR95TPR': 0.245954692363739, 'Method': 'Entropy', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9714739918708801, 'AUPR-IN': 0.9939836859703064, 'AUPR-OUT': 0.9075446724891663, 'FPR95TPR': 0.1343042105436325, 'Method': 'Entropy', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9234277606010437, 'AUPR-IN': 0.9753870368003845, 'AUPR-OUT': 0.7566969394683838, 'FPR95TPR': 0.4670981764793396, 'Method': 'Entropy', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9761037826538086, 'AUPR-IN': 0.9953868389129639, 'AUPR-OUT': 0.8945112228393555, 'FPR95TPR': 0.11812297999858856, 'Method': 'Entropy', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9690520763397217, 'AUPR-IN': 0.9936829805374146, 'AUPR-OUT': 0.8851377367973328, 'FPR95TPR': 0.15102481842041016, 'Method': 'Entropy', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9967637658119202, 'AUPR-IN': 0.9994007349014282, 'AUPR-OUT': 0.9972699284553528, 'FPR95TPR': 0.006472492124885321, 'Method': 'LogicOOD+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9967637658119202, 'AUPR-IN': 0.9994007349014282, 'AUPR-OUT': 0.9972699284553528, 'FPR95TPR': 0.006472492124885321, 'Method': 'LogicOOD+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9965381622314453, 'AUPR-IN': 0.9988638162612915, 'AUPR-OUT': 0.9967982769012451, 'FPR95TPR': 0.006472492124885321, 'Method': 'LogicOOD+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9967637658119202, 'AUPR-IN': 0.9994007349014282, 'AUPR-OUT': 0.9972699284553528, 'FPR95TPR': 0.006472492124885321, 'Method': 'LogicOOD+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9967637658119202, 'AUPR-IN': 0.9994007349014282, 'AUPR-OUT': 0.9972699284553528, 'FPR95TPR': 0.006472492124885321, 'Method': 'LogicOOD+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.7648137211799622, 'AUPR-IN': 0.9626055955886841, 'AUPR-OUT': 0.6393776535987854, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.7350137829780579, 'AUPR-IN': 0.9578055143356323, 'AUPR-OUT': 0.627409815788269, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.7192814946174622, 'AUPR-IN': 0.9289724230766296, 'AUPR-OUT': 0.6828017234802246, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.7338137626647949, 'AUPR-IN': 0.9576114416122437, 'AUPR-OUT': 0.6269699931144714, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.7408137917518616, 'AUPR-IN': 0.9587424397468567, 'AUPR-OUT': 0.6295785307884216, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9967637658119202, 'AUPR-IN': 0.9994007349014282, 'AUPR-OUT': 0.9972699284553528, 'FPR95TPR': 0.006472492124885321, 'Method': 'Logic+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9967637658119202, 'AUPR-IN': 0.9994007349014282, 'AUPR-OUT': 0.9972699284553528, 'FPR95TPR': 0.006472492124885321, 'Method': 'Logic+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9945474863052368, 'AUPR-IN': 0.9983853697776794, 'AUPR-OUT': 0.9908691048622131, 'FPR95TPR': 0.006472492124885321, 'Method': 'Logic+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9967637658119202, 'AUPR-IN': 0.9994007349014282, 'AUPR-OUT': 0.9972699284553528, 'FPR95TPR': 0.006472492124885321, 'Method': 'Logic+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9967637658119202, 'AUPR-IN': 0.9994007349014282, 'AUPR-OUT': 0.9972699284553528, 'FPR95TPR': 0.006472492124885321, 'Method': 'Logic+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9691718220710754, 'AUPR-IN': 0.9938775897026062, 'AUPR-OUT': 0.8594620823860168, 'FPR95TPR': 0.16289104521274567, 'Method': 'LogicOOD', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9752044677734375, 'AUPR-IN': 0.9945329427719116, 'AUPR-OUT': 0.9264941215515137, 'FPR95TPR': 0.10409924387931824, 'Method': 'LogicOOD', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9477678537368774, 'AUPR-IN': 0.9820874333381653, 'AUPR-OUT': 0.8515310287475586, 'FPR95TPR': 0.30690398812294006, 'Method': 'LogicOOD', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9764782190322876, 'AUPR-IN': 0.9949327111244202, 'AUPR-OUT': 0.9168253540992737, 'FPR95TPR': 0.08306364715099335, 'Method': 'LogicOOD', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9749277830123901, 'AUPR-IN': 0.9945821166038513, 'AUPR-OUT': 0.9171141982078552, 'FPR95TPR': 0.10355987399816513, 'Method': 'LogicOOD', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9664628505706787, 'AUPR-IN': 0.9934081435203552, 'AUPR-OUT': 0.8378636837005615, 'FPR95TPR': 0.165587916970253, 'Method': 'LogicOODT', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9749723076820374, 'AUPR-IN': 0.9944625496864319, 'AUPR-OUT': 0.9267786741256714, 'FPR95TPR': 0.09924487769603729, 'Method': 'LogicOODT', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9453008770942688, 'AUPR-IN': 0.9813616275787354, 'AUPR-OUT': 0.8396746516227722, 'FPR95TPR': 0.32578209042549133, 'Method': 'LogicOODT', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9753334522247314, 'AUPR-IN': 0.9947010278701782, 'AUPR-OUT': 0.9109799861907959, 'FPR95TPR': 0.0889967605471611, 'Method': 'LogicOODT', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9744319915771484, 'AUPR-IN': 0.9944674968719482, 'AUPR-OUT': 0.9157094359397888, 'FPR95TPR': 0.10302049666643143, 'Method': 'LogicOODT', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9967638254165649, 'AUPR-IN': 0.9994007349014282, 'AUPR-OUT': 0.9972699880599976, 'FPR95TPR': 0.006472492124885321, 'Method': 'LogicOODT+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9967638254165649, 'AUPR-IN': 0.9994007349014282, 'AUPR-OUT': 0.9972699880599976, 'FPR95TPR': 0.006472492124885321, 'Method': 'LogicOODT+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9962486028671265, 'AUPR-IN': 0.998777449131012, 'AUPR-OUT': 0.9957392811775208, 'FPR95TPR': 0.006472492124885321, 'Method': 'LogicOODT+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9967638254165649, 'AUPR-IN': 0.9994007349014282, 'AUPR-OUT': 0.9972699880599976, 'FPR95TPR': 0.006472492124885321, 'Method': 'LogicOODT+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9967638254165649, 'AUPR-IN': 0.9994007349014282, 'AUPR-OUT': 0.9972699880599976, 'FPR95TPR': 0.006472492124885321, 'Method': 'LogicOODT+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9679440855979919, 'AUPR-IN': 0.9938051700592041, 'AUPR-OUT': 0.8559023141860962, 'FPR95TPR': 0.16289104521274567, 'Method': 'Ensemble', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9631729125976562, 'AUPR-IN': 0.9911671876907349, 'AUPR-OUT': 0.9100760221481323, 'FPR95TPR': 0.13592232763767242, 'Method': 'Ensemble', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9400213360786438, 'AUPR-IN': 0.9787793159484863, 'AUPR-OUT': 0.8417144417762756, 'FPR95TPR': 0.323085218667984, 'Method': 'Ensemble', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9740961194038391, 'AUPR-IN': 0.9945841431617737, 'AUPR-OUT': 0.9123238325119019, 'FPR95TPR': 0.0895361378788948, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9647574424743652, 'AUPR-IN': 0.9919063448905945, 'AUPR-OUT': 0.9024584889411926, 'FPR95TPR': 0.13160733878612518, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9548472762107849, 'AUPR-IN': 0.9913896918296814, 'AUPR-OUT': 0.7528305053710938, 'FPR95TPR': 0.24919094145298004, 'Method': 'MSP', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9673613905906677, 'AUPR-IN': 0.992332398891449, 'AUPR-OUT': 0.9040465950965881, 'FPR95TPR': 0.14185544848442078, 'Method': 'MSP', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.919358491897583, 'AUPR-IN': 0.9727970361709595, 'AUPR-OUT': 0.7535417079925537, 'FPR95TPR': 0.4757281541824341, 'Method': 'MSP', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9711307883262634, 'AUPR-IN': 0.9939354062080383, 'AUPR-OUT': 0.8874828219413757, 'FPR95TPR': 0.13106796145439148, 'Method': 'MSP', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9649555683135986, 'AUPR-IN': 0.9921531677246094, 'AUPR-OUT': 0.8816080689430237, 'FPR95TPR': 0.15857605636119843, 'Method': 'MSP', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.8389219641685486, 'AUPR-IN': 0.9684432744979858, 'AUPR-OUT': 0.3731611371040344, 'FPR95TPR': 0.8608414530754089, 'Method': 'ReAct', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9184773564338684, 'AUPR-IN': 0.9805482625961304, 'AUPR-OUT': 0.7317312955856323, 'FPR95TPR': 0.2971952557563782, 'Method': 'ReAct', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.8965386152267456, 'AUPR-IN': 0.9671890735626221, 'AUPR-OUT': 0.643992006778717, 'FPR95TPR': 0.6796116232872009, 'Method': 'ReAct', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.8737855553627014, 'AUPR-IN': 0.9747709035873413, 'AUPR-OUT': 0.4940768778324127, 'FPR95TPR': 0.7141315937042236, 'Method': 'ReAct', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9145612716674805, 'AUPR-IN': 0.9808261394500732, 'AUPR-OUT': 0.6743378043174744, 'FPR95TPR': 0.3468177020549774, 'Method': 'ReAct', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9700268507003784, 'AUPR-IN': 0.9947563409805298, 'AUPR-OUT': 0.8083497285842896, 'FPR95TPR': 0.1898597627878189, 'Method': 'Energy', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.978776752948761, 'AUPR-IN': 0.9958608746528625, 'AUPR-OUT': 0.914181649684906, 'FPR95TPR': 0.12675297260284424, 'Method': 'Energy', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9314066171646118, 'AUPR-IN': 0.9790143966674805, 'AUPR-OUT': 0.7658869624137878, 'FPR95TPR': 0.5016181468963623, 'Method': 'Energy', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9856904745101929, 'AUPR-IN': 0.9974073171615601, 'AUPR-OUT': 0.9206119179725647, 'FPR95TPR': 0.06310679763555527, 'Method': 'Energy', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9757605791091919, 'AUPR-IN': 0.9953705072402954, 'AUPR-OUT': 0.8901668787002563, 'FPR95TPR': 0.13915857672691345, 'Method': 'Energy', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9695709943771362, 'AUPR-IN': 0.9946567416191101, 'AUPR-OUT': 0.807409942150116, 'FPR95TPR': 0.19147787988185883, 'Method': 'MaxLogit', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9780890941619873, 'AUPR-IN': 0.9956657290458679, 'AUPR-OUT': 0.9130763411521912, 'FPR95TPR': 0.12998920679092407, 'Method': 'MaxLogit', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9308594465255737, 'AUPR-IN': 0.9787495136260986, 'AUPR-OUT': 0.7653294801712036, 'FPR95TPR': 0.5016181468963623, 'Method': 'MaxLogit', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9851874113082886, 'AUPR-IN': 0.9972950220108032, 'AUPR-OUT': 0.9195469617843628, 'FPR95TPR': 0.0663430392742157, 'Method': 'MaxLogit', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.97511887550354, 'AUPR-IN': 0.995198130607605, 'AUPR-OUT': 0.8891298770904541, 'FPR95TPR': 0.14131607115268707, 'Method': 'MaxLogit', 'Dataset': 'TinyImageNetResize'}\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for trial in range(10):\n",
    "    print(\"label\")\n",
    "    label_net = train_model(att_index=0, num_classes=33)\n",
    "    print(\"color\")\n",
    "    color_net = train_model(att_index=1, num_classes=6)\n",
    "    print(\"fruit\")\n",
    "\n",
    "    fruit_net = train_fruit_model()\n",
    "\n",
    "    res = evaluate(label_net, color_net, fruit_net)\n",
    "    \n",
    "    for r in res:\n",
    "        r.update({\"Seed\": trial})\n",
    "    \n",
    "    results += res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c3e5287e-1bd0-4bfd-a73f-e427f369f3c2",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T19:35:37.230324320Z",
     "start_time": "2023-08-25T19:35:37.228523762Z"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result_df = pd.DataFrame(results)\n",
    "# print((result_df.groupby(by=\"Method\").agg([\"mean\", \"sem\"]) * 100)[[\"AUROC\", \"AUPR-IN\", \"AUPR-OUT\", \"FPR95TPR\"]].to_latex(float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "# s = (result_df.groupby(by=\"Method\").agg([\"mean\", \"sem\"]) * 100)[[\"AUROC\", \"AUPR-IN\", \"AUPR-OUT\", \"FPR95TPR\"]].to_latex(float_format=\"%.2f\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T19:35:37.233926944Z",
     "start_time": "2023-08-25T19:35:37.231533647Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{2}{l}{AUROC} & \\multicolumn{2}{l}{AUPR-IN} & \\multicolumn{2}{l}{AUPR-OUT} & \\multicolumn{2}{l}{FPR95TPR} \\\\\n",
      "{} &  mean &  sem &    mean &  sem &     mean &  sem &     mean &  sem \\\\\n",
      "Method      &       &      &         &      &          &      &          &      \\\\\n",
      "\\midrule\n",
      "MSP         & 96.40 & $\\pm$ 0.59 &   99.13 & $\\pm$ 0.14 &    85.99 & $\\pm$ 1.79 &    18.35 & $\\pm$ 2.84 \\\\\n",
      "Energy      & 96.74 & $\\pm$ 0.52 &   99.24 & $\\pm$ 0.12 &    86.15 & $\\pm$ 1.70 &    18.55 & $\\pm$ 2.77 \\\\\n",
      "MaxLogit    & 96.73 & $\\pm$ 0.52 &   99.24 & $\\pm$ 0.12 &    86.14 & $\\pm$ 1.69 &    18.56 & $\\pm$ 2.77 \\\\\n",
      "Entropy     & 96.61 & $\\pm$ 0.56 &   99.20 & $\\pm$ 0.13 &    86.22 & $\\pm$ 1.75 &    18.09 & $\\pm$ 2.79 \\\\\n",
      "ReAct       & 88.21 & $\\pm$ 1.13 &   96.94 & $\\pm$ 0.34 &    63.33 & $\\pm$ 2.01 &    50.98 & $\\pm$ 2.25 \\\\\n",
      "Mahalanobis & 99.86 & $\\pm$ 0.05 &   99.97 & $\\pm$ 0.01 &    99.29 & $\\pm$ 0.24 &     0.36 & $\\pm$ 0.17 \\\\\n",
      "ViM         & 99.94 & $\\pm$ 0.02 &   99.99 & $\\pm$ 0.00 &    99.72 & $\\pm$ 0.09 &     0.06 & $\\pm$ 0.02 \\\\\n",
      "Ensemble    & 98.19 & $\\pm$ 0.33 &   99.54 & $\\pm$ 0.09 &    93.54 & $\\pm$ 0.96 &     8.58 & $\\pm$ 1.45 \\\\\n",
      "Logic       & 74.55 & $\\pm$ 1.07 &   95.50 & $\\pm$ 0.18 &    64.78 & $\\pm$ 0.47 &   100.00 & $\\pm$ 0.00 \\\\\n",
      "Logic+      & 99.88 & $\\pm$ 0.04 &   99.97 & $\\pm$ 0.01 &    99.83 & $\\pm$ 0.04 &     0.17 & $\\pm$ 0.06 \\\\\n",
      "LogicOOD    & 98.51 & $\\pm$ 0.26 &   99.63 & $\\pm$ 0.07 &    94.23 & $\\pm$ 0.88 &     7.33 & $\\pm$ 1.34 \\\\\n",
      "LogicOOD+   & 99.91 & $\\pm$ 0.03 &   99.98 & $\\pm$ 0.01 &    99.93 & $\\pm$ 0.03 &     0.17 & $\\pm$ 0.06 \\\\\n",
      "LogicOODT   & 98.50 & $\\pm$ 0.27 &   99.62 & $\\pm$ 0.07 &    94.18 & $\\pm$ 0.93 &     7.22 & $\\pm$ 1.37 \\\\\n",
      "LogicOODT+  & 99.91 & $\\pm$ 0.03 &   99.98 & $\\pm$ 0.01 &    99.91 & $\\pm$ 0.03 &     0.17 & $\\pm$ 0.06 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "order = ['MSP', 'Energy', 'MaxLogit', 'Entropy', 'ReAct', 'Mahalanobis', 'ViM', 'Ensemble', 'Logic', 'Logic+', 'LogicOOD', 'LogicOOD+', 'LogicOODT', 'LogicOODT+']\n",
    "\n",
    "\n",
    "print((result_df.groupby(by=[\"Method\", \"Seed\"]).mean() * 100).groupby(\"Method\").agg([\"mean\", \"sem\"]).reindex(order).to_latex(float_format=\"%.2f\").replace(\"& 0.\", \"& $\\pm$ 0.\").replace(\"& 2.\", \"& $\\pm$ 2.\").replace(\"& 3.\", \"& $\\pm$ 3.\").replace(\"& 1.\", \"& $\\pm$ 1.\").replace(\"& 4.\", \"& $\\pm$ 4.\").replace(\"& 5.\", \"& $\\pm$ 5.\"))\n",
    "\n",
    "\n",
    "# print(s.replace(\"& 0.\", \"& \\pm 0.\").replace(\"& 1.\", \"& \\pm 1.\").replace(\"& 2.\", \"& \\pm 2.\").replace(\"& 4.\", \"& \\pm 4.\"))"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T19:35:37.282060105Z",
     "start_time": "2023-08-25T19:35:37.241633028Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T19:35:37.285400559Z",
     "start_time": "2023-08-25T19:35:37.281628702Z"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
