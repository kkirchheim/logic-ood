{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "09e68aaa-4b3b-46f8-a9bb-df16e1bd6e7a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T11:08:29.356590440Z",
     "start_time": "2023-08-25T11:08:27.601621321Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "abc\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, random_split\n",
    "import os \n",
    "from os.path import join \n",
    "from PIL import Image\n",
    "from tqdm import tqdm\n",
    "import logging\n",
    "import sys\n",
    "logger = logging.getLogger()\n",
    "logger.addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "device = \"cuda:0\"\n",
    "root = \"/home/ki/datasets/\"\n",
    "\n",
    "from detector import label_to_name, color_to_name\n",
    "\n",
    "class FruitDataset(Dataset):\n",
    "    \"\"\"\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    class_color_map = {\n",
    "        \"Apple Braeburn\": \"red\",\n",
    "        \"Apple Granny Smith\": \"green\",\n",
    "        \"Apricot\": \"orange\",\n",
    "        \"Avocado\": \"green\",\n",
    "        \"Banana\": \"yellow\",\n",
    "        \"Blueberry\": \"black\",\n",
    "        \"Cactus fruit\": \"green\",\n",
    "        \"Cantaloupe\": \"yellow\",\n",
    "        \"Cherry\": \"red\",\n",
    "        \"Clementine\": \"orange\",\n",
    "        \"Corn\": \"yellow\",\n",
    "        \"Cucumber Ripe\": \"brown\",\n",
    "        \"Grape Blue\": \"black\",\n",
    "        \"Kiwi\": \"brown\",\n",
    "        \"Lemon\": \"yellow\",\n",
    "        \"Limes\": \"green\",\n",
    "        \"Mango\": \"green\",\n",
    "        \"Onion White\": \"brown\",\n",
    "        \"Orange\": \"orange\",\n",
    "        \"Papaya\": \"green\",\n",
    "        \"Passion Fruit\": \"black\",\n",
    "        \"Peach\": \"orange\",\n",
    "        \"Pear\": \"green\", # ??\n",
    "        \"Pepper Green\": \"green\",\n",
    "        \"Pepper Red\": \"red\",\n",
    "        \"Pineapple\": \"brown\",\n",
    "        \"Plum\": \"red\",\n",
    "        \"Pomegranate\": \"red\",\n",
    "        \"Potato Red\": \"brown\",\n",
    "        \"Raspberry\": \"red\",\n",
    "        \"Strawberry\": \"red\",\n",
    "        \"Tomato\": \"red\",\n",
    "        \"Watermelon\": \"red\" \n",
    "    }\n",
    "    \n",
    "    def __init__(self, root=\"train\", transform=None, target_transform=None):\n",
    "        root = join(root, \"fruits\", \"train\", \"train\")\n",
    "\n",
    "        self.classes = os.listdir(root)\n",
    "        self.files = []\n",
    "        self.labels = []\n",
    "        self.colors = []\n",
    "        \n",
    "        self.transform = transform\n",
    "        self.target_transform = target_transform \n",
    "        \n",
    "        for c in self.classes:\n",
    "            fs = [join(root, c, f) for f in os.listdir(join(root, c))]\n",
    "            self.files += fs\n",
    "            self.labels += [c.lower().replace(\" \", \"_\")] * len(fs)\n",
    "            self.colors += [self.class_color_map[c]] * len(fs)\n",
    "\n",
    "        self.class_map = {c: n for n, c in enumerate(label_to_name)}\n",
    "        self.color_map = {c: n for n, c in enumerate(color_to_name)}\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.files)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.files[index]\n",
    "        y = self.class_map[self.labels[index]]\n",
    "        color = self.color_map[self.colors[index]]\n",
    "        \n",
    "        img = Image.open(img)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        y = torch.tensor([y, color]) \n",
    "        if self.target_transform is not None:\n",
    "            y = self.target_transform(y)\n",
    "        \n",
    "        return img, y \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "20c8d208-b5cc-40cd-95d1-984635599b8b",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T11:08:29.396313766Z",
     "start_time": "2023-08-25T11:08:29.357959508Z"
    }
   },
   "outputs": [],
   "source": [
    "ds = FruitDataset(root=root)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "20e713eb-873e-4c92-b9a5-6d5d3a069707",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T11:08:29.425297808Z",
     "start_time": "2023-08-25T11:08:29.399187816Z"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_ood.utils import ToRGB\n",
    "from torchvision.transforms import ToTensor, Resize, Compose\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "trans = Compose([ToRGB(), ToTensor(), Resize((32, 32), antialias=True)])\n",
    "\n",
    "data = FruitDataset(root=root, transform=trans)\n",
    "train_data, val_data, test_data = random_split(data, [14000,1000, 1854], generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81f40b8b-621b-47cd-bf1a-2ad43081edf8",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T11:08:29.428956953Z",
     "start_time": "2023-08-25T11:08:29.427729858Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from pytorch_ood.model import WideResNet\n",
    "\n",
    "# def override \n",
    "def Model(num_classes=None, *args, **kwargs):\n",
    "    model = WideResNet(*args, num_classes=1000, pretrained=\"imagenet32\", **kwargs)\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bd36c49f-3254-4808-a751-1c79b5c0d4a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T11:08:29.473226907Z",
     "start_time": "2023-08-25T11:08:29.432980337Z"
    }
   },
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "\n",
    "\n",
    "def train_model(att_index, num_classes):\n",
    "    \"\"\"\n",
    "    train a model for the given attribute index \n",
    "    \"\"\"\n",
    "    model = Model(num_classes=num_classes).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9, nesterov=True)\n",
    "\n",
    "    for epoch in range(5):\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        bar = tqdm(train_loader)\n",
    "        for inputs, y in bar:\n",
    "            labels = y[:, att_index]\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss = 0.8 * running_loss + 0.2 * loss.item()\n",
    "            bar.set_postfix({\"loss\": running_loss})\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            for inputs, y in test_loader:\n",
    "                labels = y[:, att_index]\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, dim=1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Accuracy of the network on the test images: {correct / total:.2%}')\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "from pytorch_ood.dataset.img import TinyImages300k\n",
    "from pytorch_ood.utils import is_known\n",
    "\n",
    "def train_fruit_model():\n",
    "    tiny = TinyImages300k(root=root, download=True, transform=trans, target_transform=ToUnknown())\n",
    "    data_train_out, data_test_out, _ = random_split(tiny, [50000, 10000, 240000], generator=torch.Generator().manual_seed(123))\n",
    "\n",
    "    data_noatt = FruitDataset(root=root, transform=trans, target_transform=lambda y: int(y[0]))\n",
    "    train_data_noatt, val_data_noatt, test_data_noatt = random_split(data_noatt, [14000,1000, 1854], generator=torch.Generator().manual_seed(0))\n",
    "\n",
    "    new_loader = DataLoader(train_data_noatt + data_train_out, batch_size=32, shuffle=True, num_workers=10)\n",
    "    new_test_loader = DataLoader(test_data_noatt + data_test_out, batch_size=32, shuffle=False, num_workers=10)\n",
    "\n",
    "    model = Model(num_classes=2).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9, nesterov=True)\n",
    "\n",
    "    accs = []\n",
    "\n",
    "    for epoch in range(10):\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "\n",
    "        bar = tqdm(new_loader)\n",
    "        for inputs, y in bar:\n",
    "            labels = is_known(y).long()\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss = 0.8 * running_loss + 0.2 * loss.item()\n",
    "            bar.set_postfix({\"loss\": running_loss})\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            for inputs, y in new_test_loader:\n",
    "                labels = is_known(y).long()\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Accuracy of the shape network on the test images: {correct / total:.2%}')\n",
    "        accs.append(correct / total)\n",
    "\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-08-25T11:08:29.473390249Z",
     "start_time": "2023-08-25T11:08:29.473098704Z"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "e1a02580-6d2c-4db2-a4b7-50c8dbbabf22",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-25T11:08:29.473499650Z",
     "start_time": "2023-08-25T11:08:29.473334282Z"
    }
   },
   "outputs": [],
   "source": [
    "from pytorch_ood.dataset.img import (LSUNCrop, LSUNResize, Textures, TinyImageNetCrop, TinyImageNetResize)\n",
    "from pytorch_ood.detector import EnergyBased, MaxSoftmax, MaxLogit, Entropy, Mahalanobis, ViM, ReAct\n",
    "from pytorch_ood.utils import OODMetrics, ToUnknown\n",
    "from detector import EnsembleDetector, PrologOOD, Prologic, PrologOODT\n",
    "\n",
    "def evaluate(label_net, color_net, fruit_net):\n",
    "    _ = label_net.eval()\n",
    "    _ = color_net.eval()\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    detectors = {\n",
    "        \"ViM\": ViM(label_net.features, w=label_net.fc.weight, b=label_net.fc.bias, d=64),\n",
    "        \"Mahalanobis\": Mahalanobis(label_net.features),\n",
    "        \"Entropy\": Entropy(label_net),\n",
    "        \"LogicOOD+\": PrologOOD(\"kb.pl\", label_net, color_net, fruit_net),\n",
    "        \"Logic\": Prologic(\"kb.pl\", label_net, color_net),\n",
    "        \"Logic+\": Prologic(\"kb.pl\", label_net, color_net, fruit_net),\n",
    "        \"LogicOOD\": PrologOOD(\"kb.pl\", label_net, color_net),\n",
    "        \"LogicOODT\": PrologOODT(\"kb.pl\", label_net, color_net),\n",
    "        \"LogicOODT+\": PrologOODT(\"kb.pl\", label_net, color_net, fruit_net),\n",
    "        # \"LogicT+\": PrologOODT(\"kb.pl\", label_net, color_net, fruit_net), # this should be exactly the same\n",
    "        \"Ensemble\": EnsembleDetector(label_net, color_net),\n",
    "        \"MSP\": MaxSoftmax(label_net),\n",
    "        \"ReAct\": ReAct(label_net.features, label_net.fc),\n",
    "        \"Energy\": EnergyBased(label_net),\n",
    "        \"MaxLogit\": MaxLogit(label_net),\n",
    "    }\n",
    "\n",
    "\n",
    "    data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
    "    _ , data_fit_label, _ = random_split(data_fit_label, [14000, 1000, 1854], generator=torch.Generator().manual_seed(0))\n",
    "    data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
    "    _, data_fit_color, _ = random_split(data_fit_color, [14000, 1000, 1854], generator=torch.Generator().manual_seed(0))\n",
    "    data_fit_color = DataLoader(data_fit_color, batch_size=32, shuffle=False, num_workers=2)\n",
    "    data_fit_label = DataLoader(data_fit_label, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "    data = FruitDataset(root=root, transform=trans, target_transform=lambda y: int(y[0]))\n",
    "    data_in_train, data_in_val, data_in = random_split(data, [14000, 1000, 1854], generator=torch.Generator().manual_seed(0))\n",
    "    train_in_loader = DataLoader(data_in_train, batch_size=32, shuffle=False, num_workers=2)\n",
    "\n",
    "    detectors[\"ViM\"].fit(train_in_loader, device=device)\n",
    "    detectors[\"LogicOODT\"].fit(data_fit_label, data_fit_color, device=device)\n",
    "    detectors[\"LogicOODT+\"].fit(data_fit_label, data_fit_color, device=device)\n",
    "    detectors[\"Mahalanobis\"].fit(train_in_loader, device=device)\n",
    "\n",
    "    datasets = {d.__name__: d for d in (LSUNCrop, LSUNResize, Textures, TinyImageNetCrop, TinyImageNetResize)}\n",
    "    \n",
    "    for detector_name, detector in detectors.items():\n",
    "        for data_name, dataset_c in datasets.items():\n",
    "            print(data_name)\n",
    "            data_out = dataset_c(root=root, transform=trans, target_transform=ToUnknown(), download=True)\n",
    "            loader = DataLoader(data_in+data_out, batch_size=256, shuffle=False, num_workers=12)\n",
    "            \n",
    "            scores = []\n",
    "            ys = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for x, y in loader:\n",
    "                    scores.append(detector(x.to(device)))\n",
    "                    ys.append(y.to(device))\n",
    "                    \n",
    "                scores = torch.cat(scores, dim=0).cpu()\n",
    "                ys = torch.cat(ys, dim=0).cpu()\n",
    "            \n",
    "            metrics = OODMetrics()\n",
    "            metrics.update(scores, ys)\n",
    "            r = metrics.compute()\n",
    "            r.update({\n",
    "                \"Method\": detector_name,\n",
    "                \"Dataset\": data_name\n",
    "            })\n",
    "            print(r)\n",
    "            results.append(r)\n",
    "    \n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "930a358f-c6ef-4fe9-9b1a-c305a37ffb70",
   "metadata": {
    "collapsed": true,
    "jupyter": {
     "outputs_hidden": true
    },
    "is_executing": true,
    "ExecuteTime": {
     "start_time": "2023-08-25T11:08:29.473439455Z"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 32.43it/s, loss=0.0528]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 96.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 31.76it/s, loss=0.0338]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 97.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:14<00:00, 30.54it/s, loss=0.00975]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:13<00:00, 31.85it/s, loss=0.02]   \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:10<00:00, 40.08it/s, loss=0.0072] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.00%\n",
      "color\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:10<00:00, 41.09it/s, loss=0.0596]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 97.30%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:10<00:00, 40.79it/s, loss=0.0285] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:10<00:00, 41.05it/s, loss=0.0554] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:10<00:00, 40.60it/s, loss=0.00361]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:11<00:00, 39.27it/s, loss=0.00353] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.00%\n",
      "fruit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:50<00:00, 39.32it/s, loss=0.00221] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.87%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:51<00:00, 38.64it/s, loss=0.00278] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:51<00:00, 39.02it/s, loss=0.00285] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:51<00:00, 39.06it/s, loss=0.00197] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:50<00:00, 39.35it/s, loss=0.000111]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:50<00:00, 39.26it/s, loss=0.000317]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:50<00:00, 39.32it/s, loss=3.31e-5] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:52<00:00, 37.90it/s, loss=0.000146]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:50<00:00, 39.30it/s, loss=0.000856]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:47<00:00, 41.91it/s, loss=0.000125]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.95%\n",
      "Computing principal space ...\n",
      "Computing alpha ...\n",
      "self.alpha=6.5486\n",
      "Fitting with temperature scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial T/NLL: 1.000/0.005\n",
      "Optimal temperature: 0.9005682468414307\n",
      "NLL after scaling: 0.00'\n",
      "Initial T/NLL: 1.000/0.005\n",
      "Optimal temperature: 0.8239295482635498\n",
      "NLL after scaling: 0.00'\n",
      "self.scorer_label.t=Parameter containing:\n",
      "tensor(0.9006, requires_grad=True)\n",
      "self.scorer_color.t=Parameter containing:\n",
      "tensor(0.8239, requires_grad=True)\n",
      "Fitting with temperature scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:84: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels).item()\n",
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:90: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels)\n",
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:97: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels).item()\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial T/NLL: 1.000/0.005\n",
      "Optimal temperature: 0.9005682468414307\n",
      "NLL after scaling: 0.00'\n",
      "Initial T/NLL: 1.000/0.005\n",
      "Optimal temperature: 0.8239295482635498\n",
      "NLL after scaling: 0.00'\n",
      "self.scorer_label.t=Parameter containing:\n",
      "tensor(0.9006, requires_grad=True)\n",
      "self.scorer_color.t=Parameter containing:\n",
      "tensor(0.8239, requires_grad=True)\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9825130701065063, 'AUPR-IN': 0.9968494176864624, 'AUPR-OUT': 0.9059414267539978, 'FPR95TPR': 0.11758360266685486, 'Method': 'ViM', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9987647533416748, 'AUPR-IN': 0.9997847080230713, 'AUPR-OUT': 0.9915980696678162, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9986789226531982, 'AUPR-IN': 0.9995987415313721, 'AUPR-OUT': 0.9952229857444763, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9901565909385681, 'AUPR-IN': 0.9982592463493347, 'AUPR-OUT': 0.9397937655448914, 'FPR95TPR': 0.02319309674203396, 'Method': 'ViM', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9973676800727844, 'AUPR-IN': 0.9995409846305847, 'AUPR-OUT': 0.9821252226829529, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9815995097160339, 'AUPR-IN': 0.996728777885437, 'AUPR-OUT': 0.8928722739219666, 'FPR95TPR': 0.10625674575567245, 'Method': 'Mahalanobis', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9984064102172852, 'AUPR-IN': 0.9997246265411377, 'AUPR-OUT': 0.9885809421539307, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9984838962554932, 'AUPR-IN': 0.9995514750480652, 'AUPR-OUT': 0.9935453534126282, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9876185655593872, 'AUPR-IN': 0.9978299140930176, 'AUPR-OUT': 0.9215142130851746, 'FPR95TPR': 0.026429342105984688, 'Method': 'Mahalanobis', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.99667888879776, 'AUPR-IN': 0.9994266033172607, 'AUPR-OUT': 0.9759668111801147, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9674727320671082, 'AUPR-IN': 0.9936689138412476, 'AUPR-OUT': 0.8815836310386658, 'FPR95TPR': 0.20334412157535553, 'Method': 'Entropy', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.985732913017273, 'AUPR-IN': 0.9971550107002258, 'AUPR-OUT': 0.9499620795249939, 'FPR95TPR': 0.07389428466558456, 'Method': 'Entropy', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9643287062644958, 'AUPR-IN': 0.9878503680229187, 'AUPR-OUT': 0.9051133394241333, 'FPR95TPR': 0.1677454113960266, 'Method': 'Entropy', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9800441265106201, 'AUPR-IN': 0.9960955381393433, 'AUPR-OUT': 0.9275656938552856, 'FPR95TPR': 0.12567421793937683, 'Method': 'Entropy', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.984431803226471, 'AUPR-IN': 0.996979296207428, 'AUPR-OUT': 0.937629222869873, 'FPR95TPR': 0.08414239436388016, 'Method': 'Entropy', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 0.9999998807907104, 'FPR95TPR': 0.0, 'Method': 'LogicOOD+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 0.9999998807907104, 'FPR95TPR': 0.0, 'Method': 'LogicOOD+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9999889135360718, 'AUPR-IN': 0.9999963045120239, 'AUPR-OUT': 0.9999661445617676, 'FPR95TPR': 0.0, 'Method': 'LogicOOD+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 0.9999998807907104, 'FPR95TPR': 0.0, 'Method': 'LogicOOD+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 0.9999998807907104, 'FPR95TPR': 0.0, 'Method': 'LogicOOD+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9131500124931335, 'AUPR-IN': 0.9864163994789124, 'AUPR-OUT': 0.7581453323364258, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.7202500104904175, 'AUPR-IN': 0.9562462568283081, 'AUPR-OUT': 0.6244462132453918, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.727482259273529, 'AUPR-IN': 0.932579755783081, 'AUPR-OUT': 0.6881088018417358, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.8156000375747681, 'AUPR-IN': 0.9711593389511108, 'AUPR-OUT': 0.6672681570053101, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.7159000039100647, 'AUPR-IN': 0.9555659294128418, 'AUPR-OUT': 0.6230095624923706, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'Logic+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'Logic+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9989361763000488, 'AUPR-IN': 0.9997367858886719, 'AUPR-OUT': 0.9967845678329468, 'FPR95TPR': 0.0, 'Method': 'Logic+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'Logic+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 1.0, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'Logic+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9968603849411011, 'AUPR-IN': 0.9994407296180725, 'AUPR-OUT': 0.9805757999420166, 'FPR95TPR': 0.0010787486098706722, 'Method': 'LogicOOD', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9917522072792053, 'AUPR-IN': 0.9984221458435059, 'AUPR-OUT': 0.9668079018592834, 'FPR95TPR': 0.051779936999082565, 'Method': 'LogicOOD', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9787079691886902, 'AUPR-IN': 0.9928972721099854, 'AUPR-OUT': 0.945083737373352, 'FPR95TPR': 0.13592232763767242, 'Method': 'LogicOOD', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.994592010974884, 'AUPR-IN': 0.9989982843399048, 'AUPR-OUT': 0.9737560749053955, 'FPR95TPR': 0.027508091181516647, 'Method': 'LogicOOD', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9900119304656982, 'AUPR-IN': 0.9981110095977783, 'AUPR-OUT': 0.9565088748931885, 'FPR95TPR': 0.06310679763555527, 'Method': 'LogicOOD', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9967723488807678, 'AUPR-IN': 0.9994237422943115, 'AUPR-OUT': 0.9801801443099976, 'FPR95TPR': 0.0026968715246766806, 'Method': 'LogicOODT', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9913895130157471, 'AUPR-IN': 0.998341977596283, 'AUPR-OUT': 0.9662520289421082, 'FPR95TPR': 0.0555555559694767, 'Method': 'LogicOODT', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9781743288040161, 'AUPR-IN': 0.992689847946167, 'AUPR-OUT': 0.9441509246826172, 'FPR95TPR': 0.1348435878753662, 'Method': 'LogicOODT', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9943832159042358, 'AUPR-IN': 0.9989546537399292, 'AUPR-OUT': 0.9734069108963013, 'FPR95TPR': 0.03236246109008789, 'Method': 'LogicOODT', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9898563623428345, 'AUPR-IN': 0.9980704188346863, 'AUPR-OUT': 0.9567227363586426, 'FPR95TPR': 0.06364616751670837, 'Method': 'LogicOODT', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9999999403953552, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'LogicOODT+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9999999403953552, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'LogicOODT+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9998267889022827, 'AUPR-IN': 0.9999502301216125, 'AUPR-OUT': 0.9992480278015137, 'FPR95TPR': 0.0, 'Method': 'LogicOODT+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9999999403953552, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'LogicOODT+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9999999403953552, 'AUPR-IN': 1.0, 'AUPR-OUT': 1.0, 'FPR95TPR': 0.0, 'Method': 'LogicOODT+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9925654530525208, 'AUPR-IN': 0.9985783100128174, 'AUPR-OUT': 0.9675961136817932, 'FPR95TPR': 0.03883495181798935, 'Method': 'Ensemble', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9890713095664978, 'AUPR-IN': 0.9978427886962891, 'AUPR-OUT': 0.9609915614128113, 'FPR95TPR': 0.06310679763555527, 'Method': 'Ensemble', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9751642346382141, 'AUPR-IN': 0.9914663434028625, 'AUPR-OUT': 0.9400582313537598, 'FPR95TPR': 0.14131607115268707, 'Method': 'Ensemble', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9913870692253113, 'AUPR-IN': 0.9983372092247009, 'AUPR-OUT': 0.9656950831413269, 'FPR95TPR': 0.04746494069695473, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9877513647079468, 'AUPR-IN': 0.9976216554641724, 'AUPR-OUT': 0.9519255757331848, 'FPR95TPR': 0.07335490733385086, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9640807509422302, 'AUPR-IN': 0.9927690625190735, 'AUPR-OUT': 0.8771347999572754, 'FPR95TPR': 0.20765911042690277, 'Method': 'MSP', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.983700156211853, 'AUPR-IN': 0.9966042041778564, 'AUPR-OUT': 0.9481009840965271, 'FPR95TPR': 0.07281553745269775, 'Method': 'MSP', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9625027775764465, 'AUPR-IN': 0.9868704676628113, 'AUPR-OUT': 0.9040136337280273, 'FPR95TPR': 0.16882416605949402, 'Method': 'MSP', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9779048562049866, 'AUPR-IN': 0.9955554008483887, 'AUPR-OUT': 0.9247105121612549, 'FPR95TPR': 0.12891046702861786, 'Method': 'MSP', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9826059341430664, 'AUPR-IN': 0.9965073466300964, 'AUPR-OUT': 0.9357378482818604, 'FPR95TPR': 0.08144552260637283, 'Method': 'MSP', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.8545701503753662, 'AUPR-IN': 0.9729675054550171, 'AUPR-OUT': 0.37869930267333984, 'FPR95TPR': 0.9352750778198242, 'Method': 'ReAct', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.975849449634552, 'AUPR-IN': 0.9958071112632751, 'AUPR-OUT': 0.8178211450576782, 'FPR95TPR': 0.07227616012096405, 'Method': 'ReAct', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.945449709892273, 'AUPR-IN': 0.9821736216545105, 'AUPR-OUT': 0.8106861114501953, 'FPR95TPR': 0.3101402521133423, 'Method': 'ReAct', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.8888193368911743, 'AUPR-IN': 0.9794808030128479, 'AUPR-OUT': 0.4656715989112854, 'FPR95TPR': 0.7567421793937683, 'Method': 'ReAct', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9638190865516663, 'AUPR-IN': 0.9936924576759338, 'AUPR-OUT': 0.7451083064079285, 'FPR95TPR': 0.2119741141796112, 'Method': 'ReAct', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9729676842689514, 'AUPR-IN': 0.994866669178009, 'AUPR-OUT': 0.8940696716308594, 'FPR95TPR': 0.2011866271495819, 'Method': 'Energy', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9859064221382141, 'AUPR-IN': 0.997281014919281, 'AUPR-OUT': 0.9460318088531494, 'FPR95TPR': 0.09277238696813583, 'Method': 'Energy', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9624386429786682, 'AUPR-IN': 0.9875468611717224, 'AUPR-OUT': 0.8966248631477356, 'FPR95TPR': 0.19147787988185883, 'Method': 'Energy', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9820839762687683, 'AUPR-IN': 0.9965829849243164, 'AUPR-OUT': 0.9296643733978271, 'FPR95TPR': 0.1332254558801651, 'Method': 'Energy', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.984984278678894, 'AUPR-IN': 0.9971588850021362, 'AUPR-OUT': 0.9358106851577759, 'FPR95TPR': 0.1003236249089241, 'Method': 'Energy', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9727380275726318, 'AUPR-IN': 0.9948129653930664, 'AUPR-OUT': 0.8937840461730957, 'FPR95TPR': 0.2011866271495819, 'Method': 'MaxLogit', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9857774972915649, 'AUPR-IN': 0.9972466826438904, 'AUPR-OUT': 0.946022629737854, 'FPR95TPR': 0.09331175684928894, 'Method': 'MaxLogit', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9624677300453186, 'AUPR-IN': 0.9875335693359375, 'AUPR-OUT': 0.8968165516853333, 'FPR95TPR': 0.19147787988185883, 'Method': 'MaxLogit', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.981909990310669, 'AUPR-IN': 0.9965418577194214, 'AUPR-OUT': 0.9294857382774353, 'FPR95TPR': 0.1332254558801651, 'Method': 'MaxLogit', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUROC': 0.9848415851593018, 'AUPR-IN': 0.9971235394477844, 'AUPR-OUT': 0.935732364654541, 'FPR95TPR': 0.09870550036430359, 'Method': 'MaxLogit', 'Dataset': 'TinyImageNetResize'}\n",
      "label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:11<00:00, 39.38it/s, loss=0.063] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 97.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:11<00:00, 39.47it/s, loss=0.0237]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 98.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:10<00:00, 40.16it/s, loss=0.0318] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:10<00:00, 40.91it/s, loss=0.00971]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:10<00:00, 40.86it/s, loss=0.00776]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.89%\n",
      "color\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:10<00:00, 40.00it/s, loss=0.102] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.62%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:10<00:00, 40.23it/s, loss=0.0658] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:11<00:00, 39.63it/s, loss=0.0092] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:10<00:00, 40.51it/s, loss=0.00382]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:10<00:00, 40.13it/s, loss=0.00739]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.00%\n",
      "fruit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:50<00:00, 39.55it/s, loss=0.00185] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.83%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:51<00:00, 39.12it/s, loss=0.00148] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:50<00:00, 39.55it/s, loss=0.0012]  \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.96%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:50<00:00, 39.51it/s, loss=0.0482]  \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.90%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:50<00:00, 39.79it/s, loss=0.000163]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:50<00:00, 39.58it/s, loss=0.000832]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:50<00:00, 39.78it/s, loss=0.000247]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:50<00:00, 39.51it/s, loss=0.000248]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:50<00:00, 39.62it/s, loss=0.000105]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:52<00:00, 38.32it/s, loss=0.00023] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.99%\n",
      "Computing principal space ...\n",
      "Computing alpha ...\n",
      "self.alpha=6.7176\n",
      "Fitting with temperature scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial T/NLL: 1.000/0.006\n",
      "Optimal temperature: 0.9064440131187439\n",
      "NLL after scaling: 0.00'\n",
      "Initial T/NLL: 1.000/0.004\n",
      "Optimal temperature: 0.8041974306106567\n",
      "NLL after scaling: 0.00'\n",
      "self.scorer_label.t=Parameter containing:\n",
      "tensor(0.9064, requires_grad=True)\n",
      "self.scorer_color.t=Parameter containing:\n",
      "tensor(0.8042, requires_grad=True)\n",
      "Fitting with temperature scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:84: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels).item()\n",
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:90: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels)\n",
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:97: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels).item()\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial T/NLL: 1.000/0.006\n",
      "Optimal temperature: 0.9064440131187439\n",
      "NLL after scaling: 0.00'\n",
      "Initial T/NLL: 1.000/0.004\n",
      "Optimal temperature: 0.8041974306106567\n",
      "NLL after scaling: 0.00'\n",
      "self.scorer_label.t=Parameter containing:\n",
      "tensor(0.9064, requires_grad=True)\n",
      "self.scorer_color.t=Parameter containing:\n",
      "tensor(0.8042, requires_grad=True)\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9923960566520691, 'AUPR-IN': 0.9986230134963989, 'AUPR-OUT': 0.959827184677124, 'FPR95TPR': 0.01995684951543808, 'Method': 'ViM', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9993399381637573, 'AUPR-IN': 0.9998793005943298, 'AUPR-OUT': 0.9976972341537476, 'FPR95TPR': 0.0005393743049353361, 'Method': 'ViM', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9993034601211548, 'AUPR-IN': 0.9997746348381042, 'AUPR-OUT': 0.9986166954040527, 'FPR95TPR': 0.0005393743049353361, 'Method': 'ViM', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9968504309654236, 'AUPR-IN': 0.9994328618049622, 'AUPR-OUT': 0.9829182624816895, 'FPR95TPR': 0.0005393743049353361, 'Method': 'ViM', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9989585876464844, 'AUPR-IN': 0.9998109340667725, 'AUPR-OUT': 0.9953781366348267, 'FPR95TPR': 0.0005393743049353361, 'Method': 'ViM', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9918364882469177, 'AUPR-IN': 0.9985387325286865, 'AUPR-OUT': 0.953925609588623, 'FPR95TPR': 0.026429342105984688, 'Method': 'Mahalanobis', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9996356964111328, 'AUPR-IN': 0.9999340772628784, 'AUPR-OUT': 0.9979041814804077, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9995419383049011, 'AUPR-IN': 0.9998554587364197, 'AUPR-OUT': 0.9985118508338928, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9966295957565308, 'AUPR-IN': 0.9994008541107178, 'AUPR-OUT': 0.9795832633972168, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9992035031318665, 'AUPR-IN': 0.9998571276664734, 'AUPR-OUT': 0.9953030347824097, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9747470021247864, 'AUPR-IN': 0.9950339794158936, 'AUPR-OUT': 0.899904191493988, 'FPR95TPR': 0.14778856933116913, 'Method': 'Entropy', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.983578085899353, 'AUPR-IN': 0.9966180920600891, 'AUPR-OUT': 0.9348055720329285, 'FPR95TPR': 0.06472492218017578, 'Method': 'Entropy', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9742631912231445, 'AUPR-IN': 0.9914057850837708, 'AUPR-OUT': 0.9210368394851685, 'FPR95TPR': 0.12998920679092407, 'Method': 'Entropy', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9666985869407654, 'AUPR-IN': 0.993402898311615, 'AUPR-OUT': 0.8528120517730713, 'FPR95TPR': 0.18446601927280426, 'Method': 'Entropy', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9796359539031982, 'AUPR-IN': 0.9958741664886475, 'AUPR-OUT': 0.9137300848960876, 'FPR95TPR': 0.08683926612138748, 'Method': 'Entropy', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9997302889823914, 'AUPR-IN': 0.9999499917030334, 'AUPR-OUT': 0.9997724294662476, 'FPR95TPR': 0.0005393743049353361, 'Method': 'LogicOOD+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9997302889823914, 'AUPR-IN': 0.9999499917030334, 'AUPR-OUT': 0.9997724294662476, 'FPR95TPR': 0.0005393743049353361, 'Method': 'LogicOOD+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9997302889823914, 'AUPR-IN': 0.999911367893219, 'AUPR-OUT': 0.999796986579895, 'FPR95TPR': 0.0005393743049353361, 'Method': 'LogicOOD+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9997302889823914, 'AUPR-IN': 0.9999499917030334, 'AUPR-OUT': 0.9997724294662476, 'FPR95TPR': 0.0005393743049353361, 'Method': 'LogicOOD+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9997302889823914, 'AUPR-IN': 0.9999499917030334, 'AUPR-OUT': 0.9997724294662476, 'FPR95TPR': 0.0005393743049353361, 'Method': 'LogicOOD+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.8356303572654724, 'AUPR-IN': 0.9742599129676819, 'AUPR-OUT': 0.6802009344100952, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.6892802715301514, 'AUPR-IN': 0.9513128995895386, 'AUPR-OUT': 0.6146944165229797, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.761255145072937, 'AUPR-IN': 0.9408323168754578, 'AUPR-OUT': 0.703737199306488, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.7538302540779114, 'AUPR-IN': 0.9614421129226685, 'AUPR-OUT': 0.6366060376167297, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.6797303557395935, 'AUPR-IN': 0.9498122334480286, 'AUPR-OUT': 0.6120347380638123, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9997303485870361, 'AUPR-IN': 0.9999499917030334, 'AUPR-OUT': 0.9997724890708923, 'FPR95TPR': 0.0005393743049353361, 'Method': 'Logic+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9997303485870361, 'AUPR-IN': 0.9999499917030334, 'AUPR-OUT': 0.9997724890708923, 'FPR95TPR': 0.0005393743049353361, 'Method': 'Logic+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9997303485870361, 'AUPR-IN': 0.999911367893219, 'AUPR-OUT': 0.9997970461845398, 'FPR95TPR': 0.0005393743049353361, 'Method': 'Logic+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9997303485870361, 'AUPR-IN': 0.9999499917030334, 'AUPR-OUT': 0.9997724890708923, 'FPR95TPR': 0.0005393743049353361, 'Method': 'Logic+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9997303485870361, 'AUPR-IN': 0.9999499917030334, 'AUPR-OUT': 0.9997724890708923, 'FPR95TPR': 0.0005393743049353361, 'Method': 'Logic+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9940077662467957, 'AUPR-IN': 0.998857855796814, 'AUPR-OUT': 0.9734165668487549, 'FPR95TPR': 0.030744336545467377, 'Method': 'LogicOOD', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9912724494934082, 'AUPR-IN': 0.9982013702392578, 'AUPR-OUT': 0.9709494113922119, 'FPR95TPR': 0.04584681615233421, 'Method': 'LogicOOD', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9866577386856079, 'AUPR-IN': 0.9955011010169983, 'AUPR-OUT': 0.9645038843154907, 'FPR95TPR': 0.07497303187847137, 'Method': 'LogicOOD', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9905992746353149, 'AUPR-IN': 0.9981548190116882, 'AUPR-OUT': 0.9625606536865234, 'FPR95TPR': 0.052858684211969376, 'Method': 'LogicOOD', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9896684885025024, 'AUPR-IN': 0.9978917241096497, 'AUPR-OUT': 0.9628026485443115, 'FPR95TPR': 0.052858684211969376, 'Method': 'LogicOOD', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.993518590927124, 'AUPR-IN': 0.9987669587135315, 'AUPR-OUT': 0.9711027145385742, 'FPR95TPR': 0.03182308375835419, 'Method': 'LogicOODT', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9909602999687195, 'AUPR-IN': 0.9981397390365601, 'AUPR-OUT': 0.9692773222923279, 'FPR95TPR': 0.0447680689394474, 'Method': 'LogicOODT', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9864653944969177, 'AUPR-IN': 0.9954426288604736, 'AUPR-OUT': 0.9630905389785767, 'FPR95TPR': 0.07119741290807724, 'Method': 'LogicOODT', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9900767803192139, 'AUPR-IN': 0.998055636882782, 'AUPR-OUT': 0.9599241018295288, 'FPR95TPR': 0.051779936999082565, 'Method': 'LogicOODT', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9895191788673401, 'AUPR-IN': 0.9978557229042053, 'AUPR-OUT': 0.9626749753952026, 'FPR95TPR': 0.051779936999082565, 'Method': 'LogicOODT', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9997303485870361, 'AUPR-IN': 0.9999499917030334, 'AUPR-OUT': 0.9997724294662476, 'FPR95TPR': 0.0005393743049353361, 'Method': 'LogicOODT+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9997303485870361, 'AUPR-IN': 0.9999499917030334, 'AUPR-OUT': 0.9997724294662476, 'FPR95TPR': 0.0005393743049353361, 'Method': 'LogicOODT+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9997303485870361, 'AUPR-IN': 0.999911367893219, 'AUPR-OUT': 0.999796986579895, 'FPR95TPR': 0.0005393743049353361, 'Method': 'LogicOODT+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9997303485870361, 'AUPR-IN': 0.9999499917030334, 'AUPR-OUT': 0.9997724294662476, 'FPR95TPR': 0.0005393743049353361, 'Method': 'LogicOODT+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9997303485870361, 'AUPR-IN': 0.9999499917030334, 'AUPR-OUT': 0.9997724294662476, 'FPR95TPR': 0.0005393743049353361, 'Method': 'LogicOODT+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9908510446548462, 'AUPR-IN': 0.998193621635437, 'AUPR-OUT': 0.9654209017753601, 'FPR95TPR': 0.0447680689394474, 'Method': 'Ensemble', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9886658191680908, 'AUPR-IN': 0.997570276260376, 'AUPR-OUT': 0.9669122695922852, 'FPR95TPR': 0.05339805781841278, 'Method': 'Ensemble', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9818043112754822, 'AUPR-IN': 0.9936215877532959, 'AUPR-OUT': 0.9555209875106812, 'FPR95TPR': 0.08306364715099335, 'Method': 'Ensemble', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9847638010978699, 'AUPR-IN': 0.9968876242637634, 'AUPR-OUT': 0.9467729330062866, 'FPR95TPR': 0.07389428466558456, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9874414205551147, 'AUPR-IN': 0.9973782896995544, 'AUPR-OUT': 0.9582844376564026, 'FPR95TPR': 0.058252427726984024, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9747472405433655, 'AUPR-IN': 0.994995653629303, 'AUPR-OUT': 0.9002594947814941, 'FPR95TPR': 0.14886730909347534, 'Method': 'MSP', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9832380414009094, 'AUPR-IN': 0.9964962005615234, 'AUPR-OUT': 0.9340710639953613, 'FPR95TPR': 0.06580366939306259, 'Method': 'MSP', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9739446640014648, 'AUPR-IN': 0.9911920428276062, 'AUPR-OUT': 0.9209256172180176, 'FPR95TPR': 0.1321467161178589, 'Method': 'MSP', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.967112123966217, 'AUPR-IN': 0.9934481382369995, 'AUPR-OUT': 0.8557218909263611, 'FPR95TPR': 0.18608413636684418, 'Method': 'MSP', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9796313643455505, 'AUPR-IN': 0.9958281517028809, 'AUPR-OUT': 0.9143819212913513, 'FPR95TPR': 0.08468177169561386, 'Method': 'MSP', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9058095216751099, 'AUPR-IN': 0.979292094707489, 'AUPR-OUT': 0.6879560351371765, 'FPR95TPR': 0.45253506302833557, 'Method': 'ReAct', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.7542913556098938, 'AUPR-IN': 0.9277681112289429, 'AUPR-OUT': 0.3907785713672638, 'FPR95TPR': 0.7820927500724792, 'Method': 'ReAct', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9243096709251404, 'AUPR-IN': 0.967918336391449, 'AUPR-OUT': 0.818659782409668, 'FPR95TPR': 0.3651564121246338, 'Method': 'ReAct', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.8067100048065186, 'AUPR-IN': 0.9475950002670288, 'AUPR-OUT': 0.5123514533042908, 'FPR95TPR': 0.6240561008453369, 'Method': 'ReAct', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.7524113059043884, 'AUPR-IN': 0.9311057329177856, 'AUPR-OUT': 0.3855225443840027, 'FPR95TPR': 0.7723840475082397, 'Method': 'ReAct', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9650726914405823, 'AUPR-IN': 0.9927980899810791, 'AUPR-OUT': 0.8614498376846313, 'FPR95TPR': 0.15210355818271637, 'Method': 'Energy', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9703304767608643, 'AUPR-IN': 0.9926396608352661, 'AUPR-OUT': 0.8929729461669922, 'FPR95TPR': 0.11434735357761383, 'Method': 'Energy', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9612122774124146, 'AUPR-IN': 0.986217737197876, 'AUPR-OUT': 0.883330225944519, 'FPR95TPR': 0.188241645693779, 'Method': 'Energy', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.943341076374054, 'AUPR-IN': 0.988086462020874, 'AUPR-OUT': 0.7519118189811707, 'FPR95TPR': 0.30636462569236755, 'Method': 'Energy', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9610955119132996, 'AUPR-IN': 0.9909869432449341, 'AUPR-OUT': 0.8439159989356995, 'FPR95TPR': 0.16235166788101196, 'Method': 'Energy', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9661747813224792, 'AUPR-IN': 0.9931734204292297, 'AUPR-OUT': 0.8625869750976562, 'FPR95TPR': 0.15210355818271637, 'Method': 'MaxLogit', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9722139835357666, 'AUPR-IN': 0.9936555624008179, 'AUPR-OUT': 0.8944400548934937, 'FPR95TPR': 0.11434735357761383, 'Method': 'MaxLogit', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9625276327133179, 'AUPR-IN': 0.9871344566345215, 'AUPR-OUT': 0.8845034837722778, 'FPR95TPR': 0.18500539660453796, 'Method': 'MaxLogit', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9450553059577942, 'AUPR-IN': 0.9887828826904297, 'AUPR-OUT': 0.7535699605941772, 'FPR95TPR': 0.30420711636543274, 'Method': 'MaxLogit', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUROC': 0.9631373286247253, 'AUPR-IN': 0.9919742941856384, 'AUPR-OUT': 0.8457326292991638, 'FPR95TPR': 0.16127292811870575, 'Method': 'MaxLogit', 'Dataset': 'TinyImageNetResize'}\n",
      "label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:10<00:00, 40.27it/s, loss=0.0687]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 93.74%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:10<00:00, 40.33it/s, loss=0.0302]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 97.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:10<00:00, 41.07it/s, loss=0.0206] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:10<00:00, 40.00it/s, loss=0.0112] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 97.36%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:10<00:00, 40.15it/s, loss=0.0103] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.00%\n",
      "color\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:10<00:00, 39.98it/s, loss=0.0998]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 86.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:10<00:00, 39.97it/s, loss=0.00461]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.68%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:10<00:00, 40.26it/s, loss=0.0148] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 98.92%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:10<00:00, 40.20it/s, loss=0.0175] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.46%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:10<00:00, 40.43it/s, loss=0.0045]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.95%\n",
      "fruit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:49<00:00, 40.17it/s, loss=0.00304] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.91%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:49<00:00, 40.02it/s, loss=0.000825]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:50<00:00, 40.00it/s, loss=0.000311]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:49<00:00, 40.01it/s, loss=0.000209]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:49<00:00, 40.22it/s, loss=0.00162] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:50<00:00, 39.98it/s, loss=0.000164]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:51<00:00, 39.02it/s, loss=5.53e-5] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:52<00:00, 37.74it/s, loss=1.74e-5] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:53<00:00, 37.63it/s, loss=0.000162]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:51<00:00, 38.67it/s, loss=0.000102]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.98%\n",
      "Computing principal space ...\n",
      "Computing alpha ...\n",
      "self.alpha=6.8942\n",
      "Fitting with temperature scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial T/NLL: 1.000/0.002\n",
      "Optimal temperature: 0.9354942440986633\n",
      "NLL after scaling: 0.00'\n",
      "Initial T/NLL: 1.000/0.007\n",
      "Optimal temperature: 0.7720860838890076\n",
      "NLL after scaling: 0.00'\n",
      "self.scorer_label.t=Parameter containing:\n",
      "tensor(0.9355, requires_grad=True)\n",
      "self.scorer_color.t=Parameter containing:\n",
      "tensor(0.7721, requires_grad=True)\n",
      "Fitting with temperature scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:84: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels).item()\n",
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:90: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels)\n",
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:97: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels).item()\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial T/NLL: 1.000/0.002\n",
      "Optimal temperature: 0.9354942440986633\n",
      "NLL after scaling: 0.00'\n",
      "Initial T/NLL: 1.000/0.007\n",
      "Optimal temperature: 0.7720860838890076\n",
      "NLL after scaling: 0.00'\n",
      "self.scorer_label.t=Parameter containing:\n",
      "tensor(0.9355, requires_grad=True)\n",
      "self.scorer_color.t=Parameter containing:\n",
      "tensor(0.7721, requires_grad=True)\n",
      "LSUNCrop\n",
      "{'AUROC': 0.999791145324707, 'AUPR-IN': 0.9999614953994751, 'AUPR-OUT': 0.9988881349563599, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9999620914459229, 'AUPR-IN': 0.9999929070472717, 'AUPR-OUT': 0.9997984170913696, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9999872446060181, 'AUPR-IN': 0.999995768070221, 'AUPR-OUT': 0.9999610781669617, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9998794198036194, 'AUPR-IN': 0.9999778270721436, 'AUPR-OUT': 0.9993565082550049, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9999251365661621, 'AUPR-IN': 0.9999860525131226, 'AUPR-OUT': 0.9996023178100586, 'FPR95TPR': 0.0, 'Method': 'ViM', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9997307658195496, 'AUPR-IN': 0.9999505877494812, 'AUPR-OUT': 0.9985291361808777, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9999679923057556, 'AUPR-IN': 0.9999940395355225, 'AUPR-OUT': 0.999829888343811, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9999532103538513, 'AUPR-IN': 0.9999846816062927, 'AUPR-OUT': 0.9998577237129211, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9998274445533752, 'AUPR-IN': 0.9999682307243347, 'AUPR-OUT': 0.9990695118904114, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9999186992645264, 'AUPR-IN': 0.9999850392341614, 'AUPR-OUT': 0.9995560646057129, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9853252172470093, 'AUPR-IN': 0.9974184036254883, 'AUPR-OUT': 0.8966119289398193, 'FPR95TPR': 0.041531823575496674, 'Method': 'Entropy', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9858852624893188, 'AUPR-IN': 0.9975245594978333, 'AUPR-OUT': 0.8996875882148743, 'FPR95TPR': 0.03559870645403862, 'Method': 'Entropy', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9742988348007202, 'AUPR-IN': 0.9923368096351624, 'AUPR-OUT': 0.8883622884750366, 'FPR95TPR': 0.11003236472606659, 'Method': 'Entropy', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9890503287315369, 'AUPR-IN': 0.9980615377426147, 'AUPR-OUT': 0.9271358847618103, 'FPR95TPR': 0.026968715712428093, 'Method': 'Entropy', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9855287075042725, 'AUPR-IN': 0.9974182844161987, 'AUPR-OUT': 0.907067060470581, 'FPR95TPR': 0.044228695333004, 'Method': 'Entropy', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9997302889823914, 'AUPR-IN': 0.9999499917030334, 'AUPR-OUT': 0.9997725486755371, 'FPR95TPR': 0.0005393743049353361, 'Method': 'LogicOOD+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9997302889823914, 'AUPR-IN': 0.9999499917030334, 'AUPR-OUT': 0.9997725486755371, 'FPR95TPR': 0.0005393743049353361, 'Method': 'LogicOOD+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9997241497039795, 'AUPR-IN': 0.9999092817306519, 'AUPR-OUT': 0.9997780919075012, 'FPR95TPR': 0.0005393743049353361, 'Method': 'LogicOOD+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9997302889823914, 'AUPR-IN': 0.9999499917030334, 'AUPR-OUT': 0.9997725486755371, 'FPR95TPR': 0.0005393743049353361, 'Method': 'LogicOOD+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9997302889823914, 'AUPR-IN': 0.9999499917030334, 'AUPR-OUT': 0.9997725486755371, 'FPR95TPR': 0.0005393743049353361, 'Method': 'LogicOOD+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.765630304813385, 'AUPR-IN': 0.9632920622825623, 'AUPR-OUT': 0.6415476202964783, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.688330352306366, 'AUPR-IN': 0.9511635899543762, 'AUPR-OUT': 0.6144241690635681, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.7021239995956421, 'AUPR-IN': 0.926153838634491, 'AUPR-OUT': 0.6776281595230103, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.705430269241333, 'AUPR-IN': 0.9538491368293762, 'AUPR-OUT': 0.6194908022880554, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.6726303696632385, 'AUPR-IN': 0.9486960172653198, 'AUPR-OUT': 0.6101357936859131, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9997303485870361, 'AUPR-IN': 0.9999499917030334, 'AUPR-OUT': 0.9997724890708923, 'FPR95TPR': 0.0005393743049353361, 'Method': 'Logic+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9997303485870361, 'AUPR-IN': 0.9999499917030334, 'AUPR-OUT': 0.9997724890708923, 'FPR95TPR': 0.0005393743049353361, 'Method': 'Logic+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9994643330574036, 'AUPR-IN': 0.9998455047607422, 'AUPR-OUT': 0.9989888668060303, 'FPR95TPR': 0.0005393743049353361, 'Method': 'Logic+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9997303485870361, 'AUPR-IN': 0.9999499917030334, 'AUPR-OUT': 0.9997724890708923, 'FPR95TPR': 0.0005393743049353361, 'Method': 'Logic+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9997303485870361, 'AUPR-IN': 0.9999499917030334, 'AUPR-OUT': 0.9997724890708923, 'FPR95TPR': 0.0005393743049353361, 'Method': 'Logic+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9913783073425293, 'AUPR-IN': 0.9983565211296082, 'AUPR-OUT': 0.9556803703308105, 'FPR95TPR': 0.03991369903087616, 'Method': 'LogicOOD', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9909151792526245, 'AUPR-IN': 0.9981525540351868, 'AUPR-OUT': 0.965794563293457, 'FPR95TPR': 0.04368932172656059, 'Method': 'LogicOOD', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9731093645095825, 'AUPR-IN': 0.9912447929382324, 'AUPR-OUT': 0.9109411835670471, 'FPR95TPR': 0.13592232763767242, 'Method': 'LogicOOD', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9909072518348694, 'AUPR-IN': 0.9981896281242371, 'AUPR-OUT': 0.9632198214530945, 'FPR95TPR': 0.04638619348406792, 'Method': 'LogicOOD', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9881672263145447, 'AUPR-IN': 0.9976276755332947, 'AUPR-OUT': 0.9513640999794006, 'FPR95TPR': 0.058252427726984024, 'Method': 'LogicOOD', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9911882877349854, 'AUPR-IN': 0.9983305335044861, 'AUPR-OUT': 0.9515064358711243, 'FPR95TPR': 0.03775620460510254, 'Method': 'LogicOODT', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9906392097473145, 'AUPR-IN': 0.9981083273887634, 'AUPR-OUT': 0.962016224861145, 'FPR95TPR': 0.041531823575496674, 'Method': 'LogicOODT', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9748841524124146, 'AUPR-IN': 0.9918631911277771, 'AUPR-OUT': 0.9115769267082214, 'FPR95TPR': 0.11434735357761383, 'Method': 'LogicOODT', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9913185238838196, 'AUPR-IN': 0.9982685446739197, 'AUPR-OUT': 0.9639074802398682, 'FPR95TPR': 0.041531823575496674, 'Method': 'LogicOODT', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9885922074317932, 'AUPR-IN': 0.9977054595947266, 'AUPR-OUT': 0.9522445201873779, 'FPR95TPR': 0.05070118606090546, 'Method': 'LogicOODT', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9997303485870361, 'AUPR-IN': 0.9999499917030334, 'AUPR-OUT': 0.9997724890708923, 'FPR95TPR': 0.0005393743049353361, 'Method': 'LogicOODT+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9997303485870361, 'AUPR-IN': 0.9999499917030334, 'AUPR-OUT': 0.9997724890708923, 'FPR95TPR': 0.0005393743049353361, 'Method': 'LogicOODT+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9996730089187622, 'AUPR-IN': 0.9998941421508789, 'AUPR-OUT': 0.9995901584625244, 'FPR95TPR': 0.0005393743049353361, 'Method': 'LogicOODT+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9997303485870361, 'AUPR-IN': 0.9999499917030334, 'AUPR-OUT': 0.9997724890708923, 'FPR95TPR': 0.0005393743049353361, 'Method': 'LogicOODT+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9997303485870361, 'AUPR-IN': 0.9999499917030334, 'AUPR-OUT': 0.9997724890708923, 'FPR95TPR': 0.0005393743049353361, 'Method': 'LogicOODT+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9888311624526978, 'AUPR-IN': 0.9977959394454956, 'AUPR-OUT': 0.9505391120910645, 'FPR95TPR': 0.052858684211969376, 'Method': 'Ensemble', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9900140166282654, 'AUPR-IN': 0.997948169708252, 'AUPR-OUT': 0.9645463824272156, 'FPR95TPR': 0.04692556709051132, 'Method': 'Ensemble', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9710429310798645, 'AUPR-IN': 0.9903506636619568, 'AUPR-OUT': 0.9084124565124512, 'FPR95TPR': 0.13807982206344604, 'Method': 'Ensemble', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9896103143692017, 'AUPR-IN': 0.9979158043861389, 'AUPR-OUT': 0.9607468843460083, 'FPR95TPR': 0.05070118606090546, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9870834946632385, 'AUPR-IN': 0.9973899126052856, 'AUPR-OUT': 0.9496062994003296, 'FPR95TPR': 0.05987054854631424, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9851579666137695, 'AUPR-IN': 0.9973614811897278, 'AUPR-OUT': 0.8986255526542664, 'FPR95TPR': 0.04314994439482689, 'Method': 'MSP', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.985785722732544, 'AUPR-IN': 0.9974768757820129, 'AUPR-OUT': 0.9023905992507935, 'FPR95TPR': 0.03667745366692543, 'Method': 'MSP', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9742448329925537, 'AUPR-IN': 0.9922361969947815, 'AUPR-OUT': 0.8901723623275757, 'FPR95TPR': 0.10787486284971237, 'Method': 'MSP', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9888096451759338, 'AUPR-IN': 0.9979956150054932, 'AUPR-OUT': 0.9283661842346191, 'FPR95TPR': 0.028586838394403458, 'Method': 'MSP', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9854699373245239, 'AUPR-IN': 0.9973766803741455, 'AUPR-OUT': 0.9098662734031677, 'FPR95TPR': 0.04530744254589081, 'Method': 'MSP', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.921425461769104, 'AUPR-IN': 0.9828762412071228, 'AUPR-OUT': 0.7635621428489685, 'FPR95TPR': 0.323085218667984, 'Method': 'ReAct', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9557569622993469, 'AUPR-IN': 0.9913133382797241, 'AUPR-OUT': 0.8329508900642395, 'FPR95TPR': 0.23570658266544342, 'Method': 'ReAct', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9432859420776367, 'AUPR-IN': 0.9825238585472107, 'AUPR-OUT': 0.787353515625, 'FPR95TPR': 0.3748651444911957, 'Method': 'ReAct', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9322665333747864, 'AUPR-IN': 0.9856085777282715, 'AUPR-OUT': 0.7972071170806885, 'FPR95TPR': 0.29557713866233826, 'Method': 'ReAct', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9487794041633606, 'AUPR-IN': 0.9897096157073975, 'AUPR-OUT': 0.8218850493431091, 'FPR95TPR': 0.2664509117603302, 'Method': 'ReAct', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9824415445327759, 'AUPR-IN': 0.9969735145568848, 'AUPR-OUT': 0.8700709939002991, 'FPR95TPR': 0.04854368790984154, 'Method': 'Energy', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9812840819358826, 'AUPR-IN': 0.9967759251594543, 'AUPR-OUT': 0.8616660833358765, 'FPR95TPR': 0.05070118606090546, 'Method': 'Energy', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9697374105453491, 'AUPR-IN': 0.9912574291229248, 'AUPR-OUT': 0.8637198209762573, 'FPR95TPR': 0.18338726460933685, 'Method': 'Energy', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9873127937316895, 'AUPR-IN': 0.9978041648864746, 'AUPR-OUT': 0.9093414545059204, 'FPR95TPR': 0.022653721272945404, 'Method': 'Energy', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9801822900772095, 'AUPR-IN': 0.9965358376502991, 'AUPR-OUT': 0.8673598170280457, 'FPR95TPR': 0.09277238696813583, 'Method': 'Energy', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9824349284172058, 'AUPR-IN': 0.9969719648361206, 'AUPR-OUT': 0.8700858950614929, 'FPR95TPR': 0.04800431430339813, 'Method': 'MaxLogit', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9812957644462585, 'AUPR-IN': 0.9967778921127319, 'AUPR-OUT': 0.8617702722549438, 'FPR95TPR': 0.05124055966734886, 'Method': 'MaxLogit', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.969746470451355, 'AUPR-IN': 0.9912585020065308, 'AUPR-OUT': 0.8637837171554565, 'FPR95TPR': 0.18338726460933685, 'Method': 'MaxLogit', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9873051643371582, 'AUPR-IN': 0.9978025555610657, 'AUPR-OUT': 0.909339427947998, 'FPR95TPR': 0.022114347666502, 'Method': 'MaxLogit', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUROC': 0.9801960587501526, 'AUPR-IN': 0.996537983417511, 'AUPR-OUT': 0.8674206733703613, 'FPR95TPR': 0.09223300963640213, 'Method': 'MaxLogit', 'Dataset': 'TinyImageNetResize'}\n",
      "label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:11<00:00, 39.65it/s, loss=0.167] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 85.49%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:10<00:00, 40.17it/s, loss=0.0377]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 95.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:10<00:00, 39.90it/s, loss=0.0395] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 98.98%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:11<00:00, 39.47it/s, loss=0.00905]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.51%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:11<00:00, 38.62it/s, loss=0.0115] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.00%\n",
      "color\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:11<00:00, 38.21it/s, loss=0.0253]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 96.60%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:10<00:00, 40.22it/s, loss=0.0206] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 95.31%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:10<00:00, 40.03it/s, loss=0.00408]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 98.27%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:11<00:00, 39.52it/s, loss=0.00397]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.84%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:11<00:00, 39.15it/s, loss=0.0028] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.78%\n",
      "fruit\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:50<00:00, 39.48it/s, loss=0.00852] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.93%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:51<00:00, 39.09it/s, loss=0.00102] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.95%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:50<00:00, 39.76it/s, loss=0.00129] \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.94%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:50<00:00, 39.91it/s, loss=0.000238]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:50<00:00, 39.56it/s, loss=0.000631]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:49<00:00, 40.12it/s, loss=0.000886]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.97%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:50<00:00, 39.80it/s, loss=7.2e-5]  \n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:51<00:00, 38.49it/s, loss=0.000285]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:51<00:00, 39.09it/s, loss=0.000161]\n",
      "  0%|          | 0/2000 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.99%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2000/2000 [00:51<00:00, 38.58it/s, loss=0.000294]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.98%\n",
      "Computing principal space ...\n",
      "Computing alpha ...\n",
      "self.alpha=6.8563\n",
      "Fitting with temperature scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial T/NLL: 1.000/0.004\n",
      "Optimal temperature: 0.9130489826202393\n",
      "NLL after scaling: 0.00'\n",
      "Initial T/NLL: 1.000/0.006\n",
      "Optimal temperature: 0.8459239602088928\n",
      "NLL after scaling: 0.00'\n",
      "self.scorer_label.t=Parameter containing:\n",
      "tensor(0.9130, requires_grad=True)\n",
      "self.scorer_color.t=Parameter containing:\n",
      "tensor(0.8459, requires_grad=True)\n",
      "Fitting with temperature scaling\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:84: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels).item()\n",
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:90: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels)\n",
      "/home/ki/projects/pytorch-ood/src/pytorch_ood/detector/tscaling.py:97: UserWarning: Implicit dimension choice for log_softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  loss = nll_loss(log_softmax(logits / self.t), labels).item()\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:31: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_label = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[0]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n",
      "<ipython-input-7-064b38277f51>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  data_fit_color = FruitDataset(root=root, transform=trans,  target_transform=lambda y: torch.tensor(y[1]))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial T/NLL: 1.000/0.004\n",
      "Optimal temperature: 0.9130489826202393\n",
      "NLL after scaling: 0.00'\n",
      "Initial T/NLL: 1.000/0.006\n",
      "Optimal temperature: 0.8459239602088928\n",
      "NLL after scaling: 0.00'\n",
      "self.scorer_label.t=Parameter containing:\n",
      "tensor(0.9130, requires_grad=True)\n",
      "self.scorer_color.t=Parameter containing:\n",
      "tensor(0.8459, requires_grad=True)\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9984092712402344, 'AUPR-IN': 0.9996959567070007, 'AUPR-OUT': 0.9953922629356384, 'FPR95TPR': 0.0026968715246766806, 'Method': 'ViM', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9991886615753174, 'AUPR-IN': 0.9998494982719421, 'AUPR-OUT': 0.9999563694000244, 'FPR95TPR': 0.0016181230312213302, 'Method': 'ViM', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.998826801776886, 'AUPR-IN': 0.9996160268783569, 'AUPR-OUT': 0.9987301230430603, 'FPR95TPR': 0.0016181230312213302, 'Method': 'ViM', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9990221261978149, 'AUPR-IN': 0.9998140335083008, 'AUPR-OUT': 0.9988125562667847, 'FPR95TPR': 0.0016181230312213302, 'Method': 'ViM', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9991782903671265, 'AUPR-IN': 0.9998472929000854, 'AUPR-OUT': 0.9998745322227478, 'FPR95TPR': 0.0016181230312213302, 'Method': 'ViM', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9945331811904907, 'AUPR-IN': 0.9990181922912598, 'AUPR-OUT': 0.9689914584159851, 'FPR95TPR': 0.007011866196990013, 'Method': 'Mahalanobis', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9999480247497559, 'AUPR-IN': 0.9999904632568359, 'AUPR-OUT': 0.9997096061706543, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.998138964176178, 'AUPR-IN': 0.9994365572929382, 'AUPR-OUT': 0.9932758808135986, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9985579252243042, 'AUPR-IN': 0.999740481376648, 'AUPR-OUT': 0.9916459918022156, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9998621940612793, 'AUPR-IN': 0.9999752044677734, 'AUPR-OUT': 0.9991927146911621, 'FPR95TPR': 0.0, 'Method': 'Mahalanobis', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9764463305473328, 'AUPR-IN': 0.9956953525543213, 'AUPR-OUT': 0.8795181512832642, 'FPR95TPR': 0.12405609339475632, 'Method': 'Entropy', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.969814121723175, 'AUPR-IN': 0.9943948984146118, 'AUPR-OUT': 0.8471052646636963, 'FPR95TPR': 0.16990290582180023, 'Method': 'Entropy', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.946343183517456, 'AUPR-IN': 0.983466625213623, 'AUPR-OUT': 0.795620858669281, 'FPR95TPR': 0.4142394959926605, 'Method': 'Entropy', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9826685190200806, 'AUPR-IN': 0.9967938661575317, 'AUPR-OUT': 0.9122681617736816, 'FPR95TPR': 0.0782092735171318, 'Method': 'Entropy', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9757599234580994, 'AUPR-IN': 0.9955522418022156, 'AUPR-OUT': 0.8676499128341675, 'FPR95TPR': 0.11272923648357391, 'Method': 'Entropy', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.998921275138855, 'AUPR-IN': 0.9998000860214233, 'AUPR-OUT': 0.9990900158882141, 'FPR95TPR': 0.0021574972197413445, 'Method': 'LogicOOD+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.998921275138855, 'AUPR-IN': 0.9998000860214233, 'AUPR-OUT': 0.9990900158882141, 'FPR95TPR': 0.0021574972197413445, 'Method': 'LogicOOD+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9989033341407776, 'AUPR-IN': 0.999639093875885, 'AUPR-OUT': 0.9991316199302673, 'FPR95TPR': 0.0021574972197413445, 'Method': 'LogicOOD+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.998921275138855, 'AUPR-IN': 0.9998000860214233, 'AUPR-OUT': 0.9990900158882141, 'FPR95TPR': 0.0021574972197413445, 'Method': 'LogicOOD+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.998921275138855, 'AUPR-IN': 0.9998000860214233, 'AUPR-OUT': 0.9990900158882141, 'FPR95TPR': 0.0021574972197413445, 'Method': 'LogicOOD+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.7552712559700012, 'AUPR-IN': 0.9615026712417603, 'AUPR-OUT': 0.6366773247718811, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.7998212575912476, 'AUPR-IN': 0.9685280919075012, 'AUPR-OUT': 0.6576976180076599, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.7020241022109985, 'AUPR-IN': 0.9256768822669983, 'AUPR-OUT': 0.6771069765090942, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.7078711986541748, 'AUPR-IN': 0.9540008306503296, 'AUPR-OUT': 0.6196739673614502, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.7776212096214294, 'AUPR-IN': 0.9650294780731201, 'AUPR-OUT': 0.6464768648147583, 'FPR95TPR': 1.0, 'Method': 'Logic', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9989212155342102, 'AUPR-IN': 0.9998000860214233, 'AUPR-OUT': 0.9990899562835693, 'FPR95TPR': 0.0021574972197413445, 'Method': 'Logic+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9989212155342102, 'AUPR-IN': 0.9998000860214233, 'AUPR-OUT': 0.9990899562835693, 'FPR95TPR': 0.0021574972197413445, 'Method': 'Logic+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9976800680160522, 'AUPR-IN': 0.9993377327919006, 'AUPR-OUT': 0.9954327940940857, 'FPR95TPR': 0.0021574972197413445, 'Method': 'Logic+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9989212155342102, 'AUPR-IN': 0.9998000860214233, 'AUPR-OUT': 0.9990899562835693, 'FPR95TPR': 0.0021574972197413445, 'Method': 'Logic+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9989212155342102, 'AUPR-IN': 0.9998000860214233, 'AUPR-OUT': 0.9990899562835693, 'FPR95TPR': 0.0021574972197413445, 'Method': 'Logic+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9740579128265381, 'AUPR-IN': 0.9950850009918213, 'AUPR-OUT': 0.8742657899856567, 'FPR95TPR': 0.13807982206344604, 'Method': 'LogicOOD', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9941191673278809, 'AUPR-IN': 0.9987936019897461, 'AUPR-OUT': 0.9750858545303345, 'FPR95TPR': 0.022114347666502, 'Method': 'LogicOOD', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9688906669616699, 'AUPR-IN': 0.990010142326355, 'AUPR-OUT': 0.885703980922699, 'FPR95TPR': 0.13969795405864716, 'Method': 'LogicOOD', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9833409190177917, 'AUPR-IN': 0.9966961741447449, 'AUPR-OUT': 0.9221418499946594, 'FPR95TPR': 0.07605177909135818, 'Method': 'LogicOOD', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9904576539993286, 'AUPR-IN': 0.9981187582015991, 'AUPR-OUT': 0.9517638087272644, 'FPR95TPR': 0.03667745366692543, 'Method': 'LogicOOD', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.972724437713623, 'AUPR-IN': 0.9948571920394897, 'AUPR-OUT': 0.8645594120025635, 'FPR95TPR': 0.1434735655784607, 'Method': 'LogicOODT', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9940815567970276, 'AUPR-IN': 0.9987857341766357, 'AUPR-OUT': 0.9751086235046387, 'FPR95TPR': 0.02319309674203396, 'Method': 'LogicOODT', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9677409529685974, 'AUPR-IN': 0.9896816611289978, 'AUPR-OUT': 0.8797528743743896, 'FPR95TPR': 0.14670981466770172, 'Method': 'LogicOODT', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9827722311019897, 'AUPR-IN': 0.9965938329696655, 'AUPR-OUT': 0.9182740449905396, 'FPR95TPR': 0.077130526304245, 'Method': 'LogicOODT', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9905887246131897, 'AUPR-IN': 0.998139500617981, 'AUPR-OUT': 0.9532572031021118, 'FPR95TPR': 0.038295578211545944, 'Method': 'LogicOODT', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.998921275138855, 'AUPR-IN': 0.9998000860214233, 'AUPR-OUT': 0.9990899562835693, 'FPR95TPR': 0.0021574972197413445, 'Method': 'LogicOODT+', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.998921275138855, 'AUPR-IN': 0.9998000860214233, 'AUPR-OUT': 0.9990899562835693, 'FPR95TPR': 0.0021574972197413445, 'Method': 'LogicOODT+', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9987955093383789, 'AUPR-IN': 0.99960857629776, 'AUPR-OUT': 0.9986636638641357, 'FPR95TPR': 0.0021574972197413445, 'Method': 'LogicOODT+', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.998921275138855, 'AUPR-IN': 0.9998000860214233, 'AUPR-OUT': 0.9990899562835693, 'FPR95TPR': 0.0021574972197413445, 'Method': 'LogicOODT+', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.998921275138855, 'AUPR-IN': 0.9998000860214233, 'AUPR-OUT': 0.9990899562835693, 'FPR95TPR': 0.0021574972197413445, 'Method': 'LogicOODT+', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9731214046478271, 'AUPR-IN': 0.994937002658844, 'AUPR-OUT': 0.8718329071998596, 'FPR95TPR': 0.13969795405864716, 'Method': 'Ensemble', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.983806312084198, 'AUPR-IN': 0.9966944456100464, 'AUPR-OUT': 0.9406253695487976, 'FPR95TPR': 0.07605177909135818, 'Method': 'Ensemble', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9658526182174683, 'AUPR-IN': 0.988979160785675, 'AUPR-OUT': 0.8800334930419922, 'FPR95TPR': 0.14994606375694275, 'Method': 'Ensemble', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.982609748840332, 'AUPR-IN': 0.9966780543327332, 'AUPR-OUT': 0.9189285635948181, 'FPR95TPR': 0.07874865084886551, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9824454188346863, 'AUPR-IN': 0.996525764465332, 'AUPR-OUT': 0.9230636358261108, 'FPR95TPR': 0.07605177909135818, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9747265577316284, 'AUPR-IN': 0.9953439831733704, 'AUPR-OUT': 0.8731476068496704, 'FPR95TPR': 0.1332254558801651, 'Method': 'MSP', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9683237075805664, 'AUPR-IN': 0.9940677881240845, 'AUPR-OUT': 0.8426461219787598, 'FPR95TPR': 0.17313915491104126, 'Method': 'MSP', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9449208378791809, 'AUPR-IN': 0.9828860759735107, 'AUPR-OUT': 0.7931220531463623, 'FPR95TPR': 0.41747573018074036, 'Method': 'MSP', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9814479351043701, 'AUPR-IN': 0.9965354204177856, 'AUPR-OUT': 0.9083808064460754, 'FPR95TPR': 0.08576051890850067, 'Method': 'MSP', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9749370217323303, 'AUPR-IN': 0.9953621625900269, 'AUPR-OUT': 0.8662569522857666, 'FPR95TPR': 0.11218985915184021, 'Method': 'MSP', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.7604086995124817, 'AUPR-IN': 0.9491509199142456, 'AUPR-OUT': 0.27235373854637146, 'FPR95TPR': 0.9406688213348389, 'Method': 'ReAct', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9360058903694153, 'AUPR-IN': 0.9849609136581421, 'AUPR-OUT': 0.8134351372718811, 'FPR95TPR': 0.2529665529727936, 'Method': 'ReAct', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.8813568353652954, 'AUPR-IN': 0.9609628319740295, 'AUPR-OUT': 0.5997551679611206, 'FPR95TPR': 0.7195253372192383, 'Method': 'ReAct', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.8200498819351196, 'AUPR-IN': 0.9608182311058044, 'AUPR-OUT': 0.3904886245727539, 'FPR95TPR': 0.8031283617019653, 'Method': 'ReAct', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9190484285354614, 'AUPR-IN': 0.9817939400672913, 'AUPR-OUT': 0.7315118908882141, 'FPR95TPR': 0.3462783098220825, 'Method': 'ReAct', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9838528633117676, 'AUPR-IN': 0.9970686435699463, 'AUPR-OUT': 0.9177576303482056, 'FPR95TPR': 0.10625674575567245, 'Method': 'Energy', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9677255749702454, 'AUPR-IN': 0.9940668344497681, 'AUPR-OUT': 0.8415259122848511, 'FPR95TPR': 0.22923408448696136, 'Method': 'Energy', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9475763440132141, 'AUPR-IN': 0.9840060472488403, 'AUPR-OUT': 0.8009927868843079, 'FPR95TPR': 0.38133764266967773, 'Method': 'Energy', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9856798052787781, 'AUPR-IN': 0.9973959922790527, 'AUPR-OUT': 0.9257873892784119, 'FPR95TPR': 0.05987054854631424, 'Method': 'Energy', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n",
      "{'AUROC': 0.9706573486328125, 'AUPR-IN': 0.994682788848877, 'AUPR-OUT': 0.8391507863998413, 'FPR95TPR': 0.19687162339687347, 'Method': 'Energy', 'Dataset': 'TinyImageNetResize'}\n",
      "LSUNCrop\n",
      "{'AUROC': 0.9837499260902405, 'AUPR-IN': 0.997046947479248, 'AUPR-OUT': 0.9175112247467041, 'FPR95TPR': 0.10625674575567245, 'Method': 'MaxLogit', 'Dataset': 'LSUNCrop'}\n",
      "LSUNResize\n",
      "{'AUROC': 0.9678330421447754, 'AUPR-IN': 0.9940877556800842, 'AUPR-OUT': 0.8417904376983643, 'FPR95TPR': 0.22869470715522766, 'Method': 'MaxLogit', 'Dataset': 'LSUNResize'}\n",
      "Textures\n",
      "Found 5640 texture files.\n",
      "{'AUROC': 0.9477255940437317, 'AUPR-IN': 0.9840565919876099, 'AUPR-OUT': 0.801252543926239, 'FPR95TPR': 0.38133764266967773, 'Method': 'MaxLogit', 'Dataset': 'Textures'}\n",
      "TinyImageNetCrop\n",
      "{'AUROC': 0.9856264591217041, 'AUPR-IN': 0.9973836541175842, 'AUPR-OUT': 0.9257252812385559, 'FPR95TPR': 0.059331174939870834, 'Method': 'MaxLogit', 'Dataset': 'TinyImageNetCrop'}\n",
      "TinyImageNetResize\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUROC': 0.9709396958351135, 'AUPR-IN': 0.9947396516799927, 'AUPR-OUT': 0.839903712272644, 'FPR95TPR': 0.19525350630283356, 'Method': 'MaxLogit', 'Dataset': 'TinyImageNetResize'}\n",
      "label\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:10<00:00, 40.01it/s, loss=0.0992]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 83.76%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:10<00:00, 40.83it/s, loss=0.044] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 97.52%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:10<00:00, 40.45it/s, loss=0.0135] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:10<00:00, 40.74it/s, loss=0.0154] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.24%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:11<00:00, 39.75it/s, loss=0.0274] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.95%\n",
      "color\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:11<00:00, 39.67it/s, loss=0.0954]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 98.54%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:11<00:00, 39.10it/s, loss=0.0391] \n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.03%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:11<00:00, 39.73it/s, loss=0.00303]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.78%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 438/438 [00:11<00:00, 39.67it/s, loss=0.00196]\n",
      "  0%|          | 0/438 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 100.00%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 11%|█▏        | 50/438 [00:01<00:09, 40.16it/s, loss=0.00213]"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "\n",
    "for trial in range(10):\n",
    "    print(\"label\")\n",
    "    label_net = train_model(att_index=0, num_classes=33)\n",
    "    print(\"color\")\n",
    "    color_net = train_model(att_index=1, num_classes=6)\n",
    "    print(\"fruit\")\n",
    "\n",
    "    fruit_net = train_fruit_model()\n",
    "\n",
    "    res = evaluate(label_net, color_net, fruit_net)\n",
    "    \n",
    "    for r in res:\n",
    "        r.update({\"Seed\": trial})\n",
    "    \n",
    "    results += res"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3e5287e-1bd0-4bfd-a73f-e427f369f3c2",
   "metadata": {
    "is_executing": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "result_df = pd.DataFrame(results)\n",
    "# print((result_df.groupby(by=\"Method\").agg([\"mean\", \"sem\"]) * 100)[[\"AUROC\", \"AUPR-IN\", \"AUPR-OUT\", \"FPR95TPR\"]].to_latex(float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# s = (result_df.groupby(by=\"Method\").agg([\"mean\", \"sem\"]) * 100)[[\"AUROC\", \"AUPR-IN\", \"AUPR-OUT\", \"FPR95TPR\"]].to_latex(float_format=\"%.2f\")"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "order = ['MSP', 'Energy', 'MaxLogit', 'Entropy', 'ReAct', 'Mahalanobis', 'ViM', 'Ensemble', 'Logic', 'Logic+', 'LogicOOD', 'LogicOOD+', 'LogicOODT', 'LogicOODT+']\n",
    "\n",
    "\n",
    "print((result_df.groupby(by=[\"Method\", \"Seed\"]).mean() * 100).groupby(\"Method\").agg([\"mean\", \"sem\"]).reindex(order).to_latex(float_format=\"%.2f\").replace(\"& 0.\", \"& $\\pm$ 0.\").replace(\"& 2.\", \"& $\\pm$ 2.\").replace(\"& 3.\", \"& $\\pm$ 3.\").replace(\"& 1.\", \"& $\\pm$ 1.\").replace(\"& 4.\", \"& $\\pm$ 4.\").replace(\"& 5.\", \"& $\\pm$ 5.\"))\n",
    "\n",
    "\n",
    "# print(s.replace(\"& 0.\", \"& \\pm 0.\").replace(\"& 1.\", \"& \\pm 1.\").replace(\"& 2.\", \"& \\pm 2.\").replace(\"& 4.\", \"& \\pm 4.\"))"
   ],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "is_executing": true
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
