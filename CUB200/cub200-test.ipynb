{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8808b2bc-639d-4d12-b600-c8737683750b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from torchvision.datasets.folder import default_loader\n",
    "from torchvision.datasets.utils import download_url\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class Cub2011(Dataset):\n",
    "    \"\"\"\n",
    "    https://github.com/TDeVries/cub2011_dataset/blob/master/cub2011.py\n",
    "    \"\"\"\n",
    "    \n",
    "    base_folder = 'CUB_200_2011/images'\n",
    "    url = 'http://www.vision.caltech.edu/visipedia-data/CUB-200-2011/CUB_200_2011.tgz'\n",
    "    filename = 'CUB_200_2011.tgz'\n",
    "    tgz_md5 = '97eceeb196236b17998738112f37df78'\n",
    "\n",
    "    def __init__(self, root, train=True, transform=None, loader=default_loader, download=True):\n",
    "        self.root = os.path.expanduser(root)\n",
    "        self.transform = transform\n",
    "        self.loader = default_loader\n",
    "        self.train = train\n",
    "\n",
    "        if download:\n",
    "            self._download()\n",
    "\n",
    "        if not self._check_integrity():\n",
    "            raise RuntimeError('Dataset not found or corrupted.' +\n",
    "                               ' You can use download=True to download it')\n",
    "\n",
    "    def _load_metadata(self):\n",
    "        images = pd.read_csv(os.path.join(self.root, 'CUB_200_2011', 'images.txt'), sep=' ',\n",
    "                             names=['img_id', 'filepath'])\n",
    "        image_class_labels = pd.read_csv(os.path.join(self.root, 'CUB_200_2011', 'image_class_labels.txt'),\n",
    "                                         sep=' ', names=['img_id', 'target'])\n",
    "        train_test_split = pd.read_csv(os.path.join(self.root, 'CUB_200_2011', 'train_test_split.txt'),\n",
    "                                       sep=' ', names=['img_id', 'is_training_img'])\n",
    "\n",
    "        data = images.merge(image_class_labels, on='img_id')\n",
    "        self.data = data.merge(train_test_split, on='img_id')\n",
    "\n",
    "        if self.train:\n",
    "            self.data = self.data[self.data.is_training_img == 1]\n",
    "        else:\n",
    "            self.data = self.data[self.data.is_training_img == 0]\n",
    "\n",
    "    def _check_integrity(self):\n",
    "        try:\n",
    "            self._load_metadata()\n",
    "        except Exception:\n",
    "            return False\n",
    "\n",
    "        for index, row in self.data.iterrows():\n",
    "            filepath = os.path.join(self.root, self.base_folder, row.filepath)\n",
    "            if not os.path.isfile(filepath):\n",
    "                print(filepath)\n",
    "                return False\n",
    "        return True\n",
    "\n",
    "    def _download(self):\n",
    "        import tarfile\n",
    "\n",
    "        if self._check_integrity():\n",
    "            print('Files already downloaded and verified')\n",
    "            return\n",
    "\n",
    "        download_url(self.url, self.root, self.filename, self.tgz_md5)\n",
    "\n",
    "        with tarfile.open(os.path.join(self.root, self.filename), \"r:gz\") as tar:\n",
    "            tar.extractall(path=self.root)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        sample = self.data.iloc[idx]\n",
    "        path = os.path.join(self.root, self.base_folder, sample.filepath)\n",
    "        target = sample.target - 1  # Targets start at 1 by default, so shift to 0\n",
    "        img = self.loader(path)\n",
    "\n",
    "        if self.transform is not None:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        return img, target, 0, 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "be1f2f89-ae9a-440c-bb09-16d718d1984e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, Resize, Compose\n",
    "import torch \n",
    "from torchvision.models import Wide_ResNet50_2_Weights\n",
    "\n",
    "trans = Wide_ResNet50_2_Weights.DEFAULT.transforms() #Compose([ToTensor(), Resize((128, 128))])\n",
    "\n",
    "train_data = data = Cub2011(\"/home/ki/projects/work/papers/logic-anomaly/code/data/CUB_200_2011/\", train=True, transform=trans)\n",
    "test_data = Cub2011(\"/home/ki/projects/work/papers/logic-anomaly/code/data/CUB_200_2011/\", train=False, transform=trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "64d69c3e-b65d-41bc-b776-6f8693d4bedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, \n",
    "                                           shuffle=True, num_workers=2)\n",
    "test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, \n",
    "                                          shuffle=False, num_workers=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "060690c1-2720-4e4c-8097-5cb30bf085d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_ood.model import WideResNet\n",
    "from torchvision.models import wide_resnet50_2\n",
    "\n",
    "\n",
    "# def override \n",
    "def LeNet(num_classes=None, *args, **kwargs):\n",
    "    model = wide_resnet50_2(num_classes=1000,  weights=Wide_ResNet50_2_Weights.DEFAULT) # ,\n",
    "    # model = WideResNet(*args, num_classes=1000, **kwargs, pretrained=\"imagenet32\")\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "1ca81467-13c9-408a-9f2e-90906a9163bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading: \"https://download.pytorch.org/models/wide_resnet50_2-9ba9bcbe.pth\" to /home/ki/.cache/torch/hub/checkpoints/wide_resnet50_2-9ba9bcbe.pth\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b32c352ef40948649aeb5eb3de50bc3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0.00/263M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1,    11] loss: 0.029\n",
      "[1,   111] loss: 0.263\n",
      "Accuracy of the label network on the test images: 1.57%\n",
      "[2,    11] loss: 0.226\n",
      "[2,   111] loss: 0.246\n",
      "Accuracy of the label network on the test images: 3.21%\n",
      "[3,    11] loss: 0.205\n",
      "[3,   111] loss: 0.208\n",
      "Accuracy of the label network on the test images: 5.09%\n",
      "[4,    11] loss: 0.160\n",
      "[4,   111] loss: 0.149\n",
      "Accuracy of the label network on the test images: 6.09%\n",
      "[5,    11] loss: 0.108\n",
      "[5,   111] loss: 0.100\n",
      "Accuracy of the label network on the test images: 6.59%\n",
      "[6,    11] loss: 0.077\n",
      "[6,   111] loss: 0.070\n",
      "Accuracy of the label network on the test images: 7.39%\n",
      "[7,    11] loss: 0.054\n",
      "[7,   111] loss: 0.051\n",
      "Accuracy of the label network on the test images: 7.65%\n",
      "[8,    11] loss: 0.042\n",
      "[8,   111] loss: 0.039\n",
      "Accuracy of the label network on the test images: 8.16%\n",
      "[9,    11] loss: 0.032\n",
      "[9,   111] loss: 0.031\n",
      "Accuracy of the label network on the test images: 8.70%\n",
      "[10,    11] loss: 0.026\n",
      "[10,   111] loss: 0.024\n",
      "Accuracy of the label network on the test images: 8.78%\n",
      "[11,    11] loss: 0.021\n",
      "[11,   111] loss: 0.019\n",
      "Accuracy of the label network on the test images: 9.30%\n",
      "[12,    11] loss: 0.017\n",
      "[12,   111] loss: 0.015\n",
      "Accuracy of the label network on the test images: 9.61%\n",
      "[13,    11] loss: 0.014\n",
      "[13,   111] loss: 0.013\n",
      "Accuracy of the label network on the test images: 9.63%\n",
      "[14,    11] loss: 0.012\n",
      "[14,   111] loss: 0.010\n",
      "Accuracy of the label network on the test images: 9.89%\n",
      "[15,    11] loss: 0.009\n",
      "[15,   111] loss: 0.009\n",
      "Accuracy of the label network on the test images: 10.01%\n",
      "[16,    11] loss: 0.008\n",
      "[16,   111] loss: 0.008\n",
      "Accuracy of the label network on the test images: 10.04%\n",
      "[17,    11] loss: 0.007\n",
      "[17,   111] loss: 0.006\n",
      "Accuracy of the label network on the test images: 10.37%\n",
      "[18,    11] loss: 0.005\n",
      "[18,   111] loss: 0.005\n",
      "Accuracy of the label network on the test images: 10.34%\n",
      "[19,    11] loss: 0.005\n",
      "[19,   111] loss: 0.004\n",
      "Accuracy of the label network on the test images: 10.36%\n",
      "[20,    11] loss: 0.004\n",
      "[20,   111] loss: 0.004\n",
      "Accuracy of the label network on the test images: 10.37%\n",
      "[21,    11] loss: 0.003\n",
      "[21,   111] loss: 0.003\n",
      "Accuracy of the label network on the test images: 10.29%\n",
      "[22,    11] loss: 0.003\n",
      "[22,   111] loss: 0.003\n",
      "Accuracy of the label network on the test images: 10.51%\n",
      "[23,    11] loss: 0.003\n",
      "[23,   111] loss: 0.003\n",
      "Accuracy of the label network on the test images: 10.55%\n",
      "[24,    11] loss: 0.002\n",
      "[24,   111] loss: 0.002\n",
      "Accuracy of the label network on the test images: 10.86%\n",
      "[25,    11] loss: 0.002\n",
      "[25,   111] loss: 0.002\n",
      "Accuracy of the label network on the test images: 10.75%\n",
      "[26,    11] loss: 0.002\n",
      "[26,   111] loss: 0.002\n",
      "Accuracy of the label network on the test images: 10.72%\n",
      "[27,    11] loss: 0.002\n",
      "[27,   111] loss: 0.002\n",
      "Accuracy of the label network on the test images: 10.70%\n",
      "[28,    11] loss: 0.002\n",
      "[28,   111] loss: 0.002\n",
      "Accuracy of the label network on the test images: 10.70%\n",
      "[29,    11] loss: 0.002\n",
      "[29,   111] loss: 0.002\n",
      "Accuracy of the label network on the test images: 10.84%\n",
      "[30,    11] loss: 0.001\n",
      "[30,   111] loss: 0.001\n",
      "Accuracy of the label network on the test images: 10.84%\n",
      "[31,    11] loss: 0.001\n",
      "[31,   111] loss: 0.001\n",
      "Accuracy of the label network on the test images: 10.99%\n",
      "[32,    11] loss: 0.001\n",
      "[32,   111] loss: 0.001\n",
      "Accuracy of the label network on the test images: 10.98%\n",
      "[33,    11] loss: 0.001\n",
      "[33,   111] loss: 0.001\n",
      "Accuracy of the label network on the test images: 11.06%\n",
      "[34,    11] loss: 0.001\n",
      "[34,   111] loss: 0.001\n",
      "Accuracy of the label network on the test images: 11.05%\n",
      "[35,    11] loss: 0.001\n",
      "[35,   111] loss: 0.001\n",
      "Accuracy of the label network on the test images: 10.93%\n",
      "[36,    11] loss: 0.001\n",
      "[36,   111] loss: 0.001\n",
      "Accuracy of the label network on the test images: 11.11%\n",
      "[37,    11] loss: 0.001\n",
      "[37,   111] loss: 0.001\n",
      "Accuracy of the label network on the test images: 11.01%\n",
      "[38,    11] loss: 0.001\n",
      "[38,   111] loss: 0.001\n",
      "Accuracy of the label network on the test images: 11.11%\n",
      "[39,    11] loss: 0.001\n",
      "[39,   111] loss: 0.001\n",
      "Accuracy of the label network on the test images: 11.10%\n",
      "[40,    11] loss: 0.001\n",
      "[40,   111] loss: 0.001\n",
      "Accuracy of the label network on the test images: 11.11%\n",
      "[41,    11] loss: 0.001\n",
      "[41,   111] loss: 0.001\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[0;32mIn [27]\u001b[0m, in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     47\u001b[0m         _, predicted \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmax(outputs\u001b[38;5;241m.\u001b[39mdata, \u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m     48\u001b[0m         total \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m shapes\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[0;32m---> 49\u001b[0m         correct \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[43m(\u001b[49m\u001b[43mpredicted\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlabels\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     51\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mAccuracy of the label network on the test images: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mcorrect \u001b[38;5;241m/\u001b[39m total\u001b[38;5;132;01m:\u001b[39;00m\u001b[38;5;124m.2%\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m     52\u001b[0m accs\u001b[38;5;241m.\u001b[39mappend(correct \u001b[38;5;241m/\u001b[39m total)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from torch import nn \n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from os.path import join\n",
    "import pandas as pd \n",
    "from PIL import Image\n",
    "from torch.optim import SGD\n",
    "import seaborn as sb \n",
    "\n",
    "device = \"cuda\"\n",
    "\n",
    "label_net = LeNet(num_classes=200).to(device)\n",
    "\n",
    "learning_rate = 1e-3\n",
    "momentum = 0.9\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(label_net.parameters(), lr=learning_rate, momentum=momentum, nesterov=True)\n",
    "\n",
    "accs = []\n",
    "running_loss = 0.0\n",
    "\n",
    "for epoch in range(100):\n",
    "    \n",
    "    for i, batch in enumerate(train_loader):\n",
    "        inputs, labels, colors, shapes  = batch\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = label_net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        running_loss += loss.item()\n",
    "        if i % 100 == 10:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %(epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch in test_loader:\n",
    "            inputs, labels, colors, shapes  = batch\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = label_net(inputs)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            total += shapes.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "\n",
    "    print(f'Accuracy of the label network on the test images: {correct / total:.2%}')\n",
    "    accs.append(correct / total)\n",
    "\n",
    "print('Finished Training LabelNet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e34b6340-a3d3-4a16-ba23-5f53f2acd068",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
