{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "2b17f874",
   "metadata": {},
   "source": [
    "# Street sign with Pre-Trained WideResNet\n",
    "\n",
    "With additional shield net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e2142c72",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8b85eb0a70>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from os.path import join\n",
    "import pandas as pd \n",
    "from PIL import Image\n",
    "from torch.optim import SGD\n",
    "import seaborn as sb \n",
    "from gtsrb import GTSRB\n",
    "from detectors import EnsembleDetector, SemanticDetector, LogicOnlyDetector\n",
    "import os \n",
    "from pytorch_ood.utils import fix_random_seed\n",
    "import seaborn as sb \n",
    "import torch \n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"MIG-GPU-bb1ccb6e-2bc9-c7a1-b25d-3eef9033e192/0/0\" \n",
    "\n",
    "sb.set()\n",
    "\n",
    "device=\"cuda:0\"\n",
    "root = \"data/\"\n",
    "\n",
    "\n",
    "fix_random_seed()\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    fix_random_seed()\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8183c0e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, Resize, Compose\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "trans = Compose([ToTensor(), Resize((32, 32))])\n",
    "\n",
    "train_data = GTSRB(root=root, train=True, transforms=trans)\n",
    "test_data = GTSRB(root=root, train=False, transforms=trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "29a8e7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2, worker_init_fn=seed_worker)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False, num_workers=2, worker_init_fn=seed_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "294016df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchvision.models.resnet import resnet18\n",
    "from pytorch_ood.model import WideResNet\n",
    "from functools import partial\n",
    "\n",
    "def myfeatures(self, x):\n",
    "    x = self.conv1(x)\n",
    "    x = self.bn1(x)\n",
    "    x = self.relu(x)\n",
    "    x = self.maxpool(x)\n",
    "\n",
    "    x = self.layer1(x)\n",
    "    x = self.layer2(x)\n",
    "    x = self.layer3(x)\n",
    "    x = self.layer4(x)\n",
    "\n",
    "    x = self.avgpool(x)\n",
    "    x = torch.flatten(x, 1)\n",
    "    return x\n",
    "    \n",
    "# def override \n",
    "def Model(num_classes=None, *args, **kwargs):\n",
    "    model = resnet18(num_classes=num_classes)\n",
    "    model.myfeatures = partial(myfeatures, model)\n",
    "#     model = WideResNet(*args, num_classes=1000, **kwargs, pretrained=\"imagenet32\")\n",
    "#     model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6d572b0f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm \n",
    "import numpy as np \n",
    "\n",
    "def train_model(att_index, num_classes):\n",
    "    \"\"\"\n",
    "    train a model for the given attribute index \n",
    "    \"\"\"\n",
    "    trans = Compose([ToTensor(), Resize((32, 32))])\n",
    "    train_data = GTSRB(root=root, train=True, transforms=trans)\n",
    "    test_data = GTSRB(root=root, train=False, transforms=trans)\n",
    "    \n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2, worker_init_fn=seed_worker)\n",
    "    test_loader = DataLoader(test_data, batch_size=32, shuffle=False, num_workers=2, worker_init_fn=seed_worker)\n",
    "    \n",
    "    model = Model(num_classes=num_classes).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9, nesterov=True)\n",
    "\n",
    "    accs = []\n",
    "\n",
    "    for epoch in range(20):\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        bar = tqdm(train_loader)\n",
    "        for inputs, y in bar:\n",
    "            labels = y[:, att_index]\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss = 0.8 * running_loss + 0.2 * loss.item()\n",
    "            bar.set_postfix({\"loss\": running_loss})\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            for inputs, y in test_loader:\n",
    "                labels = y[:, att_index]\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, dim=1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Accuracy of the network on the test images: {correct / total:.2%}')\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cf3e27bb",
   "metadata": {},
   "source": [
    "# Sign Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86938a19",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from pytorch_ood.utils import is_known\n",
    "from tqdm.notebook import tqdm \n",
    "from pytorch_ood.dataset.img import TinyImages300k\n",
    "from pytorch_ood.utils import ToUnknown\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "def train_sign_model():\n",
    "    tiny = TinyImages300k(root=root, download=True, transform=trans, target_transform=ToUnknown())\n",
    "    data_train_out, data_test_out, _ = random_split(tiny, [50000, 10000, 240000], generator=torch.Generator().manual_seed(123))\n",
    "\n",
    "    train_data_noatt = GTSRB(root=root, train=True, transforms=trans, target_transform=lambda y: y[0])\n",
    "    test_data_noatt = GTSRB(root=root, train=False, transforms=trans, target_transform=lambda y: y[0])\n",
    "\n",
    "    new_loader = DataLoader(train_data_noatt + data_train_out, batch_size=32, shuffle=True, num_workers=10, worker_init_fn=seed_worker)\n",
    "    new_test_loader = DataLoader(test_data_noatt + data_test_out, batch_size=32, shuffle=False, num_workers=10, worker_init_fn=seed_worker)\n",
    "\n",
    "    model = Model(num_classes=2).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9, nesterov=True)\n",
    "\n",
    "    accs = []\n",
    "\n",
    "    for epoch in range(20):\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        bar = tqdm(new_loader)\n",
    "        for inputs, y in bar:\n",
    "            labels = is_known(y).long()\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss = 0.8 * running_loss + 0.2 * loss.item()\n",
    "            bar.set_postfix({\"loss\": running_loss})\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            for inputs, y in new_test_loader:\n",
    "                labels = is_known(y).long()\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Accuracy of the shape network on the test images: {correct / total:.2%}')\n",
    "        accs.append(correct / total)\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "edc7d026",
   "metadata": {
    "tags": []
   },
   "source": [
    "# OOD Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "911cbe73",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_ood.dataset.img import (LSUNCrop, LSUNResize, Textures, TinyImageNetCrop, TinyImageNetResize)\n",
    "from pytorch_ood.detector import EnergyBased, MaxSoftmax, Mahalanobis, MaxLogit, ViM, Entropy\n",
    "from pytorch_ood.utils import ToRGB, OODMetrics\n",
    "\n",
    "\n",
    "def evaluate(label_net, shape_net, color_net, shield_net):\n",
    "    _ = label_net.eval()\n",
    "    _ = shape_net.eval()\n",
    "    _ = color_net.eval()\n",
    "    _ = shield_net.eval()\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    trans = Compose([Resize(size=(32, 32)), ToRGB(), ToTensor()])\n",
    "    data_in = GTSRB(root=root, train=False, transforms=trans, target_transform=lambda y: y[0])\n",
    "    data_in_train = GTSRB(root=root, train=True, transforms=trans, target_transform=lambda y: y[0])\n",
    "    loader_train = DataLoader(data_in_train, batch_size=1024, shuffle=False, worker_init_fn=seed_worker)\n",
    "    # dataset_out_test = Textures(root=root, transform=trans, target_transform=ToUnknown(), download=True)\n",
    "    \n",
    "    detectors = {\n",
    "        \"MSP\": MaxSoftmax(label_net),\n",
    "        \"Energy\": EnergyBased(label_net),\n",
    "        \"Mahalanobis\": Mahalanobis(label_net.myfeatures),\n",
    "        \"MaxLogit\": MaxLogit(label_net),\n",
    "        \"Entropy\": Entropy(label_net),\n",
    "        \"ViM\": ViM(label_net.myfeatures, w=label_net.fc.weight, b=label_net.fc.bias, d=64),\n",
    "        \"Logic\": LogicOnlyDetector(\n",
    "                label_net, \n",
    "                shape_net, \n",
    "                color_net, \n",
    "                GTSRB(root=root).class_to_shape, \n",
    "                GTSRB(root=root).class_to_color, \n",
    "        ),\n",
    "        \"Logic+\": LogicOnlyDetector(\n",
    "                label_net, \n",
    "                shape_net, \n",
    "                color_net, \n",
    "                GTSRB(root=root).class_to_shape, \n",
    "                GTSRB(root=root).class_to_color, \n",
    "                sign_net=shield_net\n",
    "        ),\n",
    "        \"Semantic-OE\": SemanticDetector(\n",
    "            label_net, \n",
    "            shape_net, \n",
    "            color_net, \n",
    "            GTSRB(root=root).class_to_shape, \n",
    "            GTSRB(root=root).class_to_color, \n",
    "            sign_net=shield_net\n",
    "        ),\n",
    "        \"Semantic\": SemanticDetector(\n",
    "                label_net, \n",
    "                shape_net, \n",
    "                color_net, \n",
    "                GTSRB(root=root).class_to_shape, \n",
    "                GTSRB(root=root).class_to_color, \n",
    "        ),\n",
    "        \"Ensemble\": EnsembleDetector(label_net, shape_net, color_net)\n",
    "    }\n",
    "    \n",
    "    for detector_name, detector in detectors.items():\n",
    "        print(f\"Fitting {detector_name}\")\n",
    "        try:\n",
    "            detector.fit(loader_train, device=device)\n",
    "        except TypeError:\n",
    "            detector.fit(loader_train)\n",
    "        \n",
    "    datasets = {d.__name__: d for d in (LSUNCrop, LSUNResize, Textures, TinyImageNetCrop, TinyImageNetResize)}\n",
    "    \n",
    "    for detector_name, detector in detectors.items():\n",
    "        for data_name, dataset_c in datasets.items():\n",
    "            data_out = dataset_c(root=root, transform=trans, target_transform=ToUnknown(), download=True)\n",
    "            loader = DataLoader(data_in+data_out, batch_size=1024, shuffle=False, worker_init_fn=seed_worker)\n",
    "            \n",
    "            scores = []\n",
    "            ys = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for x, y in loader:\n",
    "                    scores.append(detector(x.to(device)))\n",
    "                    ys.append(y.to(device))\n",
    "                    \n",
    "                scores = torch.cat(scores, dim=0).cpu()\n",
    "                ys = torch.cat(ys, dim=0).cpu()\n",
    "            \n",
    "            metrics = OODMetrics()\n",
    "            metrics.update(scores, ys)\n",
    "            r = metrics.compute()\n",
    "            r.update({\n",
    "                \"Method\": detector_name,\n",
    "                \"Dataset\": data_name\n",
    "            })\n",
    "            print(r)\n",
    "            results.append(r)\n",
    "    \n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "40899596",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_acc(net, att_idx=0, oe=False):\n",
    "    _ = net.eval()\n",
    "    \n",
    "    if oe:\n",
    "        target_trans = lambda y: torch.tensor(1)\n",
    "    else:\n",
    "         target_trans = lambda y: y[att_idx]\n",
    "\n",
    "    trans = Compose([Resize(size=(32, 32)), ToRGB(), ToTensor()])\n",
    "    data_in = GTSRB(root=root, train=False, transforms=trans, target_transform=target_trans)\n",
    "    loader = DataLoader(data_in, batch_size=1024, shuffle=False, worker_init_fn=seed_worker)\n",
    "            \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            predicted = outputs.max(dim=1).indices\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return correct / total  \n",
    "\n",
    "def evaluate_accs(label_net, shape_net, color_net, shield_net):\n",
    "    r = {}\n",
    "    names = (\"Label\", \"Color\", \"Shape\",)\n",
    "    \n",
    "    for n, net in enumerate((label_net, color_net, shape_net)): \n",
    "        acc = evaluate_acc(net, n)\n",
    "        r[names[n]] = acc\n",
    "    \n",
    "    acc = evaluate_acc(shield_net, oe=True)\n",
    "    r[\"Sign\"] = acc\n",
    "    \n",
    "    return [r] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7414bfad",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 5\u001b[0m\n\u001b[1;32m      2\u001b[0m results_acc \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m trial \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10\u001b[39m):\n\u001b[0;32m----> 5\u001b[0m     shield_net \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_sign_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     shape_net \u001b[38;5;241m=\u001b[39m train_model(att_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[1;32m      7\u001b[0m     color_net \u001b[38;5;241m=\u001b[39m train_model(att_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m, num_classes\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m)\n",
      "Cell \u001b[0;32mIn[10], line 18\u001b[0m, in \u001b[0;36mtrain_sign_model\u001b[0;34m()\u001b[0m\n\u001b[1;32m     15\u001b[0m new_loader \u001b[38;5;241m=\u001b[39m DataLoader(train_data_noatt \u001b[38;5;241m+\u001b[39m data_train_out, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, worker_init_fn\u001b[38;5;241m=\u001b[39mseed_worker)\n\u001b[1;32m     16\u001b[0m new_test_loader \u001b[38;5;241m=\u001b[39m DataLoader(test_data_noatt \u001b[38;5;241m+\u001b[39m data_test_out, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, worker_init_fn\u001b[38;5;241m=\u001b[39mseed_worker)\n\u001b[0;32m---> 18\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnum_classes\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     20\u001b[0m criterion \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m     21\u001b[0m optimizer \u001b[38;5;241m=\u001b[39m SGD(model\u001b[38;5;241m.\u001b[39mparameters(), lr\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.001\u001b[39m, momentum\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.9\u001b[39m, nesterov\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "File \u001b[0;32m~/.local/share/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1145\u001b[0m, in \u001b[0;36mModule.to\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1141\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                     non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[1;32m   1143\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m, non_blocking)\n\u001b[0;32m-> 1145\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:797\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    795\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_apply\u001b[39m(\u001b[38;5;28mself\u001b[39m, fn):\n\u001b[1;32m    796\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mchildren():\n\u001b[0;32m--> 797\u001b[0m         \u001b[43mmodule\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_apply\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfn\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    799\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mcompute_should_use_set_data\u001b[39m(tensor, tensor_applied):\n\u001b[1;32m    800\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39m_has_compatible_shallow_copy_type(tensor, tensor_applied):\n\u001b[1;32m    801\u001b[0m             \u001b[38;5;66;03m# If the new tensor has compatible tensor type as the existing tensor,\u001b[39;00m\n\u001b[1;32m    802\u001b[0m             \u001b[38;5;66;03m# the current behavior is to change the tensor in-place using `.data =`,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    807\u001b[0m             \u001b[38;5;66;03m# global flag to let the user control whether they want the future\u001b[39;00m\n\u001b[1;32m    808\u001b[0m             \u001b[38;5;66;03m# behavior of overwriting the existing tensor or not.\u001b[39;00m\n",
      "File \u001b[0;32m~/.local/share/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:820\u001b[0m, in \u001b[0;36mModule._apply\u001b[0;34m(self, fn)\u001b[0m\n\u001b[1;32m    816\u001b[0m \u001b[38;5;66;03m# Tensors stored in modules are graph leaves, and we don't want to\u001b[39;00m\n\u001b[1;32m    817\u001b[0m \u001b[38;5;66;03m# track autograd history of `param_applied`, so we have to use\u001b[39;00m\n\u001b[1;32m    818\u001b[0m \u001b[38;5;66;03m# `with torch.no_grad():`\u001b[39;00m\n\u001b[1;32m    819\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[0;32m--> 820\u001b[0m     param_applied \u001b[38;5;241m=\u001b[39m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparam\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    821\u001b[0m should_use_set_data \u001b[38;5;241m=\u001b[39m compute_should_use_set_data(param, param_applied)\n\u001b[1;32m    822\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m should_use_set_data:\n",
      "File \u001b[0;32m~/.local/share/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/nn/modules/module.py:1143\u001b[0m, in \u001b[0;36mModule.to.<locals>.convert\u001b[0;34m(t)\u001b[0m\n\u001b[1;32m   1140\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m convert_to_format \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m t\u001b[38;5;241m.\u001b[39mdim() \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m4\u001b[39m, \u001b[38;5;241m5\u001b[39m):\n\u001b[1;32m   1141\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m t\u001b[38;5;241m.\u001b[39mto(device, dtype \u001b[38;5;28;01mif\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_floating_point() \u001b[38;5;129;01mor\u001b[39;00m t\u001b[38;5;241m.\u001b[39mis_complex() \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m   1142\u001b[0m                 non_blocking, memory_format\u001b[38;5;241m=\u001b[39mconvert_to_format)\n\u001b[0;32m-> 1143\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_floating_point\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01mor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mis_complex\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01melse\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnon_blocking\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.local/share/anaconda3/envs/pytorch/lib/python3.9/site-packages/torch/cuda/__init__.py:247\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    245\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    246\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 247\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    249\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    250\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    251\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "results_acc = []\n",
    "\n",
    "for trial in range(10):\n",
    "    shield_net = train_sign_model()\n",
    "    shape_net = train_model(att_index=2, num_classes=5)\n",
    "    color_net = train_model(att_index=1, num_classes=4)\n",
    "    label_net = train_model(att_index=0, num_classes=43)\n",
    "    \n",
    "    res = evaluate(label_net, shape_net, color_net, shield_net)\n",
    "    res_acc = evaluate_accs(label_net, shape_net, color_net, shield_net)\n",
    "    \n",
    "    for r in res:\n",
    "        r.update({\"Seed\": trial})\n",
    "        \n",
    "    for r in res_acc:\n",
    "        r.update({\"Seed\": trial})\n",
    "    \n",
    "    results += res\n",
    "    results_acc += res_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59d4e44",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "result_df = pd.DataFrame(results)\n",
    "tab = (result_df.groupby(by=\"Method\").agg([\"mean\", \"sem\"]) * 100)[[\"AUROC\", \"AUPR-IN\", \"AUPR-OUT\", \"FPR95TPR\"]].sort_values(by=(\"AUROC\", \"mean\"))\n",
    "string = tab.to_latex(float_format=\"%.2f\").replace(\"& 0\", \"& $\\pm$ 0\").replace(\"& 1\", \"& $\\pm$ 1\")\n",
    "print(string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2aecd63",
   "metadata": {},
   "outputs": [],
   "source": [
    "(result_df.groupby(by=[\"Method\", \"Seed\"]).mean() * 100).groupby(\"Method\").agg([\"mean\", \"sem\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d30369",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "for metric in [\"AUROC\", \"AUPR-IN\", \"AUPR-OUT\", \"FPR95TPR\"]:\n",
    "    sem_auroc = np.array(result_df[result_df[\"Method\"] == \"Semantic\"].groupby(by=[\"Method\", \"Seed\"]).mean(numeric_only=True)[metric])\n",
    "    sem_ensemble =  np.array(result_df[result_df[\"Method\"] == \"Ensemble\"].groupby(by=[\"Method\", \"Seed\"]).mean(numeric_only=True)[metric])\n",
    "    stat, p = ttest_ind(sem_auroc, sem_ensemble, equal_var=False) # , permutations=100000\n",
    "    print(f\"Metric: {metric} -> {p}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5870f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset gives np.array([label, color, shape])\n",
    "print((pd.DataFrame(results_acc) * 100).agg([\"mean\", \"sem\"]).to_latex(float_format=\"%.2f\"))\n",
    "# results_acc = evaluate_accs(label_net, shape_net, color_net, shield_net)\n",
    "# print(results_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72d0c32f",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd7a378d-da4f-4d8a-9e6c-1cf1ecb2cc90",
   "metadata": {},
   "outputs": [],
   "source": [
    "msp = MaxSoftmax(label_net)\n",
    "semantic_detector = SemanticDetector(\n",
    "                label_net, \n",
    "                shape_net, \n",
    "                color_net, \n",
    "                GTSRB(root=root).class_to_shape, \n",
    "                GTSRB(root=root).class_to_color)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bc5abe9-d074-494f-9cec-3665d570f08b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_ood.utils import TensorBuffer\n",
    "\n",
    "data_in = GTSRB(root=root, train=False, transforms=trans, target_transform=lambda y: y[0])\n",
    "data_out = Textures(root=root, transform=trans, target_transform=ToUnknown(), download=True)\n",
    "\n",
    "loader = DataLoader(data_in + data_out, shuffle=False, batch_size=1024)\n",
    "\n",
    "buffer = TensorBuffer()\n",
    "\n",
    "for x, y in loader:\n",
    "    scores_msp = msp(x.to(device))\n",
    "    buffer.append(\"msp\", scores_msp)\n",
    "    \n",
    "    scores_sem = semantic_detector(x.to(device))\n",
    "    buffer.append(\"sem\", scores_sem)\n",
    "    \n",
    "    buffer.append(\"y\", y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbceace0-8eae-4ff4-aa25-95b31a546fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt \n",
    "sb.set_style(\"white\")\n",
    "\n",
    "df = pd.DataFrame({\"Scores\": buffer[\"sem\"].numpy() + 1, \"label\": buffer[\"y\"]})\n",
    "df[\"Type\"] = df[\"label\"].apply(lambda x: \"IN\" if is_known(x) else \"OOD\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,3))\n",
    "\n",
    "sb.kdeplot(data=df, x=\"Scores\", clip=[0,1], common_norm=False, fill=True, hue=\"Type\", palette=\"crest\", bw_method=0.2) # , palette=\"crest\" # , \n",
    "sb.move_legend(ax, \"upper right\")\n",
    "plt.xlabel(\"Normalized Outlier Score\")\n",
    "sb.despine(left=True, right=True, top=True)\n",
    "plt.xlim([0,1])\n",
    "plt.yticks([])\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"img/scores-gtsrb.pdf\", bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1cafbf6-91ab-4a47-b369-8fec3f2d0e01",
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.set_style(\"white\")\n",
    "\n",
    "df = pd.DataFrame({\"Scores\": buffer[\"msp\"].numpy() + 1, \"label\": buffer[\"y\"]})\n",
    "df[\"Type\"] = df[\"label\"].apply(lambda x: \"IN\" if is_known(x) else \"OOD\")\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(4,3))\n",
    "\n",
    "sb.kdeplot(data=df, x=\"Scores\", clip=[0,1], common_norm=False, fill=True, hue=\"Type\", palette=\"crest\", bw_method=0.2) # , palette=\"crest\" # , \n",
    "plt.xlim([0,1])\n",
    "sb.move_legend(ax, \"upper right\")\n",
    "plt.xlabel(\"Normalized Outlier Score\")\n",
    "sb.despine(left=True, right=True, top=True)\n",
    "\n",
    "plt.yticks([])\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24f0f149-f4d5-408d-846b-a786d3fe5f3e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
