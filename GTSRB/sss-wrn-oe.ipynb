{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6515def1",
   "metadata": {},
   "source": [
    "# Street sign with Pre-Trained WideResNet\n",
    "\n",
    "With additional shield net "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06b287f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.optim import SGD\n",
    "import seaborn as sb \n",
    "from gtsrb import GTSRB\n",
    "from detectors import EnsembleDetector, LogicOOD, PrologOOD\n",
    "import torch\n",
    "from pytorch_ood.utils import fix_random_seed\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "logger = logging.getLogger()\n",
    "logger.addHandler(logging.StreamHandler(stream=sys.stdout))\n",
    "logger.setLevel(logging.INFO)\n",
    "\n",
    "\n",
    "fix_random_seed(123)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    fix_random_seed(worker_id)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n",
    "\n",
    "sb.set()\n",
    "\n",
    "device=\"cuda:0\"\n",
    "root = \"../data/\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2c5cb26",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_ood.utils import ToRGB\n",
    "from torchvision.transforms import ToTensor, Resize, Compose\n",
    "import torch \n",
    "from torch.utils.data import DataLoader, random_split\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "trans = Compose([ToRGB(), ToTensor(), Resize((32, 32), antialias=True)])\n",
    "\n",
    "data = GTSRB(root=root, train=True, transforms=trans)\n",
    "print(len(data))\n",
    "train_data , val_data  = random_split(data, [35000, 4209], generator=torch.Generator().manual_seed(123))\n",
    "\n",
    "test_data = GTSRB(root=root, train=False, transforms=trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9eb2fb87",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2, worker_init_fn=seed_worker)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False, num_workers=2, worker_init_fn=seed_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9a268a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchvision.models.resnet import resnet18\n",
    "from pytorch_ood.model import WideResNet\n",
    "\n",
    "# def override \n",
    "def Model(num_classes=None, *args, **kwargs):\n",
    "    model = WideResNet(*args, num_classes=1000, **kwargs, pretrained=\"imagenet32\")\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "857cc076",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(att_index, num_classes):\n",
    "    \"\"\"\n",
    "    train a model for the given attribute index \n",
    "    \"\"\"\n",
    "    data = GTSRB(root=root, train=True, transforms=trans)\n",
    "    train_data , val_data  = random_split(data, [35000, 4209], generator=torch.Generator().manual_seed(123))\n",
    "    test_data = GTSRB(root=root, train=False, transforms=trans)\n",
    "\n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2, worker_init_fn=seed_worker)\n",
    "    test_loader = DataLoader(test_data, batch_size=32, shuffle=False, num_workers=2, worker_init_fn=seed_worker)\n",
    "    \n",
    "    model = Model(num_classes=num_classes).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9, nesterov=True)\n",
    "\n",
    "    for epoch in range(20):\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        bar = tqdm(train_loader)\n",
    "        for inputs, y in bar:\n",
    "            labels = y[:, att_index]\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss = 0.8 * running_loss + 0.2 * loss.item()\n",
    "            bar.set_postfix({\"loss\": running_loss})\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            for inputs, y in test_loader:\n",
    "                labels = y[:, att_index]\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, dim=1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Accuracy of the network on the test images: {correct / total:.2%}')\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e239ff",
   "metadata": {},
   "source": [
    "# Sign Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c87fc89",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from pytorch_ood.utils import is_known\n",
    "from tqdm.notebook import tqdm \n",
    "from pytorch_ood.dataset.img import TinyImages300k\n",
    "from pytorch_ood.utils import ToUnknown\n",
    "from torch.utils.data import random_split\n",
    "from torch.utils.data import TensorDataset\n",
    "\n",
    "\n",
    "# class GANData:\n",
    "#     def __init__(self):\n",
    "#         self.data =  torch.tensor(np.load(\"../data/gtsrb-samples-z-var-100.npz\")[\"x\"], dtype=torch.float32)\n",
    "#         resize = Resize(size=(32, 32), antialias=True)\n",
    "#         self.data = torch.stack([resize(a) for a in self.data])\n",
    "#\n",
    "#     def __len__(self):\n",
    "#         return len(self.data)\n",
    "#\n",
    "#     def __getitem__(self, item):\n",
    "#         return self.data[item], torch.tensor(-1)\n",
    "\n",
    "\n",
    "# %%\n",
    "def train_sign_model():\n",
    "    # dataset = GANData()\n",
    "    # print(len(dataset))\n",
    "    dataset = TinyImages300k(root=root, download=True, transform=trans, target_transform=lambda x: torch.tensor(-1))\n",
    "    data_train_out , data_test_out, _  = random_split(dataset, [5000, 10000, 285000], generator=torch.Generator().manual_seed(123))\n",
    "\n",
    "    train_data_noatt = GTSRB(root=root, train=True, transforms=trans, target_transform=lambda y: torch.tensor(y[0]))\n",
    "    test_data_noatt = GTSRB(root=root, train=False, transforms=trans, target_transform=lambda y: torch.tensor(y[0]))\n",
    "\n",
    "    new_loader = DataLoader(train_data_noatt + data_train_out, batch_size=128, shuffle=True, num_workers=10, worker_init_fn=seed_worker)\n",
    "    new_test_loader = DataLoader(test_data_noatt, batch_size=128, shuffle=False, num_workers=10, worker_init_fn=seed_worker)\n",
    "\n",
    "    model = Model(num_classes=2).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9, nesterov=True)\n",
    "\n",
    "    accs = []\n",
    "\n",
    "    for epoch in range(1):\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        bar = tqdm(new_loader)\n",
    "        for inputs, y in bar:\n",
    "            labels = is_known(y).long()\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss = 0.8 * running_loss + 0.2 * loss.item()\n",
    "            bar.set_postfix({\"loss\": running_loss})\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            for inputs, y in new_test_loader:\n",
    "                labels = is_known(y).long()\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Accuracy of the sign network on the test images: {correct / total:.2%}')\n",
    "        accs.append(correct / total)\n",
    "\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "# train_sign_model()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7089c25",
   "metadata": {
    "tags": []
   },
   "source": [
    "# OOD Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d017e08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from importlib import reload\n",
    "\n",
    "import detectors\n",
    "reload(detectors)\n",
    "\n",
    "from pytorch_ood.dataset.img import (LSUNCrop, LSUNResize, Textures, TinyImageNetCrop, TinyImageNetResize)\n",
    "from pytorch_ood.detector import EnergyBased, MaxSoftmax, ReAct, MaxLogit, Entropy, Mahalanobis, ViM\n",
    "from pytorch_ood.utils import ToRGB, OODMetrics\n",
    "\n",
    "from detectors import PrologOODT\n",
    "\n",
    "def evaluate(label_net, shape_net, color_net, shield_net):\n",
    "    _ = label_net.eval()\n",
    "    _ = shape_net.eval()\n",
    "    _ = color_net.eval()\n",
    "    _ = shield_net.eval()\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    data_in = GTSRB(root=root, train=False, transforms=trans, target_transform=lambda y: y[0])\n",
    "    # dataset_out_test = Textures(root=root, transform=trans, target_transform=ToUnknown(), download=True)\n",
    "\n",
    "    detectors = {\n",
    "        \"PrologOOD\": PrologOOD(\n",
    "            \"kb.pl\",\n",
    "            label_net=label_net,\n",
    "            shape_net=shape_net,\n",
    "            color_net=color_net,\n",
    "            label_file=\"../data/GTSRB/labels.txt\",\n",
    "        ),\n",
    "        \"PrologOOD+\": PrologOOD(\n",
    "            \"kb.pl\",\n",
    "            label_net,\n",
    "            shape_net,\n",
    "            color_net,\n",
    "            sign_net=shield_net,\n",
    "            label_file=\"../data/GTSRB/labels.txt\",\n",
    "        ),\n",
    "        \"PrologOODT\": PrologOODT(\n",
    "            \"kb.pl\",\n",
    "            label_net=label_net,\n",
    "            shape_net=shape_net,\n",
    "            color_net=color_net,\n",
    "            label_file=\"../data/GTSRB/labels.txt\",\n",
    "        ),\n",
    "        \"PrologOODT+\": PrologOODT(\n",
    "            \"kb.pl\",\n",
    "            label_net,\n",
    "            shape_net,\n",
    "            color_net,\n",
    "            sign_net=shield_net,\n",
    "            label_file=\"../data/GTSRB/labels.txt\",\n",
    "        ),\n",
    "        \"Logic\": LogicOOD(\n",
    "            label_net,\n",
    "            shape_net,\n",
    "            color_net,\n",
    "            data_in.class_to_shape,\n",
    "            data_in.class_to_color,\n",
    "        ),\n",
    "        \"Logic+\": LogicOOD(\n",
    "            label_net,\n",
    "            shape_net,\n",
    "            color_net,\n",
    "            data_in.class_to_shape,\n",
    "            data_in.class_to_color,\n",
    "            sign_net=shield_net,\n",
    "        ).consistent,\n",
    "        \"Ensemble\": EnsembleDetector(label_net, shape_net, color_net),\n",
    "        \"MSP\": MaxSoftmax(label_net),\n",
    "        \"Energy\": EnergyBased(label_net),\n",
    "        \"ReAct\": ReAct(label_net.features, label_net.fc, threshold=10.0),\n",
    "        \"ViM\": ViM(label_net.features, w=label_net.fc.weight, b=label_net.fc.bias, d=64),\n",
    "        \"Mahalanobis\": Mahalanobis(label_net.features),\n",
    "        \"Entropy\": Entropy(label_net),\n",
    "        \"MaxLogit\": MaxLogit(label_net),\n",
    "    }\n",
    "    data = GTSRB(root=root, train=True, transforms=trans, target_transform=lambda y: torch.tensor(y[0]))\n",
    "    _ , val_data  = random_split(data, [35000, 4209], generator=torch.Generator().manual_seed(123))\n",
    "    label_loader = DataLoader(val_data, batch_size=128, shuffle=False, worker_init_fn=seed_worker, num_workers=10)\n",
    "\n",
    "    data = GTSRB(root=root, train=True, transforms=trans, target_transform=lambda y: torch.tensor(y[1]))\n",
    "    _ , val_data  = random_split(data, [35000, 4209], generator=torch.Generator().manual_seed(123))\n",
    "    color_loader = DataLoader(val_data, batch_size=128, shuffle=False, worker_init_fn=seed_worker, num_workers=10)\n",
    "\n",
    "    data = GTSRB(root=root, train=True, transforms=trans, target_transform=lambda y: torch.tensor(y[2]))\n",
    "    _ , val_data  = random_split(data, [35000, 4209], generator=torch.Generator().manual_seed(123))\n",
    "    shape_loader = DataLoader(val_data, batch_size=128, shuffle=False, worker_init_fn=seed_worker, num_workers=10)\n",
    "\n",
    "    detectors[\"ViM\"].fit(label_loader, device=device)\n",
    "    detectors[\"Mahalanobis\"].fit(label_loader, device=device)\n",
    "    detectors[\"PrologOODT\"].fit(label_loader, color_loader, shape_loader, device=device)\n",
    "    detectors[\"PrologOODT+\"].fit(label_loader, color_loader, shape_loader, device=device)\n",
    "\n",
    "    datasets = {d.__name__: d for d in (LSUNCrop, LSUNResize, Textures, TinyImageNetCrop, TinyImageNetResize)}\n",
    "    \n",
    "    for detector_name, detector in detectors.items():\n",
    "        for data_name, dataset_c in datasets.items():\n",
    "            data_out = dataset_c(root=root, transform=trans, target_transform=ToUnknown(), download=True)\n",
    "            loader = DataLoader(data_in+data_out, batch_size=128, shuffle=False, worker_init_fn=seed_worker, num_workers=10)\n",
    "            \n",
    "            scores = []\n",
    "            ys = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for x, y in loader:\n",
    "                    scores.append(detector(x.to(device)))\n",
    "                    ys.append(y.to(device))\n",
    "                    \n",
    "                scores = torch.cat(scores, dim=0).cpu()\n",
    "                ys = torch.cat(ys, dim=0).cpu()\n",
    "            \n",
    "            metrics = OODMetrics()\n",
    "            metrics.update(scores, ys)\n",
    "            r = metrics.compute()\n",
    "            r.update({\n",
    "                \"Method\": detector_name,\n",
    "                \"Dataset\": data_name\n",
    "            })\n",
    "            print(r)\n",
    "            results.append(r)\n",
    "    \n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8ec3dec",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_acc(net, att_idx=0, oe=False):\n",
    "    _ = net.eval()\n",
    "    \n",
    "    if oe:\n",
    "        target_trans = lambda y: torch.tensor(1)\n",
    "    else:\n",
    "         target_trans = lambda y: y[att_idx]\n",
    "\n",
    "    trans = Compose([Resize(size=(32, 32), antialias=True), ToRGB(), ToTensor()])\n",
    "    data_in = GTSRB(root=root, train=False, transforms=trans, target_transform=target_trans)\n",
    "    loader = DataLoader(data_in, batch_size=1024, shuffle=False, worker_init_fn=seed_worker)\n",
    "            \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            predicted = outputs.max(dim=1).indices\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return correct / total  \n",
    "\n",
    "def evaluate_accs(label_net, shape_net, color_net, shield_net):\n",
    "    r = {}\n",
    "    names = (\"Label\", \"Color\", \"Shape\",)\n",
    "    \n",
    "    for n, net in enumerate((label_net, color_net, shape_net)): \n",
    "        acc = evaluate_acc(net, n)\n",
    "        r[names[n]] = acc\n",
    "    \n",
    "    acc = evaluate_acc(shield_net, oe=True)\n",
    "    r[\"Sign\"] = acc\n",
    "    \n",
    "    return [r] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04ea9133",
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\n",
    "results_acc = []\n",
    "\n",
    "for trial in range(10):\n",
    "    shield_net = train_sign_model()\n",
    "    shape_net = train_model(att_index=2, num_classes=5)\n",
    "    color_net = train_model(att_index=1, num_classes=4)\n",
    "    label_net = train_model(att_index=0, num_classes=43)\n",
    "    \n",
    "    res = evaluate(label_net, shape_net, color_net, shield_net)\n",
    "    res_acc = evaluate_accs(label_net, shape_net, color_net, shield_net)\n",
    "    \n",
    "    for r in res:\n",
    "        r.update({\"Seed\": trial})\n",
    "        \n",
    "    for r in res_acc:\n",
    "        r.update({\"Seed\": trial})\n",
    "    \n",
    "    results += res\n",
    "    results_acc += res_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71390c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "result_df = pd.DataFrame(results)\n",
    "\n",
    "\n",
    "order = ['MSP', 'Energy', 'MaxLogit', 'Entropy', 'ReAct', 'Mahalanobis', 'ViM', 'Ensemble', 'Logic', 'Logic+', 'LogicOOD', 'LogicOOD+']\n",
    "\n",
    "\n",
    "print((result_df.groupby(by=[\"Method\", \"Seed\"]).mean() * 100).groupby(\"Method\").agg([\"mean\", \"sem\"]).reindex(order).to_latex(float_format=\"%.2f\").replace(\"& 0.\", \"& $\\pm$ 0.\").replace(\"& 2.\", \"& $\\pm$ 2.\"))\n",
    "\n",
    "# print((result_df.groupby(by=\"Method\").agg([\"mean\", \"sem\"]) * 100)[[\"AUROC\", \"AUPR-IN\", \"AUPR-OUT\", \"FPR95TPR\"]].to_latex(float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "219460b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "sem_auroc = result_df[result_df[\"Method\"] == \"PrologOOD\"].groupby(by=[\"Method\", \"Seed\"]).mean()[\"AUROC\"]\n",
    "sem_ensemble =  result_df[result_df[\"Method\"] == \"Ensemble\"].groupby(by=[\"Method\", \"Seed\"]).mean()[\"AUROC\"]\n",
    "\n",
    "print(ttest_ind(sem_auroc, sem_ensemble, equal_var=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b7b03a3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print((pd.DataFrame(results_acc) * 100).agg([\"mean\", \"sem\"]).to_latex(float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "s = (result_df.groupby(by=\"Method\").agg([\"mean\", \"sem\"]) * 100)[[\"AUROC\", \"AUPR-IN\", \"AUPR-OUT\", \"FPR95TPR\"]].to_latex(float_format=\"%.2f\")\n",
    "\n",
    "print(s.replace(\"& 0.\", \"& $\\pm$ 0.\").replace(\"& 1.\", \"& $\\pm$ 1.\").replace(\"& 2.\", \"& $\\pm$ 2.\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "myenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
