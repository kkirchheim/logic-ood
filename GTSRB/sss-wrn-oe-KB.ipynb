{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Street sign with Pre-Trained WideResNet\n",
    "\n",
    "With additional shield net \n",
    "\n",
    "with prolog KB \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f67e6266df0>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from torch.utils.data import Dataset, DataLoader\n",
    "from os.path import join\n",
    "import pandas as pd \n",
    "from PIL import Image\n",
    "from torch.optim import SGD\n",
    "import seaborn as sb \n",
    "from gtsrb import GTSRB\n",
    "from detectors import EnsembleDetector, KBDetector\n",
    "\n",
    "sb.set()\n",
    "\n",
    "device=\"cuda:0\"\n",
    "root = \"data/\"\n",
    "\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "\n",
    "import numpy as np\n",
    "np.random.seed(0)\n",
    "\n",
    "import random\n",
    "random.seed(0)\n",
    "\n",
    "torch.use_deterministic_algorithms(False)\n",
    "\n",
    "def seed_worker(worker_id):\n",
    "    worker_seed = torch.initial_seed() % 2**32\n",
    "    np.random.seed(worker_seed)\n",
    "    random.seed(worker_seed)\n",
    "\n",
    "g = torch.Generator()\n",
    "g.manual_seed(0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torchvision\n",
    "from torchvision.transforms import ToTensor, Resize, Compose\n",
    "import torch \n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "trans = Compose([ToTensor(), Resize((32, 32))])\n",
    "\n",
    "train_data = GTSRB(root=root, train=True, transforms=trans)\n",
    "test_data = GTSRB(root=root, train=False, transforms=trans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2, worker_init_fn=seed_worker)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False, num_workers=2, worker_init_fn=seed_worker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchvision.models.resnet import resnet18\n",
    "from pytorch_ood.model import WideResNet\n",
    "\n",
    "# def override \n",
    "def Model(num_classes=None, *args, **kwargs):\n",
    "    model = WideResNet(*args, num_classes=1000, **kwargs, pretrained=\"imagenet32\")\n",
    "    model.fc = nn.Linear(model.fc.in_features, num_classes)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm \n",
    "import numpy as np \n",
    "\n",
    "def train_model(att_index, num_classes):\n",
    "    \"\"\"\n",
    "    train a model for the given attribute index \n",
    "    \"\"\"\n",
    "    trans = Compose([ToTensor(), Resize((32, 32))])\n",
    "    train_data = GTSRB(root=root, train=True, transforms=trans)\n",
    "    test_data = GTSRB(root=root, train=False, transforms=trans)\n",
    "    \n",
    "    train_loader = DataLoader(train_data, batch_size=32, shuffle=True, num_workers=2, worker_init_fn=seed_worker)\n",
    "    test_loader = DataLoader(test_data, batch_size=32, shuffle=False, num_workers=2, worker_init_fn=seed_worker)\n",
    "    \n",
    "    model = Model(num_classes=num_classes).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9, nesterov=True)\n",
    "\n",
    "    accs = []\n",
    "\n",
    "    for epoch in range(1):\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        bar = tqdm(train_loader)\n",
    "        for inputs, y in bar:\n",
    "            labels = y[:, att_index]\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss = 0.8 * running_loss + 0.2 * loss.item()\n",
    "            bar.set_postfix({\"loss\": running_loss})\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            for inputs, y in test_loader:\n",
    "                labels = y[:, att_index]\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, dim=1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Accuracy of the network on the test images: {correct / total:.2%}')\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sign Network "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from pytorch_ood.utils import is_known\n",
    "from tqdm.notebook import tqdm \n",
    "from pytorch_ood.dataset.img import TinyImages300k\n",
    "from pytorch_ood.utils import ToUnknown\n",
    "from torch.utils.data import random_split\n",
    "\n",
    "def train_sign_model():\n",
    "    tiny = TinyImages300k(root=root, download=True, transform=trans, target_transform=ToUnknown())\n",
    "    data_train_out, data_test_out, _ = random_split(tiny, [50000, 10000, 240000], generator=torch.Generator().manual_seed(123))\n",
    "\n",
    "    train_data_noatt = GTSRB(root=root, train=True, transforms=trans, target_transform=lambda y: y[0])\n",
    "    test_data_noatt = GTSRB(root=root, train=False, transforms=trans, target_transform=lambda y: y[0])\n",
    "\n",
    "    new_loader = DataLoader(train_data_noatt + data_train_out, batch_size=32, shuffle=True, num_workers=10, worker_init_fn=seed_worker)\n",
    "    new_test_loader = DataLoader(test_data_noatt + data_test_out, batch_size=32, shuffle=False, num_workers=10, worker_init_fn=seed_worker)\n",
    "\n",
    "    model = Model(num_classes=2).to(device)\n",
    "\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(model.parameters(), lr=0.001, momentum=0.9, nesterov=True)\n",
    "\n",
    "    accs = []\n",
    "\n",
    "    for epoch in range(1):\n",
    "        running_loss = 0.0\n",
    "        model.train()\n",
    "        \n",
    "        bar = tqdm(new_loader)\n",
    "        for inputs, y in bar:\n",
    "            labels = is_known(y).long()\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss = 0.8 * running_loss + 0.2 * loss.item()\n",
    "            bar.set_postfix({\"loss\": running_loss})\n",
    "\n",
    "        correct = 0\n",
    "        total = 0\n",
    "\n",
    "        with torch.no_grad():\n",
    "            model.eval()\n",
    "\n",
    "            for inputs, y in new_test_loader:\n",
    "                labels = is_known(y).long()\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                _, predicted = torch.max(outputs.data, 1)\n",
    "                total += labels.size(0)\n",
    "                correct += (predicted == labels).sum().item()\n",
    "\n",
    "        print(f'Accuracy of the shape network on the test images: {correct / total:.2%}')\n",
    "        accs.append(correct / total)\n",
    "\n",
    "    return model "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# OOD Evaluation "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_ood.dataset.img import (LSUNCrop, LSUNResize, Textures, TinyImageNetCrop, TinyImageNetResize)\n",
    "from pytorch_ood.detector import EnergyBased, MaxSoftmax\n",
    "from pytorch_ood.utils import ToRGB, OODMetrics\n",
    "\n",
    "def evaluate(label_net, shape_net, color_net, shield_net):\n",
    "    _ = label_net.eval()\n",
    "    _ = shape_net.eval()\n",
    "    _ = color_net.eval()\n",
    "    _ = shield_net.eval()\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    trans = Compose([Resize(size=(32, 32)), ToRGB(), ToTensor()])\n",
    "    data_in = GTSRB(root=root, train=False, transforms=trans, target_transform=lambda y: y[0])\n",
    "\n",
    "    detectors = {\n",
    "        \"MSP\": MaxSoftmax(label_net),\n",
    "        \"Energy\": EnergyBased(label_net),\n",
    "        \"Ensemble\": EnsembleDetector(label_net, shape_net, color_net),\n",
    "        \"Semantic\": KBDetector(\"knowledge_base.pl\", label_net, shape_net, color_net),\n",
    "        \"Semantic-OE\": KBDetector(\"knowledge_base.pl\", label_net, shape_net, color_net, sign_net=shield_net),\n",
    "    }\n",
    "    \n",
    "    datasets = {d.__name__: d for d in (LSUNCrop, LSUNResize, Textures, TinyImageNetCrop, TinyImageNetResize)}\n",
    "    \n",
    "    for detector_name, detector in detectors.items():\n",
    "        for data_name, dataset_c in datasets.items():\n",
    "            data_out = dataset_c(root=root, transform=trans, target_transform=ToUnknown(), download=True)\n",
    "            loader = DataLoader(data_in+data_out, batch_size=1024, shuffle=False, worker_init_fn=seed_worker)\n",
    "            \n",
    "            scores = []\n",
    "            ys = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for x, y in loader:\n",
    "                    scores.append(detector(x.to(device)))\n",
    "                    ys.append(y.to(device))\n",
    "                    \n",
    "                scores = torch.cat(scores, dim=0).cpu()\n",
    "                ys = torch.cat(ys, dim=0).cpu()\n",
    "            \n",
    "            metrics = OODMetrics()\n",
    "            metrics.update(scores, ys)\n",
    "            r = metrics.compute()\n",
    "            r.update({\n",
    "                \"Method\": detector_name,\n",
    "                \"Dataset\": data_name\n",
    "            })\n",
    "            print(r)\n",
    "            results.append(r)\n",
    "    \n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_acc(net, att_idx=0, oe=False):\n",
    "    _ = net.eval()\n",
    "    \n",
    "    if oe:\n",
    "        target_trans = lambda y: torch.tensor(1)\n",
    "    else:\n",
    "         target_trans = lambda y: y[att_idx]\n",
    "\n",
    "    trans = Compose([Resize(size=(32, 32)), ToRGB(), ToTensor()])\n",
    "    data_in = GTSRB(root=root, train=False, transforms=trans, target_transform=target_trans)\n",
    "    loader = DataLoader(data_in, batch_size=1024, shuffle=False, worker_init_fn=seed_worker)\n",
    "            \n",
    "    correct = 0\n",
    "    total = 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in loader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "            outputs = net(inputs)\n",
    "            predicted = outputs.max(dim=1).indices\n",
    "            total += labels.size(0)\n",
    "            correct += (predicted == labels).sum().item()\n",
    "    \n",
    "    return correct / total  \n",
    "\n",
    "def evaluate_accs(label_net, shape_net, color_net, shield_net):\n",
    "    r = {}\n",
    "    names = (\"Label\", \"Color\", \"Shape\",)\n",
    "    \n",
    "    for n, net in enumerate((label_net, color_net, shape_net)): \n",
    "        acc = evaluate_acc(net, n)\n",
    "        r[names[n]] = acc\n",
    "    \n",
    "    acc = evaluate_acc(shield_net, oe=True)\n",
    "    r[\"Sign\"] = acc\n",
    "    \n",
    "    return [r] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# %%time \n",
    "\n",
    "# _ = label_net.eval()\n",
    "# _ = shape_net.eval()\n",
    "# _ = color_net.eval()\n",
    "# _ = shield_net.eval()\n",
    "\n",
    "# results = []\n",
    "\n",
    "# trans = Compose([Resize(size=(32, 32)), ToRGB(), ToTensor()])\n",
    "# data_in = GTSRB(root=root, train=False, transforms=trans, target_transform=lambda y: y[0])\n",
    "\n",
    "# detectors = {\n",
    "#     \"abc\": KBDetector(\"knowledge_base.pl\", label_net, shape_net, color_net, sign_net=shield_net)\n",
    "# }\n",
    "\n",
    "# datasets = {d.__name__: d for d in (Textures,)}\n",
    "\n",
    "# for detector_name, detector in detectors.items():\n",
    "#     for data_name, dataset_c in datasets.items():\n",
    "#         data_out = dataset_c(root=root, transform=trans, target_transform=ToUnknown(), download=True)\n",
    "#         loader = DataLoader(data_in+data_out, batch_size=1024, shuffle=False, worker_init_fn=seed_worker)\n",
    "\n",
    "#         scores = []\n",
    "#         ys = []\n",
    "\n",
    "#         with torch.no_grad():\n",
    "#             for x, y in loader:\n",
    "#                 scores.append(detector(x.to(device)))\n",
    "#                 ys.append(y.to(device))\n",
    "\n",
    "#             scores = torch.cat(scores, dim=0).cpu()\n",
    "#             ys = torch.cat(ys, dim=0).cpu()\n",
    "\n",
    "#         metrics = OODMetrics()\n",
    "#         metrics.update(scores, ys)\n",
    "#         r = metrics.compute()\n",
    "#         r.update({\n",
    "#             \"Method\": detector_name,\n",
    "#             \"Dataset\": data_name\n",
    "#         })\n",
    "#         print(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "06e44badada5476291e8e26a8aa2463b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/2788 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the shape network on the test images: 99.83%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3a4cc8b95dff4c59b69acbcf34f1d2f0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1226 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.89%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "150ee1a9e987449caf6deca556b4b6dc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1226 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 99.31%\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "280e2386ac52450a873b416b87a56e6f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1226 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of the network on the test images: 97.29%\n",
      "{'AUROC': 0.9830532670021057, 'AUPR-IN': 0.9725900888442993, 'AUPR-OUT': 0.9891340136528015, 'ACC95TPR': 0.9461334347724915, 'FPR95TPR': 0.05692794919013977, 'Method': 'MSP', 'Dataset': 'LSUNCrop'}\n",
      "{'AUROC': 0.9915062189102173, 'AUPR-IN': 0.9864577651023865, 'AUPR-OUT': 0.9944612979888916, 'ACC95TPR': 0.9587273597717285, 'FPR95TPR': 0.03436262905597687, 'Method': 'MSP', 'Dataset': 'LSUNResize'}\n",
      "{'AUROC': 0.9812435507774353, 'AUPR-IN': 0.9512006640434265, 'AUPR-OUT': 0.9920799136161804, 'ACC95TPR': 0.9400109648704529, 'FPR95TPR': 0.0644497200846672, 'Method': 'MSP', 'Dataset': 'Textures'}\n",
      "{'AUROC': 0.9851004481315613, 'AUPR-IN': 0.9781304001808167, 'AUPR-OUT': 0.9893771409988403, 'ACC95TPR': 0.9462218284606934, 'FPR95TPR': 0.05676959455013275, 'Method': 'MSP', 'Dataset': 'TinyImageNetCrop'}\n",
      "{'AUROC': 0.9893456697463989, 'AUPR-IN': 0.9840818047523499, 'AUPR-OUT': 0.9926323294639587, 'ACC95TPR': 0.9539549350738525, 'FPR95TPR': 0.04291369765996933, 'Method': 'MSP', 'Dataset': 'TinyImageNetResize'}\n",
      "{'AUROC': 0.9934593439102173, 'AUPR-IN': 0.9888589382171631, 'AUPR-OUT': 0.9956967234611511, 'ACC95TPR': 0.966946542263031, 'FPR95TPR': 0.01963578723371029, 'Method': 'Energy', 'Dataset': 'LSUNCrop'}\n",
      "{'AUROC': 0.9966558218002319, 'AUPR-IN': 0.9941413998603821, 'AUPR-OUT': 0.9978291392326355, 'ACC95TPR': 0.971718966960907, 'FPR95TPR': 0.011084718629717827, 'Method': 'Energy', 'Dataset': 'LSUNResize'}\n",
      "{'AUROC': 0.9870169758796692, 'AUPR-IN': 0.9679931402206421, 'AUPR-OUT': 0.9937430620193481, 'ACC95TPR': 0.9623426198959351, 'FPR95TPR': 0.03214568644762039, 'Method': 'Energy', 'Dataset': 'Textures'}\n",
      "{'AUROC': 0.9933167099952698, 'AUPR-IN': 0.9902018308639526, 'AUPR-OUT': 0.9950221180915833, 'ACC95TPR': 0.9663720726966858, 'FPR95TPR': 0.020665083080530167, 'Method': 'Energy', 'Dataset': 'TinyImageNetCrop'}\n",
      "{'AUROC': 0.9939996004104614, 'AUPR-IN': 0.9904673099517822, 'AUPR-OUT': 0.9956324696540833, 'ACC95TPR': 0.9687582850456238, 'FPR95TPR': 0.016389548778533936, 'Method': 'Energy', 'Dataset': 'TinyImageNetResize'}\n",
      "{'AUROC': 0.9960477948188782, 'AUPR-IN': 0.993581235408783, 'AUPR-OUT': 0.9974544048309326, 'ACC95TPR': 0.969509482383728, 'FPR95TPR': 0.015043547376990318, 'Method': 'Ensemble', 'Dataset': 'LSUNCrop'}\n",
      "{'AUROC': 0.9984588623046875, 'AUPR-IN': 0.9977302551269531, 'AUPR-OUT': 0.9989547729492188, 'ACC95TPR': 0.9745912551879883, 'FPR95TPR': 0.005938242189586163, 'Method': 'Ensemble', 'Dataset': 'LSUNResize'}\n",
      "{'AUROC': 0.9951267838478088, 'AUPR-IN': 0.9871864318847656, 'AUPR-OUT': 0.9980133175849915, 'ACC95TPR': 0.9687466025352478, 'FPR95TPR': 0.02288202755153179, 'Method': 'Ensemble', 'Dataset': 'Textures'}\n",
      "{'AUROC': 0.9968777894973755, 'AUPR-IN': 0.9954547882080078, 'AUPR-OUT': 0.99785315990448, 'ACC95TPR': 0.9702165126800537, 'FPR95TPR': 0.013776722364127636, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetCrop'}\n",
      "{'AUROC': 0.9975943565368652, 'AUPR-IN': 0.9965231418609619, 'AUPR-OUT': 0.9983363151550293, 'ACC95TPR': 0.9725143909454346, 'FPR95TPR': 0.009659540839493275, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetResize'}\n",
      "{'AUROC': 0.9941208362579346, 'AUPR-IN': 0.9916021823883057, 'AUPR-OUT': 0.9969727396965027, 'ACC95TPR': 0.9719399213790894, 'FPR95TPR': 0.010688835754990578, 'Method': 'Semantic', 'Dataset': 'LSUNCrop'}\n",
      "{'AUROC': 0.9937936067581177, 'AUPR-IN': 0.9898896217346191, 'AUPR-OUT': 0.996864914894104, 'ACC95TPR': 0.9723376035690308, 'FPR95TPR': 0.009976247325539589, 'Method': 'Semantic', 'Dataset': 'LSUNResize'}\n",
      "{'AUROC': 0.9927423000335693, 'AUPR-IN': 0.9807603359222412, 'AUPR-OUT': 0.9974292516708374, 'ACC95TPR': 0.9725232720375061, 'FPR95TPR': 0.017418844625353813, 'Method': 'Semantic', 'Dataset': 'Textures'}\n",
      "{'AUROC': 0.9930673241615295, 'AUPR-IN': 0.9890598654747009, 'AUPR-OUT': 0.9962892532348633, 'ACC95TPR': 0.9708793759346008, 'FPR95TPR': 0.012589073739945889, 'Method': 'Semantic', 'Dataset': 'TinyImageNetCrop'}\n",
      "{'AUROC': 0.9935443997383118, 'AUPR-IN': 0.9898350834846497, 'AUPR-OUT': 0.9966383576393127, 'ACC95TPR': 0.971895694732666, 'FPR95TPR': 0.010768013074994087, 'Method': 'Semantic', 'Dataset': 'TinyImageNetResize'}\n",
      "{'AUROC': 0.9940972924232483, 'AUPR-IN': 0.9915755987167358, 'AUPR-OUT': 0.9969534277915955, 'ACC95TPR': 0.9719840884208679, 'FPR95TPR': 0.010609659366309643, 'Method': 'Semantic-OE', 'Dataset': 'LSUNCrop'}\n",
      "{'AUROC': 0.9937769174575806, 'AUPR-IN': 0.9898694157600403, 'AUPR-OUT': 0.9968518614768982, 'ACC95TPR': 0.9723376035690308, 'FPR95TPR': 0.009976247325539589, 'Method': 'Semantic-OE', 'Dataset': 'LSUNResize'}\n",
      "{'AUROC': 0.9927517175674438, 'AUPR-IN': 0.9807560443878174, 'AUPR-OUT': 0.997435986995697, 'ACC95TPR': 0.9723043441772461, 'FPR95TPR': 0.017735550180077553, 'Method': 'Semantic-OE', 'Dataset': 'Textures'}\n",
      "{'AUROC': 0.9930219650268555, 'AUPR-IN': 0.9890059232711792, 'AUPR-OUT': 0.996253490447998, 'ACC95TPR': 0.9707909822463989, 'FPR95TPR': 0.012747426517307758, 'Method': 'Semantic-OE', 'Dataset': 'TinyImageNetCrop'}\n",
      "{'AUROC': 0.993511974811554, 'AUPR-IN': 0.9897969961166382, 'AUPR-OUT': 0.9966123104095459, 'ACC95TPR': 0.971895694732666, 'FPR95TPR': 0.010768013074994087, 'Method': 'Semantic-OE', 'Dataset': 'TinyImageNetResize'}\n"
     ]
    }
   ],
   "source": [
    "results = []\n",
    "results_acc = []\n",
    "\n",
    "for trial in range(1):\n",
    "    shield_net = train_sign_model()\n",
    "    shape_net = train_model(att_index=2, num_classes=5)\n",
    "    color_net = train_model(att_index=1, num_classes=4)\n",
    "    label_net = train_model(att_index=0, num_classes=43)\n",
    "    \n",
    "    res = evaluate(label_net, shape_net, color_net, shield_net)\n",
    "    res_acc = evaluate_accs(label_net, shape_net, color_net, shield_net)\n",
    "    \n",
    "    for r in res:\n",
    "        r.update({\"Seed\": trial})\n",
    "        \n",
    "    for r in res_acc:\n",
    "        r.update({\"Seed\": trial})\n",
    "    \n",
    "    results += res\n",
    "    results_acc += res_acc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrrrrr}\n",
      "\\toprule\n",
      "{} & \\multicolumn{2}{l}{AUROC} & \\multicolumn{2}{l}{AUPR-IN} & \\multicolumn{2}{l}{AUPR-OUT} & \\multicolumn{2}{l}{FPR95TPR} \\\\\n",
      "{} &  mean &  sem &    mean &  sem &     mean &  sem &     mean &  sem \\\\\n",
      "Method      &       &      &         &      &          &      &          &      \\\\\n",
      "\\midrule\n",
      "Energy      & 99.29 & 0.16 &   98.63 & 0.47 &    99.56 & 0.07 &     2.00 & 0.35 \\\\\n",
      "Ensemble    & 99.68 & 0.06 &   99.41 & 0.19 &    99.81 & 0.03 &     1.35 & 0.28 \\\\\n",
      "MSP         & 98.60 & 0.19 &   97.45 & 0.63 &    99.15 & 0.10 &     5.11 & 0.54 \\\\\n",
      "Semantic    & 99.35 & 0.02 &   98.82 & 0.19 &    99.68 & 0.02 &     1.23 & 0.14 \\\\\n",
      "Semantic-OE & 99.34 & 0.02 &   98.82 & 0.19 &    99.68 & 0.02 &     1.24 & 0.14 \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_458462/2493558079.py:3: FutureWarning: ['Dataset'] did not aggregate successfully. If any error is raised this will raise in a future version of pandas. Drop these columns/ops to avoid this warning.\n",
      "  print((result_df.groupby(by=\"Method\").agg([\"mean\", \"sem\"]) * 100)[[\"AUROC\", \"AUPR-IN\", \"AUPR-OUT\", \"FPR95TPR\"]].to_latex(float_format=\"%.2f\"))\n",
      "/tmp/ipykernel_458462/2493558079.py:3: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print((result_df.groupby(by=\"Method\").agg([\"mean\", \"sem\"]) * 100)[[\"AUROC\", \"AUPR-IN\", \"AUPR-OUT\", \"FPR95TPR\"]].to_latex(float_format=\"%.2f\"))\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd \n",
    "result_df = pd.DataFrame(results)\n",
    "print((result_df.groupby(by=\"Method\").agg([\"mean\", \"sem\"]) * 100)[[\"AUROC\", \"AUPR-IN\", \"AUPR-OUT\", \"FPR95TPR\"]].to_latex(float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ttest_indResult(statistic=nan, pvalue=nan)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_458462/299598497.py:3: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  sem_auroc = result_df[result_df[\"Method\"] == \"Semantic\"].groupby(by=[\"Method\", \"Seed\"]).mean()[\"AUROC\"]\n",
      "/tmp/ipykernel_458462/299598497.py:4: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  sem_ensemble =  result_df[result_df[\"Method\"] == \"Ensemble\"].groupby(by=[\"Method\", \"Seed\"]).mean()[\"AUROC\"]\n",
      "/tmp/ipykernel_458462/299598497.py:6: RuntimeWarning: Precision loss occurred in moment calculation due to catastrophic cancellation. This occurs when the data are nearly identical. Results may be unreliable.\n",
      "  print(ttest_ind(sem_auroc, sem_ensemble, equal_var=False))\n",
      "/home/kirchheim/anaconda3/envs/logic-ood/lib/python3.10/site-packages/scipy/stats/_stats_py.py:1253: RuntimeWarning: divide by zero encountered in divide\n",
      "  var *= np.divide(n, n-ddof)  # to avoid error on division by zero\n",
      "/home/kirchheim/anaconda3/envs/logic-ood/lib/python3.10/site-packages/scipy/stats/_stats_py.py:1253: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  var *= np.divide(n, n-ddof)  # to avoid error on division by zero\n"
     ]
    }
   ],
   "source": [
    "from scipy.stats import ttest_ind\n",
    "\n",
    "sem_auroc = result_df[result_df[\"Method\"] == \"Semantic\"].groupby(by=[\"Method\", \"Seed\"]).mean()[\"AUROC\"]\n",
    "sem_ensemble =  result_df[result_df[\"Method\"] == \"Ensemble\"].groupby(by=[\"Method\", \"Seed\"]).mean()[\"AUROC\"]\n",
    "\n",
    "print(ttest_ind(sem_auroc, sem_ensemble, equal_var=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\\begin{tabular}{lrrrrr}\n",
      "\\toprule\n",
      "{} &  Label &  Color &  Shape &  Sign &  Seed \\\\\n",
      "\\midrule\n",
      "mean &  97.18 &  99.35 &  99.90 & 99.87 &  0.00 \\\\\n",
      "sem  &    NaN &    NaN &    NaN &   NaN &   NaN \\\\\n",
      "\\bottomrule\n",
      "\\end{tabular}\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_458462/3946649051.py:1: FutureWarning: In future versions `DataFrame.to_latex` is expected to utilise the base implementation of `Styler.to_latex` for formatting and rendering. The arguments signature may therefore change. It is recommended instead to use `DataFrame.style.to_latex` which also contains additional functionality.\n",
      "  print((pd.DataFrame(results_acc) * 100).agg([\"mean\", \"sem\"]).to_latex(float_format=\"%.2f\"))\n"
     ]
    }
   ],
   "source": [
    "print((pd.DataFrame(results_acc) * 100).agg([\"mean\", \"sem\"]).to_latex(float_format=\"%.2f\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def evaluate2(label_net, shape_net, color_net, shield_net):\n",
    "    _ = label_net.eval()\n",
    "    _ = shape_net.eval()\n",
    "    _ = color_net.eval()\n",
    "    _ = shield_net.eval()\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    trans = Compose([Resize(size=(32, 32)), ToRGB(), ToTensor()])\n",
    "    data_in = GTSRB(root=root, train=False, transforms=trans, target_transform=lambda y: y[0])\n",
    "    # dataset_out_test = Textures(root=root, transform=trans, target_transform=ToUnknown(), download=True)\n",
    "\n",
    "    detectors = {\n",
    "        \"MSP\": MaxSoftmax(label_net),\n",
    "        \"Energy\": EnergyBased(label_net),\n",
    "        \"Ensemble\": EnsembleDetector(label_net, shape_net, color_net),\n",
    "        \"Semantic\": KBDetector(\"knowledge_base.pl\", label_net, shape_net, color_net),\n",
    "        \"Semantic-OE\": KBDetector(\"knowledge_base.pl\", label_net, shape_net, color_net, sign_net=shield_net),\n",
    "    }\n",
    "    \n",
    "    datasets = {d.__name__: d for d in (LSUNCrop, LSUNResize, Textures, TinyImageNetCrop, TinyImageNetResize)}\n",
    "    \n",
    "    for detector_name, detector in detectors.items():\n",
    "        for data_name, dataset_c in datasets.items():\n",
    "            data_out = dataset_c(root=root, transform=trans, target_transform=ToUnknown(), download=True)\n",
    "            loader = DataLoader(data_in+data_out, batch_size=1024, shuffle=False, worker_init_fn=seed_worker)\n",
    "            \n",
    "            scores = []\n",
    "            ys = []\n",
    "            ys_hat = []\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                for x, y in loader:\n",
    "                    y_hat = label_net(x.to(device)).max(dim=1).indices\n",
    "                    ys_hat.append(y_hat)\n",
    "                    scores.append(detector(x.to(device)))\n",
    "                    ys.append(y.to(device))\n",
    "                    \n",
    "                scores = torch.cat(scores, dim=0).cpu()\n",
    "                ys = torch.cat(ys, dim=0).cpu()\n",
    "                ys_hat = torch.cat(ys_hat, dim=0).cpu()\n",
    "                \n",
    "                l = -(ys != ys_hat).long()\n",
    "            metrics = OODMetrics()\n",
    "            metrics.update(scores, l)\n",
    "            r = metrics.compute()\n",
    "            r.update({\n",
    "                \"Method\": detector_name,\n",
    "                \"Dataset\": data_name\n",
    "            })\n",
    "            print(r)\n",
    "            results.append(r)\n",
    "    \n",
    "    return results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUROC': 0.9887268543243408, 'AUPR-IN': 0.9847310185432434, 'AUPR-OUT': 0.9913710355758667, 'ACC95TPR': 0.9554131627082825, 'FPR95TPR': 0.040084730833768845, 'Method': 'MSP', 'Dataset': 'LSUNCrop'}\n",
      "{'AUROC': 0.9940543174743652, 'AUPR-IN': 0.9925169348716736, 'AUPR-OUT': 0.99504154920578, 'ACC95TPR': 0.9653999209403992, 'FPR95TPR': 0.021671826019883156, 'Method': 'MSP', 'Dataset': 'LSUNResize'}\n",
      "{'AUROC': 0.9857792854309082, 'AUPR-IN': 0.9711555242538452, 'AUPR-OUT': 0.9926772713661194, 'ACC95TPR': 0.9500821232795715, 'FPR95TPR': 0.049942970275878906, 'Method': 'MSP', 'Dataset': 'Textures'}\n",
      "{'AUROC': 0.9893912672996521, 'AUPR-IN': 0.9870184659957886, 'AUPR-OUT': 0.9909202456474304, 'ACC95TPR': 0.9551480412483215, 'FPR95TPR': 0.040573570877313614, 'Method': 'MSP', 'Dataset': 'TinyImageNetCrop'}\n",
      "{'AUROC': 0.9922948479652405, 'AUPR-IN': 0.9906942844390869, 'AUPR-OUT': 0.9934539198875427, 'ACC95TPR': 0.9617322087287903, 'FPR95TPR': 0.028434088453650475, 'Method': 'MSP', 'Dataset': 'TinyImageNetResize'}\n",
      "{'AUROC': 0.9898166060447693, 'AUPR-IN': 0.9891467690467834, 'AUPR-OUT': 0.9897165298461914, 'ACC95TPR': 0.9670349359512329, 'FPR95TPR': 0.018657324835658073, 'Method': 'Energy', 'Dataset': 'LSUNCrop'}\n",
      "{'AUROC': 0.9919992089271545, 'AUPR-IN': 0.9923102855682373, 'AUPR-OUT': 0.9912968277931213, 'ACC95TPR': 0.9715864062309265, 'FPR95TPR': 0.010265601798892021, 'Method': 'Energy', 'Dataset': 'LSUNResize'}\n",
      "{'AUROC': 0.9798855781555176, 'AUPR-IN': 0.9690091609954834, 'AUPR-OUT': 0.9874686002731323, 'ACC95TPR': 0.9274219870567322, 'FPR95TPR': 0.08367280662059784, 'Method': 'Energy', 'Dataset': 'Textures'}\n",
      "{'AUROC': 0.9893457293510437, 'AUPR-IN': 0.9893818497657776, 'AUPR-OUT': 0.9889000058174133, 'ACC95TPR': 0.9650022387504578, 'FPR95TPR': 0.02240508422255516, 'Method': 'Energy', 'Dataset': 'TinyImageNetCrop'}\n",
      "{'AUROC': 0.989874005317688, 'AUPR-IN': 0.9898557662963867, 'AUPR-OUT': 0.9893951416015625, 'ACC95TPR': 0.9677419066429138, 'FPR95TPR': 0.017353756353259087, 'Method': 'Energy', 'Dataset': 'TinyImageNetResize'}\n",
      "{'AUROC': 0.9963926672935486, 'AUPR-IN': 0.9953438639640808, 'AUPR-OUT': 0.9969731569290161, 'ACC95TPR': 0.9715864062309265, 'FPR95TPR': 0.010265601798892021, 'Method': 'Ensemble', 'Dataset': 'LSUNCrop'}\n",
      "{'AUROC': 0.9975814819335938, 'AUPR-IN': 0.9972983598709106, 'AUPR-OUT': 0.9977465271949768, 'ACC95TPR': 0.9743703007698059, 'FPR95TPR': 0.005132800899446011, 'Method': 'Ensemble', 'Dataset': 'LSUNResize'}\n",
      "{'AUROC': 0.9945910573005676, 'AUPR-IN': 0.9892820715904236, 'AUPR-OUT': 0.9971697330474854, 'ACC95TPR': 0.9691844582557678, 'FPR95TPR': 0.021508879959583282, 'Method': 'Ensemble', 'Dataset': 'Textures'}\n",
      "{'AUROC': 0.9967246055603027, 'AUPR-IN': 0.9961276054382324, 'AUPR-OUT': 0.9971213936805725, 'ACC95TPR': 0.9710119366645813, 'FPR95TPR': 0.011324751190841198, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetCrop'}\n",
      "{'AUROC': 0.9971032738685608, 'AUPR-IN': 0.9966618418693542, 'AUPR-OUT': 0.9973896145820618, 'ACC95TPR': 0.9719399213790894, 'FPR95TPR': 0.009613817557692528, 'Method': 'Ensemble', 'Dataset': 'TinyImageNetResize'}\n",
      "{'AUROC': 0.994419515132904, 'AUPR-IN': 0.992945671081543, 'AUPR-OUT': 0.9962198734283447, 'ACC95TPR': 0.971895694732666, 'FPR95TPR': 0.009695290587842464, 'Method': 'Semantic', 'Dataset': 'LSUNCrop'}\n",
      "{'AUROC': 0.9941929578781128, 'AUPR-IN': 0.9918316006660461, 'AUPR-OUT': 0.9961431622505188, 'ACC95TPR': 0.9730446338653564, 'FPR95TPR': 0.007576991803944111, 'Method': 'Semantic', 'Dataset': 'LSUNResize'}\n",
      "{'AUROC': 0.9924079179763794, 'AUPR-IN': 0.9834509491920471, 'AUPR-OUT': 0.9965258836746216, 'ACC95TPR': 0.9703885912895203, 'FPR95TPR': 0.019716473296284676, 'Method': 'Semantic', 'Dataset': 'Textures'}\n",
      "{'AUROC': 0.9937518239021301, 'AUPR-IN': 0.9913430213928223, 'AUPR-OUT': 0.9957840442657471, 'ACC95TPR': 0.9703049063682556, 'FPR95TPR': 0.012628319673240185, 'Method': 'Semantic', 'Dataset': 'TinyImageNetCrop'}\n",
      "{'AUROC': 0.994046151638031, 'AUPR-IN': 0.9918168783187866, 'AUPR-OUT': 0.9960036277770996, 'ACC95TPR': 0.9716747403144836, 'FPR95TPR': 0.010102655738592148, 'Method': 'Semantic', 'Dataset': 'TinyImageNetResize'}\n",
      "{'AUROC': 0.9943230152130127, 'AUPR-IN': 0.9928470253944397, 'AUPR-OUT': 0.996130108833313, 'ACC95TPR': 0.9718073606491089, 'FPR95TPR': 0.009858236648142338, 'Method': 'Semantic-OE', 'Dataset': 'LSUNCrop'}\n",
      "{'AUROC': 0.9941083192825317, 'AUPR-IN': 0.9917459487915039, 'AUPR-OUT': 0.9960635900497437, 'ACC95TPR': 0.972912073135376, 'FPR95TPR': 0.007821410894393921, 'Method': 'Semantic-OE', 'Dataset': 'LSUNResize'}\n",
      "{'AUROC': 0.9922808408737183, 'AUPR-IN': 0.9832335114479065, 'AUPR-OUT': 0.9964580535888672, 'ACC95TPR': 0.9695128798484802, 'FPR95TPR': 0.021020041778683662, 'Method': 'Semantic-OE', 'Dataset': 'Textures'}\n",
      "{'AUROC': 0.9936367869377136, 'AUPR-IN': 0.9912216663360596, 'AUPR-OUT': 0.995680034160614, 'ACC95TPR': 0.9699955582618713, 'FPR95TPR': 0.013198630884289742, 'Method': 'Semantic-OE', 'Dataset': 'TinyImageNetCrop'}\n",
      "{'AUROC': 0.993944525718689, 'AUPR-IN': 0.9917117357254028, 'AUPR-OUT': 0.9959099888801575, 'ACC95TPR': 0.971453845500946, 'FPR95TPR': 0.010510020889341831, 'Method': 'Semantic-OE', 'Dataset': 'TinyImageNetResize'}\n"
     ]
    }
   ],
   "source": [
    "res = evaluate2(label_net, shape_net, color_net, shield_net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_458462/735313046.py:1: FutureWarning: The default value of numeric_only in DataFrameGroupBy.mean is deprecated. In a future version, numeric_only will default to False. Either specify numeric_only or select only columns which should be valid for the function.\n",
      "  (result_df.groupby(by=[\"Method\", \"Seed\"]).mean() * 100).groupby(\"Method\").agg([\"mean\", \"sem\"])\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"2\" halign=\"left\">AUROC</th>\n",
       "      <th colspan=\"2\" halign=\"left\">AUPR-IN</th>\n",
       "      <th colspan=\"2\" halign=\"left\">AUPR-OUT</th>\n",
       "      <th colspan=\"2\" halign=\"left\">ACC95TPR</th>\n",
       "      <th colspan=\"2\" halign=\"left\">FPR95TPR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>mean</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>sem</th>\n",
       "      <th>mean</th>\n",
       "      <th>sem</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Method</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>Energy</th>\n",
       "      <td>99.288969</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.633252</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.558470</td>\n",
       "      <td>NaN</td>\n",
       "      <td>96.722770</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.998416</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Ensemble</th>\n",
       "      <td>99.682112</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.409517</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.812239</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97.111565</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.346002</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>MSP</th>\n",
       "      <td>98.604983</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97.449214</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.153694</td>\n",
       "      <td>NaN</td>\n",
       "      <td>94.900970</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5.108472</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Semantic</th>\n",
       "      <td>99.345369</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.822942</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.683890</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97.191517</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.228820</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Semantic-OE</th>\n",
       "      <td>99.343197</td>\n",
       "      <td>NaN</td>\n",
       "      <td>98.820080</td>\n",
       "      <td>NaN</td>\n",
       "      <td>99.682142</td>\n",
       "      <td>NaN</td>\n",
       "      <td>97.186254</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1.236738</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 AUROC        AUPR-IN       AUPR-OUT       ACC95TPR      \\\n",
       "                  mean sem       mean sem       mean sem       mean sem   \n",
       "Method                                                                    \n",
       "Energy       99.288969 NaN  98.633252 NaN  99.558470 NaN  96.722770 NaN   \n",
       "Ensemble     99.682112 NaN  99.409517 NaN  99.812239 NaN  97.111565 NaN   \n",
       "MSP          98.604983 NaN  97.449214 NaN  99.153694 NaN  94.900970 NaN   \n",
       "Semantic     99.345369 NaN  98.822942 NaN  99.683890 NaN  97.191517 NaN   \n",
       "Semantic-OE  99.343197 NaN  98.820080 NaN  99.682142 NaN  97.186254 NaN   \n",
       "\n",
       "             FPR95TPR      \n",
       "                 mean sem  \n",
       "Method                     \n",
       "Energy       1.998416 NaN  \n",
       "Ensemble     1.346002 NaN  \n",
       "MSP          5.108472 NaN  \n",
       "Semantic     1.228820 NaN  \n",
       "Semantic-OE  1.236738 NaN  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(result_df.groupby(by=[\"Method\", \"Seed\"]).mean() * 100).groupby(\"Method\").agg([\"mean\", \"sem\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Label</th>\n",
       "      <th>Color</th>\n",
       "      <th>Shape</th>\n",
       "      <th>Sign</th>\n",
       "      <th>Seed</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.971813</td>\n",
       "      <td>0.993508</td>\n",
       "      <td>0.998971</td>\n",
       "      <td>0.998654</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sem</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         Label     Color     Shape      Sign  Seed\n",
       "mean  0.971813  0.993508  0.998971  0.998654   0.0\n",
       "sem        NaN       NaN       NaN       NaN   NaN"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(results_acc).agg([\"mean\", \"sem\"])\n",
    "# dataset gives np.array([label, color, shape])\n",
    "\n",
    "# results_acc = evaluate_accs(label_net, shape_net, color_net, shield_net)\n",
    "# print(results_acc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ood_label(x):\n",
    "    if x == True:\n",
    "        return \"Normal\"\n",
    "    else:\n",
    "        return \"Anomaly\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "trans = Compose([Resize(size=(32, 32)), ToRGB(), ToTensor()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pytorch_ood.utils import OODMetrics, ToRGB, ToUnknown\n",
    "from pytorch_ood.dataset.img import Textures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "58647e5af9db478ead649e2fa44c4149",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUROC': 0.1820300966501236, 'AUPR-IN': 0.5679941773414612, 'AUPR-OUT': 0.998434841632843, 'ACC95TPR': 0.9922276735305786, 'FPR95TPR': 0.010055423714220524}\n"
     ]
    }
   ],
   "source": [
    "test_in_data = GTSRB(root=root, train=False, transforms=trans, target_transform=lambda y: y[0])\n",
    "dataset_out_test = Textures(root=root, transform=trans, target_transform=ToUnknown(), download=True)\n",
    "loader = DataLoader(dataset_out_test +  test_in_data, batch_size=128, worker_init_fn=seed_worker)\n",
    "\n",
    "sem_detector = SemanticDetector(\n",
    "            label_net, \n",
    "            shape_net, \n",
    "            color_net, \n",
    "            GTSRB(root=root).class_to_shape, \n",
    "            GTSRB(root=root).class_to_color, \n",
    "            sign_net=shield_net\n",
    "        )\n",
    "\n",
    "with torch.no_grad():\n",
    "    metrics = OODMetrics()\n",
    "    for x, y in tqdm(loader):\n",
    "        metrics.update(sem_detector(x.to(device)), y.to(device))\n",
    "\n",
    "print(metrics.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d049d30ee2594bd784ea345e90cdc4c1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUROC': 0.9999501705169678, 'AUPR-IN': 0.9998885989189148, 'AUPR-OUT': 0.9999778866767883, 'ACC95TPR': 0.984455406665802, 'FPR95TPR': 0.0001583531266078353}\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    metrics = OODMetrics()\n",
    "    for x, y in tqdm(loader):\n",
    "        metrics.update(-shield_net(x.to(device)).softmax(dim=1)[:,1].cpu(), y.to(device))\n",
    "\n",
    "sign_net_metric = metrics.compute()\n",
    "print(sign_net_metric)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b909d808f0364d8e8b93889f82a16804",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/143 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUROC': 0.994219183921814, 'AUPR-IN': 0.9748163819313049, 'AUPR-OUT': 0.997749388217926, 'ACC95TPR': 0.967870831489563, 'FPR95TPR': 0.0241488516330719}\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm \n",
    "\n",
    "test_in_data = GTSRB(root=root, train=False, transforms=trans, target_transform=lambda y: y[0])\n",
    "dataset_out_test = Textures(root=root, transform=trans, target_transform=ToUnknown(), download=True)\n",
    "loader = DataLoader(dataset_out_test +  test_in_data, batch_size=128, worker_init_fn=seed_worker)\n",
    "\n",
    "detector = MaxSoftmax(label_net)\n",
    "    \n",
    "sem_detector = KBDetector(\"knowledge_base.pl\", label_net, shape_net, color_net, sign_net=shield_net)\n",
    "\n",
    "scores = []\n",
    "my_scores = []\n",
    "ys = []\n",
    "xs = []\n",
    "ys_hat = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in tqdm(loader):\n",
    "        x = x.to(device)\n",
    "        scores.append(detector(x))\n",
    "        ys_hat.append(label_net(x).softmax(dim=1).max(dim=1).indices) \n",
    "        my_scores.append(sem_detector(x))\n",
    "        \n",
    "        ys.append(y)\n",
    "        xs.append(x.cpu())\n",
    "\n",
    "\n",
    "scores = torch.cat(scores, dim=0).cpu()\n",
    "ys = torch.cat(ys, dim=0).cpu()\n",
    "ys_hat = torch.cat(ys_hat, dim=0).cpu()\n",
    "my_scores = torch.cat(my_scores, dim=0).cpu()\n",
    "xs = torch.cat(xs, dim=0).cpu()\n",
    "\n",
    "\n",
    "metrics = OODMetrics()\n",
    "metrics.update(my_scores, ys)\n",
    "print(metrics.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b8e5386e6cf348ce851e5702c86fd8c5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c08041cde9674d26b1786914dd3d422b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "68f3bf9d71034e4f8edf4485c8d0c3ed",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/45 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b7d7c4d453c047fd883fd660d13ba579",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2bafffd3d1d404495122dfae871190b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/79 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d19d2c67ac6449f2ae89926b62782a33",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/99 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'AUROC': 0.9969454407691956, 'AUPR-IN': 0.9989409446716309, 'AUPR-OUT': 0.9932375550270081, 'ACC95TPR': 0.958040177822113, 'FPR95TPR': 0.012905780225992203}\n",
      "{'AUROC': 0.9969454407691956, 'AUPR-IN': 0.9989409446716309, 'AUPR-OUT': 0.9932375550270081, 'ACC95TPR': 0.958040177822113, 'FPR95TPR': 0.012905780225992203}\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm \n",
    "\n",
    "\n",
    "test_in_data = GTSRB(root=root, train=False, transforms=trans, target_transform=lambda y: y[0])\n",
    "\n",
    "\n",
    "sem_detector = KBDetector(\"knowledge_base.pl\", label_net, shape_net, color_net, sign_net=shield_net)\n",
    "\n",
    "my_scores = []\n",
    "ys = []\n",
    "\n",
    "metrics = OODMetrics()\n",
    "\n",
    "datasets = (LSUNCrop, LSUNResize, Textures, TinyImageNetCrop, TinyImageNetResize)\n",
    "datasets = [c(root=root, transform=trans, target_transform=ToUnknown(), download=True) for c in datasets]\n",
    "loaders = [DataLoader(d, batch_size=128, worker_init_fn=seed_worker) for d in datasets]\n",
    "loaders.append( DataLoader(test_in_data, batch_size=128, worker_init_fn=seed_worker))\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "     for loader in loaders:\n",
    "        for x, y in tqdm(loader):\n",
    "            x = x.to(device)\n",
    "            scores = sem_detector(x)\n",
    "\n",
    "            metrics.update(scores.cpu(), y.cpu())\n",
    "            my_scores.append(scores)\n",
    "            ys.append(y)\n",
    "\n",
    "print(metrics.compute())\n",
    "\n",
    "ys = torch.cat(ys, dim=0).cpu()\n",
    "my_scores = torch.cat(my_scores, dim=0).cpu()\n",
    "\n",
    "metrics = OODMetrics()\n",
    "metrics.update(my_scores, ys)\n",
    "print(metrics.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (3662306820.py, line 8)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[30], line 8\u001b[0;36m\u001b[0m\n\u001b[0;31m    for data_name, dataset_c in datasets.items():\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "from tqdm.notebook import tqdm \n",
    "\n",
    "test_in_data = GTSRB(root=root, train=False, transforms=trans, target_transform=lambda y: y[0])\n",
    "dataset_out_test = Textures(root=root, transform=trans, target_transform=ToUnknown(), download=True)\n",
    "loader = DataLoader(dataset_out_test +  test_in_data, batch_size=128, worker_init_fn=seed_worker)\n",
    "\n",
    "\n",
    " for data_name, dataset_c in datasets.items():\n",
    "            data_out = dataset_c(root=root, transform=trans, target_transform=ToUnknown(), download=True)\n",
    "            loader = DataLoader(data_in+data_out, batch_size=1024, shuffle=False, worker_init_fn=seed_worker)\n",
    "            \n",
    "sem_detector = KBDetector(\"knowledge_base.pl\", label_net, shape_net, color_net, sign_net=shield_net)\n",
    "\n",
    "my_scores = []\n",
    "ys = []\n",
    "\n",
    "metrics = OODMetrics()\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in tqdm(loader):\n",
    "        x = x.to(device)\n",
    "        scores = sem_detector(x)\n",
    "        \n",
    "        metrics.update(scores.cpu(), y.cpu())\n",
    "        my_scores.append(scores)\n",
    "        ys.append(y)\n",
    "\n",
    "print(metrics.compute())\n",
    "\n",
    "ys = torch.cat(ys, dim=0).cpu()\n",
    "my_scores = torch.cat(my_scores, dim=0).cpu()\n",
    "\n",
    "metrics = OODMetrics()\n",
    "metrics.update(my_scores, ys)\n",
    "print(metrics.compute())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(xs.shape)\n",
    "print(my_scores.shape)\n",
    "print(ys_hat.shape)\n",
    "print(ys.shape)\n",
    "print(scores.shape)\n",
    "print(len(test_in_data))\n",
    "print(len(dataset_out_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import DataFrame\n",
    "\n",
    "df_1 = DataFrame()\n",
    "df_1[\"Scores\"] = scores.cpu().numpy()\n",
    "df_1[\"Labels\"] = ys >= 0\n",
    "df_1[\"Labels\"] = df_1[\"Labels\"].apply(ood_label)\n",
    "df_1[\"Method\"] = \"Implicit\"\n",
    "\n",
    "sb.histplot(data=df_1, x=\"Scores\", hue=\"Labels\", common_norm=False, stat=\"probability\", bins=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Examples "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# custom_params = {\"axes.spines.right\": True, \"axes.spines.top\": True, \"axes.spines.bottom\": True, \"axes.spines.left\": True}\n",
    "import matplotlib.pyplot as plt \n",
    "import numpy as np \n",
    "\n",
    "sb.set_theme(style=\"white\")\n",
    "\n",
    "sem_detector = KBDetector(\"knowledge_base.pl\", label_net, shape_net, color_net, sign_net=shield_net)\n",
    "\n",
    "dataset = GTSRB(root=root)\n",
    "\n",
    "top_values, top_indxs = (-scores[ys<0]).topk(50)\n",
    "datasets = dataset_out_test +  test_in_data\n",
    "imgs = [datasets[i][0] for i in top_indxs]\n",
    "\n",
    "for n, img in enumerate(imgs):\n",
    "    img_batch = img.unsqueeze(0).to(device)\n",
    "    \n",
    "    print(img_batch.shape)\n",
    "    with torch.no_grad():\n",
    "        l = label_net(img_batch)\n",
    "        s = shape_net(img_batch)\n",
    "        c = color_net(img_batch)\n",
    "        o = shield_net(img_batch)\n",
    "        \n",
    "    sign_detected = o.max(dim=1).indices.cpu()\n",
    "        \n",
    "    my_score = sem_detector(img_batch)[0]\n",
    "    \n",
    "    lindex = l.argmax(dim=1).item()\n",
    "    sindex = s.argmax(dim=1).item()\n",
    "    cindex = c.argmax(dim=1).item()\n",
    "    oindex = o.argmax(dim=1).item()\n",
    "    \n",
    "    lname = dataset.class_to_name[lindex]\n",
    "    sname = dataset.shape_to_name[sindex]\n",
    "    cname = dataset.color_to_name[cindex]\n",
    "    oname = \"Sign\" if oindex else \"NoSign\"\n",
    "    \n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.imshow(np.moveaxis(img.numpy(), 0, -1))\n",
    "    plt.suptitle(f\"'{lname.title()}' ({l.softmax(dim=1).max().item():.2%}) \\n {sname.title()} | {cname.title()} | {oname.title()}\")\n",
    "    plt.title(f\"Consistent: {'Yes' if abs(my_score) > 0.0 else 'No'}\") # 'Yes' if my_score > 0.0 else 'No'\n",
    "    plt.tight_layout(pad=0.5)\n",
    "    plt.savefig(f\"img/prediction-example-{n}.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_detector = KBDetector(\"knowledge_base.pl\", label_net, shape_net, color_net, sign_net=shield_net)\n",
    "\n",
    "top_values, top_indxs = (scores[ys>=0]).topk(10)\n",
    "\n",
    "for n, i in enumerate([i for i in top_indxs]):\n",
    "    img = xs[ys>=0][i]\n",
    "    img_batch = img.unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        l = label_net(img_batch)\n",
    "        s = shape_net(img_batch)\n",
    "        c = color_net(img_batch)\n",
    "        o = shield_net(img_batch)\n",
    "    \n",
    "    # my_score = detector(img_batch)[0]\n",
    "    my_score = my_scores[ys>=0][i]\n",
    "    # score = scores[ys>=0][i]\n",
    "    \n",
    "    lindex = l.argmax(dim=1).item()\n",
    "    sindex = s.argmax(dim=1).item()\n",
    "    cindex = c.argmax(dim=1).item()\n",
    "    oindex = o.argmax(dim=1).item()\n",
    "        \n",
    "    lname = dataset.class_to_name[lindex]\n",
    "    sname = dataset.shape_to_name[sindex]\n",
    "    cname = dataset.color_to_name[cindex]\n",
    "    oname = \"Sign\" if oindex == 1 else \"NoSign\"\n",
    "        \n",
    "    plt.imshow(np.moveaxis(img.numpy(), 0, -1))\n",
    "    plt.suptitle(f\"'{lname.title()}' ({l.softmax(dim=1).max().item():.2%}) \\n {sname.title()} | {cname.title()} | {oname}\")\n",
    "    plt.title(f\"Consistent: {'Yes' if abs(my_score) > 0.0 else 'No'}\") # 'Yes' if my_score > 0.0 else 'No'\n",
    "    plt.xticks([])\n",
    "    plt.yticks([])\n",
    "    plt.tight_layout(pad=0.5)\n",
    "    plt.savefig(f\"img/prediction-example-in-{n}.pdf\", bbox_inches=\"tight\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_detector = KBDetector(\"knowledge_base.pl\", label_net, shape_net, color_net, sign_net=shield_net)\n",
    "\n",
    "# in-distribution where prediction was not correct and was rejected \n",
    "index = (ys>=0) & (ys != ys_hat.cpu()) & (my_scores.abs() == 0.0)\n",
    "if index.sum().item() > 0:\n",
    "    top_values, top_indxs = (-scores[index]).topk(1)\n",
    "\n",
    "    for n, i in enumerate([i for i in top_indxs]):\n",
    "        img = xs[index][i]\n",
    "        img_batch = img.unsqueeze(0).to(device)\n",
    "\n",
    "        print(img_batch.shape)\n",
    "        with torch.no_grad():\n",
    "            l = label_net(img_batch)\n",
    "            s = shape_net(img_batch)\n",
    "            c = color_net(img_batch)\n",
    "            o = shield_net(img_batch)\n",
    "\n",
    "        my_score = sem_detector(img_batch)[0]\n",
    "\n",
    "        lindex = l.argmax(dim=1).item()\n",
    "        sindex = s.argmax(dim=1).item()\n",
    "        cindex = c.argmax(dim=1).item()\n",
    "\n",
    "        lname = dataset.class_to_name[lindex]\n",
    "        sname = dataset.shape_to_name[sindex]\n",
    "        cname = dataset.color_to_name[cindex]\n",
    "        oname = \"Sign\" if oindex == 1 else \"NoSign\"\n",
    "\n",
    "        plt.imshow(np.moveaxis(img.numpy(), 0, -1))\n",
    "        plt.suptitle(f\"'{lname.title()}' ({l.softmax(dim=1).max().item():.2%}) \\n {sname.title()} | {cname.title()} | {oname.title()}\")\n",
    "        plt.title(f\"Consistent: {'Yes' if abs(my_score) > 0.0 else 'No'}\") # 'Yes' if my_score > 0.0 else 'No'\n",
    "        plt.tight_layout(pad=0.5)\n",
    "        plt.xticks([])\n",
    "        plt.yticks([])\n",
    "        plt.savefig(f\"img/prediction-example-in-error-{n}.pdf\", bbox_inches=\"tight\")\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import seaborn as sb \n",
    "\n",
    "ys_hat_anom = ys_hat.clone()\n",
    "ys_hat_anom[my_scores > -0.99] = 44 \n",
    "\n",
    "fig, ax = plt.subplots(figsize=(20,20))\n",
    "m = confusion_matrix(ys[ys >= 0], ys_hat_anom[ys >= 0])\n",
    "\n",
    "for i in range(m.shape[0]):\n",
    "    m[i,i] = 0\n",
    "    \n",
    "disp = ConfusionMatrixDisplay(m, display_labels=list(dataset.class_to_name.values()) + [\"Anomaly\"])\n",
    "disp.plot(ax=ax, xticks_rotation=\"vertical\", colorbar=False)\n",
    "\n",
    "plt.tight_layout(pad=0)\n",
    "\n",
    "plt.savefig(\"img/confusion.pdf\")\n",
    "plt.savefig(\"img/confusion.jpg\", dpi=300, bbox_inches=\"tight\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "min_score = my_scores[ys < 0][my_scores[ys < 0] != -1].min().item() - 0.00001\n",
    "\n",
    "ys_hat_anom = ys_hat.clone()\n",
    "ys_hat_anom[my_scores > -0.99] = -1 \n",
    "\n",
    "(ys_hat_anom[ys >= 0] == 44).sum()\n",
    "\n",
    "print(f\"Total number of street signs {(ys >= 0).sum()}\")\n",
    "print(f\"Total number of anomalies {(ys < 0).sum()}\")\n",
    "print(f\"Signs mistakenly marked as anomalies (no matter prediction) {(ys_hat_anom[ys >= 0] < 0).sum()}\")\n",
    "print(f\"Signs mistakenly marked as anomalies (correct sign predicted) {(ys_hat_anom[ys == ys_hat] < 0).sum()}\")\n",
    "print(f\"Signs mistakenly marked as anomalies (false sign predicted) {(ys_hat_anom[(ys != ys_hat) & (ys >= 0)] < 0).sum()}\")\n",
    "\n",
    "print(f\"Signs maked as non-anomalous (false sign predicted) {(  (ys_hat_anom >= 0) & (ys_hat_anom != ys) ).sum() }\")\n",
    "print(f\"Anomaly mistakenly marked as sign (no matter prediction) {(ys_hat_anom[ys < 0] >= 0).sum()}\")\n",
    "\n",
    "t = (ys_hat_anom[ys == ys_hat] < 0).sum() + ((ys_hat_anom >= 0) & (ys_hat_anom != ys)).sum() + (ys_hat_anom[ys < 0] >= 0).sum()\n",
    "total = ys.shape[0]\n",
    "print(f\"Instances where our decision was not optimal: {t}/{total}={t/total:.3%}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "red_classes = [k for k, v in dataset.class_to_color.items() if dataset.color_to_name[v] == \"red\"]\n",
    "red_classes.sort()\n",
    "# red_classes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "detector = EnsembleDetector(label_net, shape_net, color_net)\n",
    "\n",
    "with torch.no_grad():\n",
    "    metrics = OODMetrics()\n",
    "    for x, y in loader:\n",
    "        metrics.update(detector(x.to(device)), y.to(device))\n",
    "\n",
    "metrics_ensemble = metrics.compute()\n",
    "print(metrics_ensemble)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sem_detector = SemanticDetector(\n",
    "    label_net, \n",
    "    shape_net, \n",
    "    color_net, \n",
    "    GTSRB(root=root).class_to_shape, \n",
    "    GTSRB(root=root).class_to_color, \n",
    "    sign_net=shield_net\n",
    ")\n",
    "\n",
    "scores = []\n",
    "ys = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for x, y in loader:\n",
    "        scores.append(sem_detector(x.to(device)).cpu())\n",
    "        ys.append(y)\n",
    "        \n",
    "scores = torch.cat(scores)\n",
    "ys = torch.cat(ys)\n",
    "\n",
    "\n",
    "metrics = OODMetrics()\n",
    "metrics.update(scores, ys)\n",
    "metrics_semantic = metrics.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2 = DataFrame()\n",
    "df_2[\"Scores\"] = scores.cpu().numpy()\n",
    "df_2[\"Labels\"] = ys >= 0\n",
    "df_2[\"Labels\"] = df_2[\"Labels\"].apply(ood_label)\n",
    "df_2[\"Method\"] = \"Explicit\"\n",
    "\n",
    "sb.histplot(data=df_2, x=\"Scores\", hue=\"Labels\", common_norm=False, stat=\"probability\", bins=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sb.displot(pd.concat([df_1, df_2]).reset_index(), x=\"Scores\", hue=\"Labels\", col=\"Method\", stat=\"probability\", bins=30, kind=\"hist\",  common_norm=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_semantic[\"Method\"] = \"Explicit\"\n",
    "metrics_semantic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_softmax[\"Method\"] = \"Implicit\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_ensemble[\"Method\"] = \"Ensemble\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_df = pd.DataFrame([metrics_semantic, metrics_softmax, metrics_ensemble])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot distplot\n",
    "fig, ax = plt.subplots()\n",
    "sb.barplot(data=result_df, x=\"Method\", y=\"AUROC\")\n",
    "\n",
    "# change the limits of X-axis\n",
    "ax.set_ylim(0.95, 1)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print((result_df.groupby(by=\"Method\").mean() * 100)[[\"AUROC\", \"AUPR-IN\", \"AUPR-OUT\", \"FPR95TPR\"]].to_latex(float_format=\"%.2f\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "logic-ood",
   "language": "python",
   "name": "logic-ood"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
